{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted files to /Users/AlexG/Downloads/极品GRE红宝书.apkg-decompress\n",
      "blocked main thread for 825ms:\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/m0/h3b4dj7x09vfsx38dqn5s41r0000gn/T/ipykernel_78040/3553855856.py\", line 53, in <module>\n",
      "    col = Collection(ANKI21_PATH)\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/anki/collection.py\", line 150, in __init__\n",
      "    self.reopen()\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/anki/collection.py\", line 291, in reopen\n",
      "    self._backend.open_collection(\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/anki/_backend_generated.py\", line 143, in open_collection\n",
      "    raw_bytes = self._run_command(3, 0, message.SerializeToString())\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/anki/_backend.py\", line 167, in _run_command\n",
      "    print(\"\".join(traceback.format_stack()))\n",
      "\n",
      "Decks in collection:\n",
      "Deck ID: 1685695741424, Name: 极品GRE红宝书\n",
      "Deck ID: 1, Name: Default\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7513/7513 [00:01<00:00, 4036.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mLast review time: time.struct_time(tm_year=2025, tm_mon=3, tm_mday=16, tm_hour=13, tm_min=23, tm_sec=40, tm_wday=6, tm_yday=75, tm_isdst=1)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7513/7513 [00:01<00:00, 5557.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[ivl=1]\u001b[0m \u001b[32m['approbation', \"[`æprə'beiʃən]\", 'n.称赞，认可', '<b>【英】</b> n.称赞, 认可(official approval, sanction, or commendation)<br><b>【考】</b> praise : approbation / demur : objection(赞扬表示认可/异议 表示反对)censure : blameless / approbation : reprehensible(无过失的不应该受责 难/应谴责的不应该受 称赞)反义词：opprobrium( 谴责，恶名声)；cond emnation(谴责); castigation(斥责)<br><b>【记】</b> ap+prob=prove( 证实)+ation→证实是 好的→称赞', 'approbation (n.) <br>late 14c., \"proven effectiveness, excellence,\" from Old French aprobacion or directly from Latin approbationem (nominative approbatio) \"an approval,\" noun of action from past participle stem of approbare (see approve). Meaning \"approval, endorsement\" is from early 15c.', 'n. (官方)称赞, 认可, 批准<br>【例】The play received the approbation of the mass media 这个剧本受到大众媒体的欢迎.<br>【记】1)prob = prove证实－证实好－称赞2)ap朝向, prob = probe探索, ate吃的过去式. 朝向探索吃的. 因为闻到了香味, 这种吃的值得赞扬和官方的认可. 3)ap, porb = probe探索, ation: 探索精神值得称赞<br>【区】reprobate(n 堕落者v 非难)是approbate的反义词<br>【类】praise: approbation = demur: objection = endorse: approbation赞扬表示嘉许 = 异议表示反对 = 背书同意表示许可censure: blameless = approbation: reprehensible无可责备的不被责难 = 应该谴责的不被称赞<br>【反】opprobrium(n 谴责, 恶名声); condemnation(n 谴责); castigation(n 强烈谴责); execration(n 诅咒; 憎恶)', '【记】ap + 源自prove；同approve；ap 肯定 加强 + probe 探针探测,检验→检验后肯定<br>【相关】probate 遗嘱检验，reprobate 谴责，opprobrious 无礼的, 该骂的, 可耻的', '[sound:approbation.mp3]']\u001b[0m\n",
      "\u001b[31m[ivl=1]\u001b[0m \u001b[32m['behoove', \"[bi'həuv]\", 'v.理应，有义务', '无', 'behoove (v.) <br>Old English behofian \"to have need of, have use for,\" verbal form of the ancient compound word represented by behoof.<br>Historically, it rimes with move, prove, but being now mainly a literary word, it is generally made to rime with rove, grove, by those who know it only in books. [OED]', '无', '无', '[sound:behoove.mp3]']\u001b[0m\n",
      "\u001b[31m[ivl=1]\u001b[0m \u001b[32m['boisterous', \"['bɔistərəs]\", 'a.喧闹的；猛烈的', '<b>【英】</b> adj.喧闹的(noisy and unruly)<br><b>【考】</b> 反义词：quiet(安静 的)<br><b>【记】</b> boister(喧闹)+ous→ 喧闹的<br><b>【同】</b> 近形词：bolster(支 持); preposterous(荒谬的)<br><b>【例】</b> The friends played a boisterous game of football.', 'boisterous (adj.) <br>late 15c., unexplained alteration of Middle English boistous (c.1300) \"rough, coarse (as of food), clumsy, violent,\" of unknown origin, perhaps from Anglo-French bustous \"rough (road),\" which is perhaps from Old French boisteos \"curved, lame; uneven, rough\" (Modern French boiteux), itself of obscure origin. Another guess traces it via Celtic to Latin bestia. Used of persons from 1560s. Related: Boisterously; boisterousness.', 'adj. (风、海水、行动、言词等)狂暴的, 汹涌的; (人或其行动兴高采烈的)喧闹的, 吵吵嚷嚷的 = noisy = unruly = blatant<br>【根】boister(粗鲁的, 粗暴的, 喧闹), ous多－喧闹的, 狂暴的<br>【记】boi = boil(v 沸腾)－水沸腾的感觉－喧闹的, 狂暴的<br>【参】preposterous(adj 荒谬的)<br>【反】quiet(adj 安静的)', '【记】汹涌, 猛烈, 热情高涨，喧闹的 boil 水沸腾般；', '[sound:boisterous.mp3]']\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# extract package\n",
    "import sqlite3\n",
    "import anki\n",
    "from anki.collection import Collection\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import termcolor\n",
    "import tqdm\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "import pickle\n",
    "# import pandas as pd\n",
    "import editdistance\n",
    "\n",
    "EDICT_PATH = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data/ecdict.csv'\n",
    "MEDIA_FOLDER = '/Users/AlexG/Library/Application Support/Anki2/User 1/collection.media'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/MyPaperNotes.apkg'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/ivl10.apkg'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/极品GRE红宝书.apkg'\n",
    "DATA_PATH = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data'\n",
    "outdir = PKG_PATH + '-decompress'\n",
    "# outdir = '/Users/AlexG/Downloads/ivl10-decompress'\n",
    "# ANKI2_PATH = '/Users/AlexG/Downloads/ivl10-decompress/collection.anki2'\n",
    "ANKI21_PATH = outdir + '/collection.anki21'\n",
    "# EXPORT_PATH_WEBSITE = '/Users/AlexG/Documents/GitHub/typingpractices/website/onepager/onepager/templates/onepager/cards.html'\n",
    "EXPORT_PATH_WEBSITE = '/Users/AlexG/Documents/GitHub/anki_html/docs/gre_cards.html'\n",
    "\n",
    "def unzip_anki(pkg_file_path, outdir):\n",
    "    # Create the extract directory if it doesn't exist\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    # Unzip the .apkg file\n",
    "    with zipfile.ZipFile(PKG_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(outdir)\n",
    "\n",
    "    print(f\"Extracted files to {outdir}\")\n",
    "\n",
    "def get_lastest_review_time(col, cards):\n",
    "    rev_times = []\n",
    "    for card in tqdm.tqdm(cards):\n",
    "        timestamp_ms = col.db.scalar(\n",
    "            \"SELECT max(id) FROM revlog WHERE cid = {} AND type IN (1, 3)\".format(card)\n",
    "        )\n",
    "        rev_times.append(timestamp_ms)\n",
    "    mt = max(rev_times)\n",
    "    return time.localtime(mt/1000)\n",
    "\n",
    "try:\n",
    "    unzip_anki(PKG_PATH, outdir)\n",
    "    col = Collection(ANKI21_PATH)\n",
    "    # Fetch all decks in the collection\n",
    "    decks = col.decks.all()\n",
    "    print(\"Decks in collection:\")\n",
    "    for deck in decks:\n",
    "        print(f\"Deck ID: {deck['id']}, Name: {deck['name']}\")\n",
    "    # query = 'prop:ivl<=30'\n",
    "    cards = col.find_cards('')\n",
    "\n",
    "    # max review time\n",
    "    last_review_time = get_lastest_review_time(col, cards)\n",
    "    print(termcolor.colored(f\"Last review time: {last_review_time}\", 'green'))\n",
    "\n",
    "    # top 3 cards by ivl\n",
    "    data = []\n",
    "    for cid in tqdm.tqdm(cards):\n",
    "        card = col.get_card(cid)\n",
    "        note = card.note()\n",
    "        fields = note.fields\n",
    "        data.append((card.ivl, fields))\n",
    "    data.sort()\n",
    "    for ivl, c in data[:3]:\n",
    "        print(termcolor.colored('[ivl={}]'.format(ivl), 'red'), \n",
    "              termcolor.colored('{}'.format(c), 'green'))\n",
    "\n",
    "except Exception as e:\n",
    "    error = '{}'.format(e)\n",
    "    print('exceptin while extracting apkg: {}...'.format(e[:12]))\n",
    "\n",
    "col.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please write a story using the following comma separated words: \"canard,candid,congregate,deference,dilettante,disparate,emollient,feign,flabby,gaucherie,havoc,impassive,indignant,lassitude,malcontent,meek,pallid,precipitant,precipitation,shingle,somnolent,supple,thwart,abound,abstinent,acarpous,ancillary,appellation,apprise,arachnid,astray,awning,badinage,bale,banal,bauble,bauxite,belch,beseech,blight,braid,broach,brocade,cactus,calamity,calipers,cantankerous,capsize,carapace,caricature,celibate,cephalic,checkered,chirp,clairvoyance,colander,commonwealth,conclave,concoct,confection,conjugal,consummate,contiguous,contumacious,copious,coruscate,countenance,crux,curator,curtail,dalliance,dampen,deciduous,deign,demur,despondent,desultory,diaphanous,diffident,disclaimer,disconcert,discriminatory,disquisition,dissemble,distrait,divagate,divulge,doff,doggo,domicile,dullard,earthly,easel,effete,egregious,emphatic,enamored,encomium,enervate,engrave\"\n"
     ]
    }
   ],
   "source": [
    "# write story prompt\n",
    "words = []\n",
    "for ivl, c in data[:100]:\n",
    "    words.append(c[0])\n",
    "\n",
    "print('Please write a story using the following comma separated words, and please also provide Chinese translation of the provided words in parentheses: \"{}\"'.format(','.join(words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_backend', '_build_sort_mode', '_check_backend_undo_status', '_clear_caches', '_deprecated_aliases', '_deprecated_attributes', '_load_scheduler', '_logRem', '_pb_search_separator', '_remNotes', '_startReps', '_supported_scheduler_versions', 'abort_media_sync', 'abort_sync', 'addNote', 'add_custom_undo_entry', 'add_image_occlusion_note', 'add_image_occlusion_notetype', 'add_note', 'add_notes', 'after_note_updates', 'all_browser_columns', 'all_config', 'autosave', 'await_backup_completion', 'backend', 'browser_row_for_id', 'build_search_string', 'cardStats', 'card_count', 'card_ids_of_note', 'card_stats', 'card_stats_data', 'close', 'close_for_full_sync', 'compare_answer', 'compute_memory_state', 'conf', 'create_backup', 'crt', 'db', 'decks', 'default_deck_for_notetype', 'defaults_for_adding', 'emptyCids', 'export_anki_package', 'export_card_csv', 'export_collection_package', 'export_note_csv', 'extract_cloze_for_typing', 'field_names_for_note_ids', 'find_and_replace', 'find_cards', 'find_dupes', 'find_notes', 'fix_integrity', 'flush', 'format_timespan', 'full_upload_or_download', 'fuzz_delta', 'genCards', 'get_aux_notetype_config', 'get_aux_template_config', 'get_browser_column', 'get_card', 'get_config', 'get_config_bool', 'get_config_string', 'get_csv_metadata', 'get_empty_cards', 'get_image_for_occlusion', 'get_image_occlusion_note', 'get_note', 'get_preferences', 'group_searches', 'i18n_resources', 'import_anki_package', 'import_csv', 'import_json_file', 'import_json_string', 'initialize_backend_logging', 'is_empty', 'join_searches', 'latest_progress', 'load_browser_card_columns', 'load_browser_note_columns', 'log', 'media', 'media_sync_status', 'merge_undo_entries', 'mod', 'mod_schema', 'models', 'name', 'newNote', 'new_note', 'nextID', 'note_count', 'op_made_changes', 'optimize', 'path', 'redo', 'register_deprecated_aliases', 'register_deprecated_attributes', 'remNotes', 'remove_cards_and_orphaned_notes', 'remove_config', 'remove_notes', 'remove_notes_by_card', 'render_markdown', 'reopen', 'replace_in_search_node', 'reset', 'save', 'sched', 'sched_ver', 'schema_changed', 'server', 'setMod', 'set_aux_notetype_config', 'set_aux_template_config', 'set_browser_card_columns', 'set_browser_note_columns', 'set_config', 'set_config_bool', 'set_config_string', 'set_deck', 'set_preferences', 'set_schema_modified', 'set_user_flag_for_cards', 'set_v3_scheduler', 'set_wants_abort', 'startTimebox', 'stats', 'studied_today', 'sync_collection', 'sync_login', 'sync_media', 'sync_status', 'tags', 'timeboxReached', 'tr', 'undo', 'undo_name', 'undo_status', 'updateFieldCache', 'update_card', 'update_cards', 'update_image_occlusion_note', 'update_note', 'update_notes', 'upgrade_to_v2_scheduler', 'usn', 'v3_scheduler', 'weakref']\n",
      "total card count:  7513\n",
      "\u001b[32mquery  find 7513 cards\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7513 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7513/7513 [00:02<00:00, 3329.47it/s]\n",
      "770612it [00:07, 106993.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 750698 lines from /Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data/ecdict.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing word: brash:   2%|▏         | 3/200 [00:01<01:34,  2.08it/s]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mpaste-c3cd5f28474ce9f22c35cd0a4f65bc8db068046f.png\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing word: beseech:  56%|█████▌    | 111/200 [00:17<00:08,  9.97it/s]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mpaste-1438c0813bb4175e1f903f799b7063aaa3187c13.jpg\u001b[0m\n",
      "\u001b[31mGuinea-asks-bauxite-miners-to-present-local-refinery-plans-by-May.jpeg\u001b[0m\n",
      "\u001b[31mpaste-9c4c03b0a6345aaa754b5767e49078d8f7af2fe2.png\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing word: blight:  58%|█████▊    | 117/200 [00:18<00:08,  9.29it/s]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mpaste-3523e57ad956c937882df3f3007eeec60c6a0a3d.jpg\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing word: commonwealth:  70%|██████▉   | 139/200 [00:20<00:06,  9.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mpaste-90bc0f2e1ace5933848540588d0f9cfe6050a720.jpg\u001b[0m\n",
      "\u001b[31mpaste-c10718275d766cfd76becc4dbf268c97de695ebb.png\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing word: crux:  78%|███████▊  | 157/200 [00:22<00:03, 13.19it/s]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mpaste-269f42e29d2a01f624b5f937b600f96a34d81bbb.jpg\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing word: diaphanous:  85%|████████▌ | 170/200 [00:23<00:03,  9.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mpaste-6ba10c81153adc71fe825e59ebf26ff0f720307c.png\u001b[0m\n",
      "\u001b[31m220px-Anti-CPE_harangue_at_the_Assemblée_Nationale,_2006.jpg\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing word: emulsify: 100%|██████████| 200/200 [00:26<00:00,  7.59it/s]      \n",
      "ls: images: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.jpg', '.png', '.jpeg'}\n",
      "\u001b[34mData samples:\u001b[0m\n",
      "[\"<div>approbation</div><div>[`æprə'beiʃən]</div>\", '<div class=\"textscroll\">  <div class=\"simsub\"><b>appro(\\'æprәu)</b>: n. (商俚)看货后再作决定</div> <div class=\"simsub\"><b>ation(eɪʃən)</b>:  增强辐射, 文明阶段</div> <div class=\"simsub\"><b>bati()</b>: n. (Bati)人名；(阿拉伯)巴提；(法、罗)巴蒂</div> <div class=\"simsub\"><b>probation(prәu\\'beiʃәn)</b>: n. 鉴定, 查验, 证明, 试用, 察看, 缓刑\\\\n[医] 审辨, 试验, 鉴定, 试用, 见习</div> <div class=\"simsub\"><b>tion()</b>: n. 象征式互动；支票电托收</div></div>', 'n.称赞，认可']\n",
      "[\"<div>behoove</div><div>[bi'həuv]</div>\", '<div class=\"textscroll\">  <div class=\"simsub\"><b>hoove(hu:v)</b>: n. 家畜的胃气胀\\\\n[医] 胃气胀(牛羊等)</div></div>', 'v.理应，有义务']\n",
      "[\"<div>boisterous</div><div>['bɔistərəs]</div>\", '<div class=\"textscroll\">  <div class=\"simsub\"><b>bois(bwɑ:)</b>: n. 木香, 本香</div> <div class=\"simsub\"><b>ister()</b>: n. 伊斯忒耳（希腊神话中的神）</div> <div class=\"simsub\"><b>rous(rajs)</b>: n. 劳斯（姓氏）</div> <div class=\"simsub\"><b>ster()</b>: n. 灭菌, 绝育；消毒器, 杀菌器</div></div>', 'a.喧闹的；猛烈的']\n",
      "['<div>brash</div><div>[bræʃ]</div>', '<div class=\"textscroll\"> <div class=\"sim2\">abash: v.使害羞，使尴尬</div> <div class=\"sim2\">base: a.卑鄙的</div> <div class=\"sim2\">bask: v.晒太阳，取暖</div> <div class=\"sim2\">blast: >n.一阵(大风)；冲击波；v.爆破；枯萎</div> <div class=\"sim2\">blush: v.因某事物脸红；n.因羞愧等脸上泛出的红晕</div> <div class=\"sim2\">boast: v./n.自夸</div> <div class=\"sim2\">brace: v.使稳固，架稳；n.支撑物</div> <div class=\"sim2\">brag: v.吹嘘</div> <div class=\"sim2\">braid: n.穗子；发辫；v.编成辫子</div> <div class=\"sim2\">brake: n.刹车；v.减速，阻止</div> <div class=\"sim2\">brand: n.商标；烙印；v.在某事物上打烙印</div> <div class=\"sim2\">brassy: a.厚脸皮的，无礼的</div> <div class=\"sim2\">brat: n.孩子；顽童</div> <div class=\"sim2\">brawl: v./n.争吵，打架</div> <div class=\"sim2\">breach: n.裂缝，缺口；v.打破，裂开；违背</div> <div class=\"sim2\">brisk: a.敏捷的，活泼的；清新健康的</div> <div class=\"sim2\">broach: v.开瓶；提出(讨论)</div> <div class=\"sim2\">clash: v.冲突，撞击</div> <div class=\"sim2\">crass: a.愚钝的，粗糙的</div> <div class=\"sim2\">erase: v.擦掉，抹去</div> <div class=\"sim2\">gash: n.深长的伤口，裂缝</div> <div class=\"sim2\">lash: n.鞭子；v.鞭打；捆住</div> <div class=\"sim2\">mash: v.捣成糊状</div> <div class=\"sim2\">quash: v.镇压；取消</div> <div class=\"sim2\">sash: n.肩带</div> <div class=\"sim2\">blase: adj. 厌倦享乐的，冷漠的</div> <div class=\"sim2\">bray: v. 大声而刺耳地发出（叫唤或声音）</div> <div class=\"sim2\">leash: n. (系狗的)绳子</div> <div class=\"sim2\">rasp: v. 发出刺耳的声音</div> <div class=\"sim2\">stash: v. 藏匿，隐藏</div> <div class=\"sim2\">thrash: v. 鞭打</div> <div class=\"sim2\">wrath: n. 愤怒，大怒<div><img src=\"paste-92c7906631b9f566763567990b2e07b70767cb5e.png\"><br></div></div> <div class=\"simsub\"><b>rash(ræʃ)</b>: a. 轻率的, 匆忙的, 鲁莽的\\\\nn. 皮疹</div></div>', 'a.性急的；无礼的']\n",
      "['<div>brood</div><div>[bru:d]</div>', '<div class=\"textscroll\"><div class=\"sim1\">brook: n.小河</div> <div class=\"sim2\">boom: n.繁荣昌盛时期；v.发出深沉有回响的声音</div> <div class=\"sim2\">boon: n.恩惠，天赐福利</div> <div class=\"sim2\">boor: n.举止粗野的人；乡下人</div> <div class=\"sim2\">braid: n.穗子；发辫；v.编成辫子</div> <div class=\"sim2\">brand: n.商标；烙印；v.在某事物上打烙印</div> <div class=\"sim2\">breed: v.繁殖；教养；n.品种，种类</div> <div class=\"sim2\">crook: v.使弯曲；n.钩状物</div> <div class=\"sim2\">droop: v.低垂；沮丧，萎靡</div> <div class=\"sim2\">frond: n.羊齿、棕榈等的叶子<div><img src=\"paste-645c614da50d522c7bd17bb495cfeda3d7d9dd69.png\"><br></div></div> <div class=\"sim2\">groom: n.马夫；新郎</div> <div class=\"sim2\">prod: v.刺，捅；激励</div> <div class=\"sim2\">boo: v. 作嘘声，嘘（某人）用嘘声表示不满、蔑视或反对</div> <div class=\"sim2\">broil: v. 烧烤</div> <div class=\"sim2\">brooch: n. 胸针</div> <div class=\"sim2\">croon: v. 低声歌唱</div> <div class=\"simsub\"><b>broo(bru:)</b>: [苏][北英]任何可饮用的清饮料(清汤, 果汁等)</div> <div class=\"simsub\"><b>rood(ru:d)</b>: n. 十字架, 十字架上的基督像</div></div>', 'n.一窝幼鸟；v.孵蛋；冥想']\n",
      "[\"<div>canard</div><div>[kæ'nɑ:d]</div>\", '<div class=\"textscroll\"><div class=\"sim1\">canary: n.金丝雀；女歌星</div> <div class=\"sim2\">candid: a.率直的</div> <div class=\"sim2\">coward: n.胆小鬼</div> <div class=\"sim2\">hazard: n.危险</div> <div class=\"simsub\"><b>anar()</b>: n. (Anar)人名；(土、阿塞、土库)阿纳尔</div> <div class=\"simsub\"><b>cana(\\'keinә)</b>: n. 迦南（巴勒斯坦北部的一个村庄）</div> <div class=\"simsub\"><b>nard(nɑ:d)</b>: n. 甘松, 甘松油膏\\\\n[医] 甘松香, 甘松</div></div>', 'n. 谣言，假新闻']\n",
      "[\"<div>candid</div><div>['kændid]</div>\", '<div class=\"textscroll\"> <div class=\"sim2\">candor: n.坦白，率直</div> <div class=\"sim2\">rancid: a.不新鲜的，变味的</div> <div class=\"sim2\">canard: n. 谣言，假新闻</div> <div class=\"simsub\"><b>andi()</b>: 安迪（名字）</div> <div class=\"simsub\"><b>cand()</b>: [化] 萤石</div> <div class=\"simsub\"><b>candi()</b>: n. (Candi)人名；(意)坎迪</div></div>', 'a.率直的']\n",
      "[\"<div>carillon</div><div>[kə'riljən]</div>\", '<div class=\"textscroll\">  <div class=\"simsub\"><b>aril(\\'æril)</b>: n. 假种皮\\\\n[医] 假种皮</div> <div class=\"simsub\"><b>illo()</b>:  [地名] [韩国] 一老; [地名] [尼日利亚] 伊洛</div> <div class=\"simsub\"><b>rill(ril)</b>: n. 小河, 细流, 小溪\\\\nvi. 潺潺流</div></div>', 'n. 编钟，钟琴']\n",
      "[\"<div>circumference</div><div>[sə'kʌmfərəns]</div>\", '<div class=\"textscroll\">  <div class=\"simsub\"><b>circ()</b>: [计] 广义圈</div> <div class=\"simsub\"><b>circum()</b>: 圆週; 週边</div> <div class=\"simsub\"><b>ence()</b>: 具...的行为,状态</div> <div class=\"simsub\"><b>fere(fɪə)</b>: n. 伴侣, 配偶</div> <div class=\"simsub\"><b>ferenc()</b>: n. (Ferenc)人名；(德、匈、罗、塞、波、捷)费伦茨</div> <div class=\"simsub\"><b>ference()</b>:  [人名] 费尔伦斯</div> <div class=\"simsub\"><b>rence(rens)</b>: 盲目降落区的障碍物间隔</div></div>', 'n.周围；圆周；周长']\n",
      "['<div>clique</div><div>[kli:k]</div>', '<div class=\"textscroll\"> <div class=\"sim2\">oblique: a.间接的；斜的</div> <div class=\"sim2\">pique: n./v.(因自尊心受伤害而导致的)不悦，愤怒；v.冒犯</div> <div class=\"sim2\">plaque: n.匾；血小板</div> <div class=\"sim2\">unique: a.独一无二的，独特的；无与伦比的</div> <div class=\"sim2\">cliche: adj. 陈腐的</div> </div>', 'n.朋党派系，小集团']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 141.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20250404(18:15:58)\n",
      "[main 90c231a] update cards.html on date 20250404(18:15:58)\n",
      " 14 files changed, 200 insertions(+), 200 deletions(-)\n",
      " create mode 100644 \"docs/images/220px-Anti-CPE_harangue_at_the_Assembl\\303\\251e_Nationale,_2006.jpg\"\n",
      " delete mode 100644 docs/images/images-5186a2c0d72bb6ced6ab1099107be5415e7b0709.jpg\n",
      " delete mode 100644 docs/images/images-b98b8556ba0a25e88d1579b2ffc16a79c139c395.jpg\n",
      " create mode 100644 docs/images/paste-269f42e29d2a01f624b5f937b600f96a34d81bbb.jpg\n",
      " delete mode 100644 docs/images/paste-4d6dcb69a7f61358ac70d067530f28f683cf7a2a.jpg\n",
      " delete mode 100644 docs/images/paste-62f481c52aa82b6eaa5daa5b73639b2703d21517.jpg\n",
      " delete mode 100644 docs/images/paste-874ee63383f0585352d353c95290c6159defebf7.jpg\n",
      " create mode 100644 docs/images/paste-90bc0f2e1ace5933848540588d0f9cfe6050a720.jpg\n",
      " delete mode 100644 docs/images/paste-b8739423b0a7ba9cfebee845f29554a1cafe1f47.jpg\n",
      " create mode 100644 docs/images/paste-c3cd5f28474ce9f22c35cd0a4f65bc8db068046f.png\n",
      " delete mode 100644 docs/images/paste-c6d9ef7b846c57cb5c1ef5adb746da2c61a00a84.png\n",
      " delete mode 100644 docs/images/paste-cfae8b5068b640cdbd4a2e2fbbe3dcdbc08a3ac6.jpg\n",
      " delete mode 100644 docs/images/paste-d09601d83066b3063f7e388121fa2fedee1d1c0f.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To github.com:pengfeigao2021/anki_html.git\n",
      " + 21b1cf2...90c231a main -> main (forced update)\n"
     ]
    }
   ],
   "source": [
    "# read sqlite3\n",
    "import sqlite3\n",
    "import anki\n",
    "from anki.collection import Collection\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import termcolor\n",
    "import tqdm\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "import pickle\n",
    "# import pandas as pd\n",
    "import editdistance\n",
    "\n",
    "EDICT_PATH = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data/ecdict.csv'\n",
    "MEDIA_FOLDER = '/Users/AlexG/Library/Application Support/Anki2/User 1/collection.media'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/MyPaperNotes.apkg'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/ivl10.apkg'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/极品GRE红宝书.apkg'\n",
    "DATA_PATH = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data'\n",
    "outdir = PKG_PATH + '-decompress'\n",
    "# outdir = '/Users/AlexG/Downloads/ivl10-decompress'\n",
    "# ANKI2_PATH = '/Users/AlexG/Downloads/ivl10-decompress/collection.anki2'\n",
    "ANKI21_PATH = outdir + '/collection.anki21'\n",
    "# EXPORT_PATH_WEBSITE = '/Users/AlexG/Documents/GitHub/typingpractices/website/onepager/onepager/templates/onepager/cards.html'\n",
    "EXPORT_PATH_WEBSITE = '/Users/AlexG/Documents/GitHub/anki_html/docs/gre_cards.html'\n",
    "\n",
    "def unzip_anki(pkg_file_path, outdir):\n",
    "    # Create the extract directory if it doesn't exist\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    # Unzip the .apkg file\n",
    "    with zipfile.ZipFile(PKG_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(outdir)\n",
    "\n",
    "    print(f\"Extracted files to {outdir}\")\n",
    "\n",
    "class EditDistanceFinder(object):\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "        self.load_cache()\n",
    "\n",
    "    def load_cache(self):\n",
    "        cache_path = os.path.join(DATA_PATH, 'edit_distance_cache.pkl')\n",
    "        if os.path.exists(cache_path):\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                self.cache = pickle.load(f)\n",
    "        else:\n",
    "            self.cache = {}\n",
    "\n",
    "    def save_cache(self):\n",
    "        cache_path = os.path.join(DATA_PATH, 'edit_distance_cache.pkl')\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump(self.cache, f)\n",
    "\n",
    "    def find(self, query, \n",
    "             edit_distance_lower=1, \n",
    "             edit_distance_upper=1,\n",
    "             html_class=\"sim1\"):\n",
    "        res = []\n",
    "\n",
    "        # find similar words with cache\n",
    "        similar_words = []\n",
    "        key = (query, edit_distance_lower, edit_distance_upper)\n",
    "        if key in self.cache:\n",
    "            # use cache\n",
    "            similar_words = self.cache[key]\n",
    "        else:\n",
    "            for w in self.words:\n",
    "                if w[0] == query:\n",
    "                    continue\n",
    "                d = editdistance.eval(w[0], query)\n",
    "                if d >= edit_distance_lower and d <= edit_distance_upper:\n",
    "                    similar_words.append((d, w[0], w[2]))\n",
    "            # save cache\n",
    "            self.cache[key] = similar_words\n",
    "        for w in similar_words:\n",
    "            text = f'<div class=\"{html_class}\">{w[1]}: {w[2]}</div>'\n",
    "            res.append(text)\n",
    "        \n",
    "        return ' '.join(res)\n",
    "\n",
    "\n",
    "class SubStrFinder(object):\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "        self.load_cache()\n",
    "\n",
    "    def load_cache(self):\n",
    "        cache_path = os.path.join(DATA_PATH, 'sub_word_cache.pkl')\n",
    "        if os.path.exists(cache_path):\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                self.cache = pickle.load(f)\n",
    "        else:\n",
    "            self.cache = {}\n",
    "\n",
    "    def save_cache(self):\n",
    "        cache_path = os.path.join(DATA_PATH, 'sub_word_cache.pkl')\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump(self.cache, f)\n",
    "\n",
    "\n",
    "    def load_edict(self, path):\n",
    "        total_lines = 0\n",
    "        with open(path, 'r') as f:\n",
    "            rd = csv.reader(f)\n",
    "            for cols in tqdm.tqdm(rd):\n",
    "                if len(cols[0]) < 4 or len(cols[3]) == 0:\n",
    "                    continue\n",
    "                if 'abbr.' in cols[3]:\n",
    "                    continue\n",
    "                if cols[0].isupper():\n",
    "                    continue\n",
    "                fields = (cols[0], cols[1], cols[3])\n",
    "                self.words.append(fields)\n",
    "                total_lines += 1\n",
    "        print(f\"Loaded {total_lines} lines from {path}\")\n",
    "\n",
    "    def find(self, query, html_class=\"simsub\"):\n",
    "        res = []\n",
    "        # print(\"Searching for {}...\".format(termcolor.colored(query, 'red')))\n",
    "        similar_words = []\n",
    "        key = query.lower()\n",
    "        if key in self.cache:\n",
    "            similar_words = self.cache[key]\n",
    "        else:\n",
    "            for w in self.words:\n",
    "                if w[0] == query:\n",
    "                    continue\n",
    "                if w[0].lower() in query.lower():\n",
    "                    similar_words.append(w)\n",
    "            self.cache[key] = similar_words\n",
    "\n",
    "        for w in similar_words:\n",
    "            text = f'<div class=\"{html_class}\"><b>{w[0]}({w[1]})</b>: {w[2]}</div>'\n",
    "            res.append(text)\n",
    "        \n",
    "        return ' '.join(res)\n",
    "\n",
    "class ImageSrcUrls(object):\n",
    "    def __init__(self):\n",
    "        self.purge_script_path = '/Users/AlexG/Documents/GitHub/anki_html/run_images.sh'\n",
    "        self.git_push_script_path = '/Users/AlexG/Documents/GitHub/anki_html/run_upload.sh'\n",
    "        self.source_image_folder = MEDIA_FOLDER\n",
    "        self.dest_image_folder = '/Users/AlexG/Documents/GitHub/anki_html/docs/images'\n",
    "        self.IMG_PAT = re.compile(r'<img\\b.*\\bsrc=\"')\n",
    "        self.SRC_PAT = re.compile(r'<img\\b.*\\bsrc=\"([^\"]+)\"')\n",
    "        self.srcs = []\n",
    "        self.exts = set()\n",
    "\n",
    "    def replace_image_url(self, text):\n",
    "        res = self.SRC_PAT.search(text)\n",
    "        if res:\n",
    "            for s in res.groups():\n",
    "                self.srcs.append(s)\n",
    "                ext = os.path.splitext(s)[1]\n",
    "                self.exts.add(ext)\n",
    "                print(termcolor.colored(s, 'red'))\n",
    "\n",
    "        return self.IMG_PAT.sub('<img src=\"./images/', text)\n",
    "    \n",
    "    def purge_images(self):\n",
    "        os.system(f\"bash {self.purge_script_path}\") \n",
    "\n",
    "    def git_push(self):\n",
    "        os.system(f\"bash {self.git_push_script_path}\")\n",
    "        \n",
    "    def move_images(self):\n",
    "        self.purge_images()\n",
    "        for src in tqdm.tqdm(self.srcs):\n",
    "            shutil.copyfile(\n",
    "                os.path.join(self.source_image_folder, src), \n",
    "                os.path.join(self.dest_image_folder, src)\n",
    "            )\n",
    "\n",
    "\n",
    "def format_cards_to_html(cards, collection, outpath, max_word_length=-1, max_cards=200):\n",
    "    data = []\n",
    "    image_src = ImageSrcUrls()\n",
    "    for cid in tqdm.tqdm(cards):\n",
    "        card = collection.get_card(cid)\n",
    "        note = card.note()\n",
    "        fields = note.fields\n",
    "        # fields[1] = fields[1][:20]\n",
    "        # fields = [field if len(field) <= max_text_length else field[:max_text_length] + '...' for field in fields]\n",
    "        data.append((card.ivl, fields))\n",
    "\n",
    "    edist_finder = EditDistanceFinder([fields for _, fields in data])\n",
    "    substr_finder = SubStrFinder([fields for _, fields in data])\n",
    "    substr_finder.load_edict(EDICT_PATH)\n",
    "    data.sort()\n",
    "\n",
    "    # format html table rows\n",
    "    html_table_rows = []\n",
    "    with tqdm.tqdm(total=max_cards) as pbar:\n",
    "        for fields in data:\n",
    "            if max_word_length == -1 or len(fields[1][0]) <= max_word_length:\n",
    "                if len(html_table_rows) >= max_cards:\n",
    "                    break\n",
    "                fields = [image_src.replace_image_url(field) for field in fields[1]]\n",
    "                html_table_rows.append(\n",
    "                    [\n",
    "                        \"<div>{}</div><div>{}</div>\".format(fields[0], fields[1]),\n",
    "                        '<div class=\"textscroll\">{} {} {}</div>'.format(\n",
    "                            edist_finder.find(fields[0]),\n",
    "                            edist_finder.find(\n",
    "                                fields[0],\n",
    "                                edit_distance_lower=2,\n",
    "                                edit_distance_upper=2,\n",
    "                                html_class=\"sim2\",\n",
    "                            ),\n",
    "                            substr_finder.find(fields[0]),\n",
    "                        ),\n",
    "                        fields[2],\n",
    "                    ]\n",
    "                )\n",
    "                # pbar.set_description('processing word: {}'.format(termcolor.colored(fields[0]), 'red'))\n",
    "                pbar.set_description('processing word: {}'.format(fields[0]))\n",
    "                pbar.update(1)\n",
    "\n",
    "    # data = [v for v in data if max_word_length == -1 or len(v[1][0]) <= max_word_length]\n",
    "    # data = [[image_src.replace_image_url(field) for field in fields]\n",
    "    #         for _, fields in data[:max_cards]]\n",
    "    # data = [['<div>{}</div><div>{}</div>'.format(fields[0], fields[1]), \n",
    "    #          '<div class=\"textscroll\">{} {} {}</div>'.format(\n",
    "    #             edist_finder.find(fields[0]),\n",
    "    #             edist_finder.find(fields[0],\n",
    "    #                               edit_distance_lower=2, \n",
    "    #                               edit_distance_upper=2,\n",
    "    #                               html_class=\"sim2\"),\n",
    "    #             substr_finder.find(fields[0]),\n",
    "    #          ),\n",
    "    #          fields[2], \n",
    "    #          ] for fields in tqdm.tqdm(data)]\n",
    "\n",
    "    data = html_table_rows\n",
    "    # save cache\n",
    "    edist_finder.save_cache()\n",
    "    substr_finder.save_cache()\n",
    "\n",
    "    print(image_src.exts)\n",
    "    print(termcolor.colored('Data samples:', 'blue'))\n",
    "    print('\\n'.join(['{}'.format(v) for v in data[:10]]))\n",
    "    image_src.move_images()\n",
    "    # with open(outpath+'.csv', 'w') as f:\n",
    "    #     wt = csv.writer(f)\n",
    "    #     wt.writerows([v[:3] for v in data])\n",
    "    rows = ['<tr><td class=\"indextd\">{}</td><td>{}</td></tr>\\n'.format(fi, '</td><td class=\"fixwtd\">'.join(field)) for fi, field in enumerate(data)]\n",
    "    html = f'''\n",
    "    <head>\n",
    "        <style>\n",
    "            body {{\n",
    "                background: #202020;\n",
    "            }}\n",
    "\n",
    "            table {{\n",
    "                background-color: #A0A0A0;\n",
    "                --color: #d0d0f5;\n",
    "                margin: 3em;\n",
    "            }}\n",
    "\n",
    "            thead,\n",
    "            tfoot {{\n",
    "                background: var(--color);\n",
    "            }}\n",
    "\n",
    "            tbody tr:nth-child(even) {{\n",
    "                background: color-mix(in srgb, var(--color), transparent 60%);\n",
    "            }}\n",
    "            div, p, td {{\n",
    "                width: 200px;\n",
    "                word-wrap: break-word;\n",
    "                white-space: normal;\n",
    "            }}\n",
    "            td img {{\n",
    "                max-height: 8em;\n",
    "            }}\n",
    "            .fixwtd {{\n",
    "                width: 200px;\n",
    "                word-wrap: break-word;\n",
    "                white-space: normal;\n",
    "            }}\n",
    "            .indextd {{\n",
    "                width: 10px;\n",
    "                word-wrap: break-word;\n",
    "                white-space: normal;\n",
    "            }}\n",
    "            td {{\n",
    "                border: 1px ridge #333333;\n",
    "            }}\n",
    "            img {{\n",
    "                max-height: 5em;\n",
    "                max-width: 5em;\n",
    "            }}\n",
    "            .sim1 {{\n",
    "                width: 26em;\n",
    "                background-color: #eadcb7;\n",
    "            }}\n",
    "            .sim2 {{\n",
    "                width: 26em;\n",
    "                background-color: #e99a06;\n",
    "            }}\n",
    "            .simsub {{\n",
    "                width: 26em;\n",
    "                background-color: #afcd95;\n",
    "            }}\n",
    "            b {{\n",
    "                background-color: #f1c40f;\n",
    "            }}\n",
    "            .textscroll {{\n",
    "                /* height: 3em; */\n",
    "                width: 27em;\n",
    "                border: 1px solid #ccc;\n",
    "                overflow: auto;\n",
    "            }}\n",
    "        </style>\n",
    "        <link rel=\"stylesheet\" href=\"css/fancy-button.css\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <button onclick=\"changeWidth(0.5)\" class=\"button-74\">Half Column Width</button>\n",
    "        <button onclick=\"hideRows(10)\" class=\"button-73 fix-width-button\">Hide Rows</button>\n",
    "        <table id=\"cards\">\n",
    "          {\"\".join(rows)}\n",
    "        </table>\n",
    "        <script src=\"js/change-width.js\"></script>\n",
    "        <script src=\"js/hide-rows.js\"></script>\n",
    "    </body>\n",
    "    '''\n",
    "    with open(outpath, 'w') as f:\n",
    "        f.write(html)\n",
    "\n",
    "    # git push\n",
    "    image_src.git_push()\n",
    "\n",
    "try:\n",
    "    col = Collection(ANKI21_PATH)\n",
    "    print(dir(col))\n",
    "    print('total card count: ', col.card_count())\n",
    "    # print('note count: ', col.node_count())\n",
    "\n",
    "    # query = 'prop:ivl<=30'\n",
    "    query = ''\n",
    "    cards = col.find_cards(query)\n",
    "    print(termcolor.colored(f'query {query} find {len(cards)} cards', 'green'))\n",
    "\n",
    "    outpath = os.path.join(outdir, 'cards.html')\n",
    "    # format_cards_to_html(cards, col, outpath)\n",
    "    format_cards_to_html(cards, col, EXPORT_PATH_WEBSITE,\n",
    "                         max_word_length=-1)\n",
    "\n",
    "    # Fetch all notes (cards content) in the collection\n",
    "    # print(\"\\nNotes in collection:\")\n",
    "    # for note_id in col.db.list(\"select id from notes\"):\n",
    "    #     note = col.get_note(note_id)\n",
    "    #     fields = note.fields\n",
    "    #     print(f\"Note ID: {note_id}, Fields: {fields}\")\n",
    "\n",
    "    # # Fetch all cards (metadata) in the collection\n",
    "    # print(\"\\nCards in collection:\")\n",
    "    # for card_id in col.db.list(\"select id from cards\"):\n",
    "    #     card = col.get_card(card_id)\n",
    "    #     print(f\"Card ID: {card.id}, Note ID: {card.nid}, Deck ID: {card.did}, Type: {card.type}\")\n",
    "\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Caught an exception: {e}\")\n",
    "    import traceback\n",
    "    print(traceback.format_exc())\n",
    "finally:\n",
    "    # Close the collection\n",
    "    col.close()\n",
    "\n",
    "col.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------\n",
    "# Offline batch similar words\n",
    "# -------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read sqlite3\n",
    "import sqlite3\n",
    "import anki\n",
    "from anki.collection import Collection\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import termcolor\n",
    "import tqdm\n",
    "import csv\n",
    "import shutil\n",
    "# import pandas as pd\n",
    "import editdistance\n",
    "\n",
    "EDICT_PATH = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data/ecdict.csv'\n",
    "MEDIA_FOLDER = '/Users/AlexG/Library/Application Support/Anki2/User 1/collection.media'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/MyPaperNotes.apkg'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/ivl10.apkg'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/极品GRE红宝书.apkg'\n",
    "outdir = PKG_PATH + '-decompress'\n",
    "# outdir = '/Users/AlexG/Downloads/ivl10-decompress'\n",
    "# ANKI2_PATH = '/Users/AlexG/Downloads/ivl10-decompress/collection.anki2'\n",
    "ANKI21_PATH = outdir + '/collection.anki21'\n",
    "# EXPORT_PATH_WEBSITE = '/Users/AlexG/Documents/GitHub/typingpractices/website/onepager/onepager/templates/onepager/cards.html'\n",
    "EXPORT_PATH_WEBSITE = '/Users/AlexG/Documents/GitHub/anki_html/docs/gre_cards.html'\n",
    "\n",
    "def unzip_anki(pkg_file_path, outdir):\n",
    "    # Create the extract directory if it doesn't exist\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    # Unzip the .apkg file\n",
    "    with zipfile.ZipFile(PKG_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(outdir)\n",
    "\n",
    "    print(f\"Extracted files to {outdir}\")\n",
    "\n",
    "class EditDistanceFinder(object):\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "\n",
    "    def find(self, query, \n",
    "             edit_distance_lower=1, \n",
    "             edit_distance_upper=1,\n",
    "             html_class=\"sim1\"):\n",
    "        res = []\n",
    "        for w in self.words:\n",
    "            if w[0] == query:\n",
    "                continue\n",
    "            d = editdistance.eval(w[0], query)\n",
    "            if d >= edit_distance_lower and d <= edit_distance_upper:\n",
    "                text = f'<div class=\"{html_class}\">{w[0]}: {w[2]}</div>'\n",
    "                res.append(text)\n",
    "        \n",
    "        return ' '.join(res)\n",
    "            \n",
    "        \n",
    "class SubStrFinder(object):\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "\n",
    "    def load_edict(self, path):\n",
    "        total_lines = 0\n",
    "        with open(path, 'r') as f:\n",
    "            rd = csv.reader(f)\n",
    "            for cols in tqdm.tqdm(rd):\n",
    "                if len(cols[0]) < 4 or len(cols[3]) == 0:\n",
    "                    continue\n",
    "                if 'abbr.' in cols[3]:\n",
    "                    continue\n",
    "                if cols[0].isupper():\n",
    "                    continue\n",
    "                fields = (cols[0], cols[1], cols[3])\n",
    "                self.words.append(fields)\n",
    "                total_lines += 1\n",
    "        print(f\"Loaded {total_lines} lines from {path}\")\n",
    "\n",
    "    def find(self, query, html_class=\"simsub\"):\n",
    "        res = []\n",
    "        print(\"Searching for {}...\".format(termcolor.colored(query, 'red')))\n",
    "        for w in tqdm.tqdm(self.words):\n",
    "            if w[0] == query:\n",
    "                continue\n",
    "            if w[0].lower() in query.lower():\n",
    "                text = f'<div class=\"{html_class}\"><b>{w[0]}({w[1]})</b>: {w[2]}</div>'\n",
    "                res.append(text)\n",
    "        \n",
    "        return ' '.join(res)\n",
    "\n",
    "class ImageSrcUrls(object):\n",
    "    def __init__(self):\n",
    "        self.purge_script_path = '/Users/AlexG/Documents/GitHub/anki_html/run_images.sh'\n",
    "        self.git_push_script_path = '/Users/AlexG/Documents/GitHub/anki_html/run_upload.sh'\n",
    "        self.source_image_folder = MEDIA_FOLDER\n",
    "        self.dest_image_folder = '/Users/AlexG/Documents/GitHub/anki_html/docs/images'\n",
    "        self.IMG_PAT = re.compile(r'<img\\b.*\\bsrc=\"')\n",
    "        self.SRC_PAT = re.compile(r'<img\\b.*\\bsrc=\"([^\"]+)\"')\n",
    "        self.srcs = []\n",
    "        self.exts = set()\n",
    "\n",
    "    def replace_image_url(self, text):\n",
    "        res = self.SRC_PAT.search(text)\n",
    "        if res:\n",
    "            for s in res.groups():\n",
    "                self.srcs.append(s)\n",
    "                ext = os.path.splitext(s)[1]\n",
    "                self.exts.add(ext)\n",
    "                print(termcolor.colored(s, 'red'))\n",
    "\n",
    "        return self.IMG_PAT.sub('<img src=\"./images/', text)\n",
    "    \n",
    "    def purge_images(self):\n",
    "        os.system(f\"bash {self.purge_script_path}\") \n",
    "\n",
    "    def git_push(self):\n",
    "        os.system(f\"bash {self.git_push_script_path}\")\n",
    "        \n",
    "    def move_images(self):\n",
    "        self.purge_images()\n",
    "        for src in tqdm.tqdm(self.srcs):\n",
    "            shutil.copyfile(\n",
    "                os.path.join(self.source_image_folder, src), \n",
    "                os.path.join(self.dest_image_folder, src)\n",
    "            )\n",
    "\n",
    "\n",
    "def format_cards_to_html(cards, collection, outpath, max_word_length=-1, max_cards=200):\n",
    "    data = []\n",
    "    image_src = ImageSrcUrls()\n",
    "    for cid in tqdm.tqdm(cards):\n",
    "        card = collection.get_card(cid)\n",
    "        note = card.note()\n",
    "        fields = note.fields\n",
    "        # fields[1] = fields[1][:20]\n",
    "        # fields = [field if len(field) <= max_text_length else field[:max_text_length] + '...' for field in fields]\n",
    "        data.append((card.ivl, fields))\n",
    "\n",
    "    edist_finder = EditDistanceFinder([fields for _, fields in data])\n",
    "    substr_finder = SubStrFinder([fields for _, fields in data])\n",
    "    substr_finder.load_edict(EDICT_PATH)\n",
    "    data.sort()\n",
    "    data = [v for v in data if max_word_length == -1 or len(v[1][0]) <= max_word_length]\n",
    "    data = [[image_src.replace_image_url(field) for field in fields] \n",
    "            for _, fields in data[:max_cards]]\n",
    "    data = [['<div>{}</div><div>{}</div>'.format(fields[0], fields[1]), \n",
    "             '<div class=\"textscroll\">{} {} {}</div>'.format(\n",
    "                edist_finder.find(fields[0]),\n",
    "                edist_finder.find(fields[0],\n",
    "                                  edit_distance_lower=2, \n",
    "                                  edit_distance_upper=2,\n",
    "                                  html_class=\"sim2\"),\n",
    "                substr_finder.find(fields[0]),\n",
    "             ),\n",
    "             fields[2], \n",
    "             ] for fields in data]\n",
    "    print(image_src.exts)\n",
    "    print(termcolor.colored('Data samples:', 'blue'))\n",
    "    print('\\n'.join(['{}'.format(v) for v in data[:10]]))\n",
    "\n",
    "unzip_anki(PKG_PATH, outdir)\n",
    "col = Collection(ANKI21_PATH)\n",
    "# Fetch all decks in the collection\n",
    "decks = col.decks.all()\n",
    "print(\"Decks in collection:\")\n",
    "for deck in decks:\n",
    "    print(f\"Deck ID: {deck['id']}, Name: {deck['name']}\")\n",
    "\n",
    "try:\n",
    "    print(dir(col))\n",
    "    print('total card count: ', col.card_count())\n",
    "    # print('note count: ', col.node_count())\n",
    "\n",
    "    # query = 'prop:ivl<=30'\n",
    "    query = ''\n",
    "    cards = col.find_cards(query)\n",
    "    print(termcolor.colored(f'query {query} find {len(cards)} cards', 'green'))\n",
    "\n",
    "    outpath = os.path.join(outdir, 'cards.html')\n",
    "    # format_cards_to_html(cards, col, outpath)\n",
    "    format_cards_to_html(cards, col, EXPORT_PATH_WEBSITE,\n",
    "                         max_word_length=-1)\n",
    "\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    # Close the collection\n",
    "    col.close()\n",
    "\n",
    "col.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====================\n",
    "# build all words list\n",
    "# ====================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decks in collection:\n",
      "Deck ID: 1, Name: Default\n",
      "Deck ID: 1685695741424, Name: 极品GRE红宝书\n",
      "total card count:  7513\n",
      "\u001b[32mquery  find 7513 cards\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7513/7513 [00:02<00:00, 3308.24it/s]\n",
      "770612it [00:03, 229250.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 750698 lines from /Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data/ecdict.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750698/750698 [00:00<00:00, 1021786.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 758211\n"
     ]
    }
   ],
   "source": [
    "# read sqlite3\n",
    "import sqlite3\n",
    "import anki\n",
    "from anki.collection import Collection\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import termcolor\n",
    "import tqdm\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "# import pandas as pd\n",
    "import editdistance\n",
    "\n",
    "EDICT_PATH = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data/ecdict.csv'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/极品GRE红宝书.apkg'\n",
    "outdir = PKG_PATH + '-decompress'\n",
    "ANKI21_PATH = outdir + '/collection.anki21'\n",
    "\n",
    "class SubStrFinder(object):\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "\n",
    "    def load_edict(self, path):\n",
    "        total_lines = 0\n",
    "        with open(path, 'r') as f:\n",
    "            rd = csv.reader(f)\n",
    "            for cols in tqdm.tqdm(rd):\n",
    "                if len(cols[0]) < 4 or len(cols[3]) == 0:\n",
    "                    continue\n",
    "                if 'abbr.' in cols[3]:\n",
    "                    continue\n",
    "                if cols[0].isupper():\n",
    "                    continue\n",
    "                fields = (cols[0], cols[1], cols[3])\n",
    "                self.words.append(fields)\n",
    "                total_lines += 1\n",
    "        print(f\"Loaded {total_lines} lines from {path}\")\n",
    "\n",
    "\n",
    "col = Collection(ANKI21_PATH)\n",
    "# Fetch all decks in the collection\n",
    "decks = col.decks.all()\n",
    "all_words_set = set()\n",
    "all_words = []\n",
    "gre_words = []\n",
    "print(\"Decks in collection:\")\n",
    "for deck in decks:\n",
    "    print(f\"Deck ID: {deck['id']}, Name: {deck['name']}\")\n",
    "\n",
    "try:\n",
    "    print('total card count: ', col.card_count())\n",
    "\n",
    "    query = ''\n",
    "    cards = col.find_cards(query)\n",
    "    print(termcolor.colored(f'query {query} find {len(cards)} cards', 'green'))\n",
    "\n",
    "    for cid in tqdm.tqdm(cards):\n",
    "        card = col.get_card(cid)\n",
    "        note = card.note()\n",
    "        fields = note.fields\n",
    "        all_words_set.add(fields[0])\n",
    "        gre_words.append(fields[0])\n",
    "    substr_finder = SubStrFinder([])\n",
    "    substr_finder.load_edict(EDICT_PATH)\n",
    "    for w in tqdm.tqdm(substr_finder.words):\n",
    "        all_words_set.add(w)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    # Close the collection\n",
    "    col.close()\n",
    "\n",
    "col.close()\n",
    "\n",
    "all_words = list(all_words_set)\n",
    "print(f\"Total words: {len(all_words)}\")\n",
    "with open('/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/all_words.json', 'w') as f:\n",
    "    json.dump(all_words, f)\n",
    "with open('/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/gre_words.json', 'w') as f:\n",
    "    json.dump(gre_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from /Users/AlexG/Documents/GitHub/mindmaps/anki_tools/gre_words.json and /Users/AlexG/Documents/GitHub/mindmaps/anki_tools/all_words.json\n",
      "loaded 7513 gre words and 758211 all words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 152303449/5696439243 [01:34<57:08, 1616872.64it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m N_gre \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(gre_words)\n\u001b[1;32m     40\u001b[0m N_all \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_words)\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n1, n2 \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(itertools\u001b[38;5;241m.\u001b[39mproduct(\u001b[38;5;28mrange\u001b[39m(N_gre), \u001b[38;5;28mrange\u001b[39m(N_all)), total\u001b[38;5;241m=\u001b[39mN_gre \u001b[38;5;241m*\u001b[39m N_all):\n\u001b[1;32m     42\u001b[0m     w1 \u001b[38;5;241m=\u001b[39m gre_words[n1][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     43\u001b[0m     w2 \u001b[38;5;241m=\u001b[39m all_words[n2][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/tqdm/std.py:1187\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m n \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlast_print_n\u001b[49m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminiters:\n\u001b[1;32m   1188\u001b[0m     cur_t \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m   1189\u001b[0m     dt \u001b[38;5;241m=\u001b[39m cur_t \u001b[38;5;241m-\u001b[39m last_print_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "# read sqlite3\n",
    "import sqlite3\n",
    "import anki\n",
    "from anki.collection import Collection\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import termcolor\n",
    "import tqdm\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "# import pandas as pd\n",
    "import editdistance\n",
    "# a = [1,2,3]\n",
    "# b= itertools.product(a, a)\n",
    "# print(list(b))\n",
    "\n",
    "gre_words_path = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/gre_words.json'\n",
    "all_words_path = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/all_words.json'\n",
    "print('loading from {} and {}'.format(gre_words_path, all_words_path))\n",
    "gre_words = json.load(open(gre_words_path))\n",
    "all_words = json.load(open(all_words_path))\n",
    "print('loaded {} gre words and {} all words'.format(len(gre_words), len(all_words)))\n",
    "\n",
    "def find(self, query, html_class=\"simsub\"):\n",
    "    res = []\n",
    "    print(\"Searching for {}...\".format(termcolor.colored(query, 'red')))\n",
    "    for w in tqdm.tqdm(self.words):\n",
    "        if w[0] == query:\n",
    "            continue\n",
    "        if w[0].lower() in query.lower():\n",
    "            text = f'<div class=\"{html_class}\"><b>{w[0]}({w[1]})</b>: {w[2]}</div>'\n",
    "            res.append(text)\n",
    "    \n",
    "    return ' '.join(res)\n",
    "\n",
    "N_gre = len(gre_words)\n",
    "N_all = len(all_words)\n",
    "for n1, n2 in tqdm.tqdm(itertools.product(range(N_gre), range(N_all)), total=N_gre * N_all):\n",
    "    w1 = gre_words[n1][0]\n",
    "    w2 = all_words[n2][0]\n",
    "    # d = editdistance.eval(w1, w2)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
