{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted files to /Users/AlexG/Downloads/极品GRE红宝书.apkg-decompress\n",
      "Decks in collection:\n",
      "Deck ID: 1685695741424, Name: 极品GRE红宝书\n",
      "Deck ID: 1, Name: Default\n"
     ]
    }
   ],
   "source": [
    "# extract package\n",
    "import sqlite3\n",
    "import anki\n",
    "from anki.collection import Collection\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import termcolor\n",
    "import tqdm\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "import pickle\n",
    "# import pandas as pd\n",
    "import editdistance\n",
    "\n",
    "EDICT_PATH = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data/ecdict.csv'\n",
    "MEDIA_FOLDER = '/Users/AlexG/Library/Application Support/Anki2/User 1/collection.media'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/MyPaperNotes.apkg'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/ivl10.apkg'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/极品GRE红宝书.apkg'\n",
    "DATA_PATH = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data'\n",
    "outdir = PKG_PATH + '-decompress'\n",
    "# outdir = '/Users/AlexG/Downloads/ivl10-decompress'\n",
    "# ANKI2_PATH = '/Users/AlexG/Downloads/ivl10-decompress/collection.anki2'\n",
    "ANKI21_PATH = outdir + '/collection.anki21'\n",
    "# EXPORT_PATH_WEBSITE = '/Users/AlexG/Documents/GitHub/typingpractices/website/onepager/onepager/templates/onepager/cards.html'\n",
    "EXPORT_PATH_WEBSITE = '/Users/AlexG/Documents/GitHub/anki_html/docs/gre_cards.html'\n",
    "\n",
    "def unzip_anki(pkg_file_path, outdir):\n",
    "    # Create the extract directory if it doesn't exist\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    # Unzip the .apkg file\n",
    "    with zipfile.ZipFile(PKG_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(outdir)\n",
    "\n",
    "    print(f\"Extracted files to {outdir}\")\n",
    "\n",
    "try:\n",
    "    unzip_anki(PKG_PATH, outdir)\n",
    "    col = Collection(ANKI21_PATH)\n",
    "    # Fetch all decks in the collection\n",
    "    decks = col.decks.all()\n",
    "    print(\"Decks in collection:\")\n",
    "    for deck in decks:\n",
    "        print(f\"Deck ID: {deck['id']}, Name: {deck['name']}\")\n",
    "except Exception as e:\n",
    "    error = '{}'.format(e)\n",
    "    print('exceptin while extracting apkg: {}...'.format(e[:12]))\n",
    "\n",
    "col.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_backend', '_build_sort_mode', '_check_backend_undo_status', '_clear_caches', '_deprecated_aliases', '_deprecated_attributes', '_load_scheduler', '_logRem', '_pb_search_separator', '_remNotes', '_startReps', '_supported_scheduler_versions', 'abort_media_sync', 'abort_sync', 'addNote', 'add_custom_undo_entry', 'add_image_occlusion_note', 'add_image_occlusion_notetype', 'add_note', 'add_notes', 'after_note_updates', 'all_browser_columns', 'all_config', 'autosave', 'await_backup_completion', 'backend', 'browser_row_for_id', 'build_search_string', 'cardStats', 'card_count', 'card_ids_of_note', 'card_stats', 'card_stats_data', 'close', 'close_for_full_sync', 'compare_answer', 'compute_memory_state', 'conf', 'create_backup', 'crt', 'db', 'decks', 'default_deck_for_notetype', 'defaults_for_adding', 'emptyCids', 'export_anki_package', 'export_card_csv', 'export_collection_package', 'export_note_csv', 'extract_cloze_for_typing', 'field_names_for_note_ids', 'find_and_replace', 'find_cards', 'find_dupes', 'find_notes', 'fix_integrity', 'flush', 'format_timespan', 'full_upload_or_download', 'fuzz_delta', 'genCards', 'get_aux_notetype_config', 'get_aux_template_config', 'get_browser_column', 'get_card', 'get_config', 'get_config_bool', 'get_config_string', 'get_csv_metadata', 'get_empty_cards', 'get_image_for_occlusion', 'get_image_occlusion_note', 'get_note', 'get_preferences', 'group_searches', 'i18n_resources', 'import_anki_package', 'import_csv', 'import_json_file', 'import_json_string', 'initialize_backend_logging', 'is_empty', 'join_searches', 'latest_progress', 'load_browser_card_columns', 'load_browser_note_columns', 'log', 'media', 'media_sync_status', 'merge_undo_entries', 'mod', 'mod_schema', 'models', 'name', 'newNote', 'new_note', 'nextID', 'note_count', 'op_made_changes', 'optimize', 'path', 'redo', 'register_deprecated_aliases', 'register_deprecated_attributes', 'remNotes', 'remove_cards_and_orphaned_notes', 'remove_config', 'remove_notes', 'remove_notes_by_card', 'render_markdown', 'reopen', 'replace_in_search_node', 'reset', 'save', 'sched', 'sched_ver', 'schema_changed', 'server', 'setMod', 'set_aux_notetype_config', 'set_aux_template_config', 'set_browser_card_columns', 'set_browser_note_columns', 'set_config', 'set_config_bool', 'set_config_string', 'set_deck', 'set_preferences', 'set_schema_modified', 'set_user_flag_for_cards', 'set_v3_scheduler', 'set_wants_abort', 'startTimebox', 'stats', 'studied_today', 'sync_collection', 'sync_login', 'sync_media', 'sync_status', 'tags', 'timeboxReached', 'tr', 'undo', 'undo_name', 'undo_status', 'updateFieldCache', 'update_card', 'update_cards', 'update_image_occlusion_note', 'update_note', 'update_notes', 'upgrade_to_v2_scheduler', 'usn', 'v3_scheduler', 'weakref']\n",
      "total card count:  7513\n",
      "\u001b[32mquery  find 7513 cards\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7513/7513 [00:02<00:00, 2987.17it/s]\n",
      "770612it [00:03, 245885.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 750698 lines from /Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data/ecdict.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing word: abstinent:  37%|███▋      | 74/200 [00:00<00:00, 294.86it/s]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mpaste-d09601d83066b3063f7e388121fa2fedee1d1c0f.jpg\u001b[0m\n",
      "\u001b[31mpaste-c6d9ef7b846c57cb5c1ef5adb746da2c61a00a84.png\u001b[0m\n",
      "\u001b[31mpaste-b8739423b0a7ba9cfebee845f29554a1cafe1f47.jpg\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing word: braid:  44%|████▍     | 88/200 [00:00<00:00, 394.48it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mGuinea-asks-bauxite-miners-to-present-local-refinery-plans-by-May.jpeg\u001b[0m\n",
      "\u001b[31mpaste-9c4c03b0a6345aaa754b5767e49078d8f7af2fe2.png\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing word: commonwealth:  50%|█████     | 101/200 [00:00<00:00, 394.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mpaste-cfae8b5068b640cdbd4a2e2fbbe3dcdbc08a3ac6.jpg\u001b[0m\n",
      "\u001b[31mimages-b98b8556ba0a25e88d1579b2ffc16a79c139c395.jpg\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing word: inured: 100%|██████████| 200/200 [00:00<00:00, 271.12it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mpaste-6ba10c81153adc71fe825e59ebf26ff0f720307c.png\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ls: images: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.jpg', '.jpeg', '.png'}\n",
      "\u001b[34mData samples:\u001b[0m\n",
      "[\"<div>ancillary</div><div>[æn'siləri]</div>\", '<div class=\"textscroll\">  <div class=\"simsub\"><b>ancilla(æn\\'silә)</b>: n. 附属物, 女仆人</div> <div class=\"simsub\"><b>cill(sɪl)</b>:  [法]窗台; 岩床; 基石; 门槛</div> <div class=\"simsub\"><b>Cilla()</b>: n. (Cilla)人名；(意)奇拉；(西)西利亚</div> <div class=\"simsub\"><b>lary()</b>: n. 天琴座；拉里（男子名）</div></div>', 'a.辅助的；n.助手']\n",
      "[\"<div>appellation</div><div>[`æpe'leiʃən]</div>\", '<div class=\"textscroll\">  <div class=\"simsub\"><b>appel(ɑ:\\'pel)</b>: n. 垫步</div> <div class=\"simsub\"><b>appella()</b>: [医] 阿配拉(苹果精粉)</div> <div class=\"simsub\"><b>ation(eɪʃən)</b>:  增强辐射, 文明阶段</div> <div class=\"simsub\"><b>ella(\\'elә)</b>: n. 艾拉（女子名）</div> <div class=\"simsub\"><b>lati(\\'læti. \\'lɑ:ti)</b>: n. （lat的复数）1940年前拉脱维亚（Latvia）的货币单位；（紫檀状）双雄苏木</div> <div class=\"simsub\"><b>lation()</b>: n. 副调制</div> <div class=\"simsub\"><b>pell(pel)</b>: n. 一卷羊皮纸</div> <div class=\"simsub\"><b>pella()</b>: 斗篷</div> <div class=\"simsub\"><b>tion()</b>: n. 象征式互动；支票电托收</div></div>', 'n.名称，称呼']\n",
      "[\"<div>bauble</div><div>['bɔ:bl]</div>\", '<div class=\"textscroll\"><div class=\"sim1\">babble: v.胡言乱语；牙牙学语；喋喋不休</div> <div class=\"sim2\">amble: n./v.漫步，缓行</div> <div class=\"sim2\">bale: n.大包裹；灾祸，不幸</div> <div class=\"sim2\">dabble: v.涉足，浅赏</div> <div class=\"sim2\">gamble: v./n.赌博；孤注一掷</div> <div class=\"sim2\">garble: v.曲解，窜改</div> <div class=\"sim2\">marble: n.大理石</div> <div class=\"sim2\">rabble: n.乌合之众；下等人</div> <div class=\"sim2\">ramble: n./v.漫步</div> <div class=\"sim2\">table: v.搁置，不加考虑</div> <div class=\"sim2\">baffle: v. 使困惑，难倒</div> <div class=\"sim2\">bubble: v. 起泡；n. 气泡，水泡</div> <div class=\"sim2\">bumble: v. 说话含糊；拙劣地做</div> <div class=\"sim2\">gabble: v. 急促而不清楚地说</div> <div class=\"sim2\">warble: v. (尤指鸟)叫出柔和的颤音<img src=\"paste-366aa796038107e781c388a35eb0f23659560db6.jpg\"></div> </div>', 'n. 花哨的小玩意儿；没价值的东西']\n",
      "[\"<div>canard</div><div>[kæ'nɑ:d]</div>\", '<div class=\"textscroll\"><div class=\"sim1\">canary: n.金丝雀；女歌星</div> <div class=\"sim2\">candid: a.率直的</div> <div class=\"sim2\">coward: n.胆小鬼</div> <div class=\"sim2\">hazard: n.危险</div> <div class=\"simsub\"><b>anar()</b>: n. (Anar)人名；(土、阿塞、土库)阿纳尔</div> <div class=\"simsub\"><b>cana(\\'keinә)</b>: n. 迦南（巴勒斯坦北部的一个村庄）</div> <div class=\"simsub\"><b>nard(nɑ:d)</b>: n. 甘松, 甘松油膏\\\\n[医] 甘松香, 甘松</div></div>', 'n. 谣言，假新闻']\n",
      "[\"<div>candid</div><div>['kændid]</div>\", '<div class=\"textscroll\"> <div class=\"sim2\">candor: n.坦白，率直</div> <div class=\"sim2\">rancid: a.不新鲜的，变味的</div> <div class=\"sim2\">canard: n. 谣言，假新闻</div> <div class=\"simsub\"><b>andi()</b>: 安迪（名字）</div> <div class=\"simsub\"><b>cand()</b>: [化] 萤石</div> <div class=\"simsub\"><b>candi()</b>: n. (Candi)人名；(意)坎迪</div></div>', 'a.率直的']\n",
      "[\"<div>checkered</div><div>['tʃekəd]</div>\", '<div class=\"textscroll\">  <div class=\"simsub\"><b>check([tʃek])</b>: v.使突然停止，阻止</div> <div class=\"simsub\"><b>check(tʃek)</b>: n. 检查, 支票, 阻止物, 寄物牌, 象棋中将军\\\\nvt. 检查, 阻止, 核对, 寄存, 托运\\\\nvi. 逐项相符, 开支票\\\\n[计] 复选</div> <div class=\"simsub\"><b>checker(\\'tʃekә)</b>: n. 制止者, 查对者, 阻止者\\\\n[计] 检查程序, 检验程序, 检验器, 西洋跳棋</div> <div class=\"simsub\"><b>ecker()</b>:  [人名] 埃克</div> <div class=\"simsub\"><b>heck(hek)</b>: int. 真见鬼（hell的委婉说法）</div> <div class=\"simsub\"><b>hecker()</b>:  [人名] 赫克</div> <div class=\"simsub\"><b>kere()</b>: n. (Kere)人名；(芬)凯雷</div></div>', 'adj. 盛衰无常的']\n",
      "[\"<div>clairvoyance</div><div>[klɛə'vɔiəns]</div>\", '<div class=\"textscroll\"> <div class=\"sim2\">clairvoyant: adj. 透视的，有洞察力的</div> <div class=\"simsub\"><b>lair([lɛə])</b>: n.野兽的巢穴；躲藏处</div> <div class=\"simsub\"><b>ance(eins)</b>: n. 状态；性质；情况</div> <div class=\"simsub\"><b>clai()</b>: n. (Clai)人名；(意)克拉伊</div> <div class=\"simsub\"><b>lair(lєә)</b>: n. 兽穴, 兽窝, 泥潭\\\\nvi. 进入兽穴, 在泥中打滚\\\\nvt. 放于穴中, 使陷入泥潭</div> <div class=\"simsub\"><b>voya()</b>:  [地名] [俄罗斯] 沃亚河</div> <div class=\"simsub\"><b>yance()</b>:  [地名] [古巴] 扬塞</div></div>', 'n.超人的洞察力']\n",
      "[\"<div>concoct</div><div>[kən'kɔkt]</div>\", '<div class=\"textscroll\"> <div class=\"sim2\">conceit: n.自负，自大</div> <div class=\"sim2\">concord: n.和睦；公约</div> <div class=\"sim2\">conduct: n.品德，行为；v.领导，引导</div> <div class=\"sim2\">contact: n./v.接触；互通信息</div> <div class=\"sim2\">contort: v.(使)扭曲(deform)；曲解</div> <div class=\"sim2\">convict: v.定罪；n.罪犯</div> <div class=\"sim2\">consort: v. 结交，配对；n. 配偶</div> <div class=\"simsub\"><b>conc(\\'kɒŋk)</b>:  [医][=concentrated]集中起来的, 经浓缩的</div> <div class=\"simsub\"><b>onco(\\'ɒŋkə)</b>:  [医]肿瘤的</div></div>', 'v. 调制；捏造']\n",
      "[\"<div>congregate</div><div>['kɔŋgrigeit]</div>\", '<div class=\"textscroll\">  <div class=\"simsub\"><b>gate(geit)</b>: n. 门, 牌楼, 大门, 通道, 闸\\\\nvt. 装门于\\\\n[计] 门; 栅</div> <div class=\"simsub\"><b>greg(greg)</b>: n. 格雷格（男子名, 等于Gregory）</div> <div class=\"simsub\"><b>rega()</b>: n. 君子（英国音响品牌）；君威（别克的汽车名）</div></div>', 'v.聚集，集合']\n",
      "[\"<div>consonance</div><div>['kɔnsənəns]</div>\", '<div class=\"textscroll\"> <div class=\"sim2\">consonant: a.调和的，一致的</div> <div class=\"simsub\"><b>ance(eins)</b>: n. 状态；性质；情况</div> <div class=\"simsub\"><b>nanc()</b>:  [医][=non-adrenergic,non-cholinergic]非肾上腺素能，非胆碱能</div> <div class=\"simsub\"><b>nance(\\'nɑ:nsei)</b>: n. 女人气的男人, 搞同性关系的人</div> <div class=\"simsub\"><b>onan()</b>:  油浸自冷（变压器冷却方式，Oil Natural Air Natural）</div> <div class=\"simsub\"><b>sonance(\\'sәjnәns)</b>: n. 有声状态, 声音</div></div>', 'n. 一致，调和；和音']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 81.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20250114(23:46:27)\n",
      "[main bce6ea3] update cards.html on date 20250114(23:46:27)\n",
      " 15 files changed, 237 insertions(+), 110 deletions(-)\n",
      " create mode 100644 docs/images/Guinea-asks-bauxite-miners-to-present-local-refinery-plans-by-May.jpeg\n",
      " create mode 100644 docs/images/images-b98b8556ba0a25e88d1579b2ffc16a79c139c395.jpg\n",
      " delete mode 100644 docs/images/images-e232c45b1db6f1af89fc1688113275e37baaa329.jpg\n",
      " delete mode 100644 docs/images/paste-3734f2bb9ca20bb7bd2e2c08e753015ab130772f.jpg\n",
      " create mode 100644 docs/images/paste-6ba10c81153adc71fe825e59ebf26ff0f720307c.png\n",
      " create mode 100644 docs/images/paste-9c4c03b0a6345aaa754b5767e49078d8f7af2fe2.png\n",
      " create mode 100644 docs/images/paste-b8739423b0a7ba9cfebee845f29554a1cafe1f47.jpg\n",
      " delete mode 100644 docs/images/paste-c0dd42e84aaaf23e322d5e9b978e109d75b170c6.jpg\n",
      " create mode 100644 docs/images/paste-c6d9ef7b846c57cb5c1ef5adb746da2c61a00a84.png\n",
      " create mode 100644 docs/images/paste-cfae8b5068b640cdbd4a2e2fbbe3dcdbc08a3ac6.jpg\n",
      " create mode 100644 docs/images/paste-d09601d83066b3063f7e388121fa2fedee1d1c0f.jpg\n",
      " delete mode 100644 docs/images/paste-dea8bb55b40d6a755fc9c8190c7bcfa0cff0cbd4.jpg\n",
      " delete mode 100644 docs/images/paste-ebebd3acddf1167667e5afbea68bd1142c6c8447.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To github.com:pengfeigao2021/anki_html.git\n",
      " + 2804cda...bce6ea3 main -> main (forced update)\n"
     ]
    }
   ],
   "source": [
    "# read sqlite3\n",
    "import sqlite3\n",
    "import anki\n",
    "from anki.collection import Collection\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import termcolor\n",
    "import tqdm\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "import pickle\n",
    "# import pandas as pd\n",
    "import editdistance\n",
    "\n",
    "EDICT_PATH = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data/ecdict.csv'\n",
    "MEDIA_FOLDER = '/Users/AlexG/Library/Application Support/Anki2/User 1/collection.media'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/MyPaperNotes.apkg'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/ivl10.apkg'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/极品GRE红宝书.apkg'\n",
    "DATA_PATH = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data'\n",
    "outdir = PKG_PATH + '-decompress'\n",
    "# outdir = '/Users/AlexG/Downloads/ivl10-decompress'\n",
    "# ANKI2_PATH = '/Users/AlexG/Downloads/ivl10-decompress/collection.anki2'\n",
    "ANKI21_PATH = outdir + '/collection.anki21'\n",
    "# EXPORT_PATH_WEBSITE = '/Users/AlexG/Documents/GitHub/typingpractices/website/onepager/onepager/templates/onepager/cards.html'\n",
    "EXPORT_PATH_WEBSITE = '/Users/AlexG/Documents/GitHub/anki_html/docs/gre_cards.html'\n",
    "\n",
    "def unzip_anki(pkg_file_path, outdir):\n",
    "    # Create the extract directory if it doesn't exist\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    # Unzip the .apkg file\n",
    "    with zipfile.ZipFile(PKG_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(outdir)\n",
    "\n",
    "    print(f\"Extracted files to {outdir}\")\n",
    "\n",
    "class EditDistanceFinder(object):\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "        self.load_cache()\n",
    "\n",
    "    def load_cache(self):\n",
    "        cache_path = os.path.join(DATA_PATH, 'edit_distance_cache.pkl')\n",
    "        if os.path.exists(cache_path):\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                self.cache = pickle.load(f)\n",
    "        else:\n",
    "            self.cache = {}\n",
    "\n",
    "    def save_cache(self):\n",
    "        cache_path = os.path.join(DATA_PATH, 'edit_distance_cache.pkl')\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump(self.cache, f)\n",
    "\n",
    "    def find(self, query, \n",
    "             edit_distance_lower=1, \n",
    "             edit_distance_upper=1,\n",
    "             html_class=\"sim1\"):\n",
    "        res = []\n",
    "\n",
    "        # find similar words with cache\n",
    "        similar_words = []\n",
    "        key = (query, edit_distance_lower, edit_distance_upper)\n",
    "        if key in self.cache:\n",
    "            # use cache\n",
    "            similar_words = self.cache[key]\n",
    "        else:\n",
    "            for w in self.words:\n",
    "                if w[0] == query:\n",
    "                    continue\n",
    "                d = editdistance.eval(w[0], query)\n",
    "                if d >= edit_distance_lower and d <= edit_distance_upper:\n",
    "                    similar_words.append((d, w[0], w[2]))\n",
    "            # save cache\n",
    "            self.cache[key] = similar_words\n",
    "        for w in similar_words:\n",
    "            text = f'<div class=\"{html_class}\">{w[1]}: {w[2]}</div>'\n",
    "            res.append(text)\n",
    "        \n",
    "        return ' '.join(res)\n",
    "\n",
    "\n",
    "class SubStrFinder(object):\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "        self.load_cache()\n",
    "\n",
    "    def load_cache(self):\n",
    "        cache_path = os.path.join(DATA_PATH, 'sub_word_cache.pkl')\n",
    "        if os.path.exists(cache_path):\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                self.cache = pickle.load(f)\n",
    "        else:\n",
    "            self.cache = {}\n",
    "\n",
    "    def save_cache(self):\n",
    "        cache_path = os.path.join(DATA_PATH, 'sub_word_cache.pkl')\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump(self.cache, f)\n",
    "\n",
    "\n",
    "    def load_edict(self, path):\n",
    "        total_lines = 0\n",
    "        with open(path, 'r') as f:\n",
    "            rd = csv.reader(f)\n",
    "            for cols in tqdm.tqdm(rd):\n",
    "                if len(cols[0]) < 4 or len(cols[3]) == 0:\n",
    "                    continue\n",
    "                if 'abbr.' in cols[3]:\n",
    "                    continue\n",
    "                if cols[0].isupper():\n",
    "                    continue\n",
    "                fields = (cols[0], cols[1], cols[3])\n",
    "                self.words.append(fields)\n",
    "                total_lines += 1\n",
    "        print(f\"Loaded {total_lines} lines from {path}\")\n",
    "\n",
    "    def find(self, query, html_class=\"simsub\"):\n",
    "        res = []\n",
    "        # print(\"Searching for {}...\".format(termcolor.colored(query, 'red')))\n",
    "        similar_words = []\n",
    "        key = query.lower()\n",
    "        if key in self.cache:\n",
    "            similar_words = self.cache[key]\n",
    "        else:\n",
    "            for w in self.words:\n",
    "                if w[0] == query:\n",
    "                    continue\n",
    "                if w[0].lower() in query.lower():\n",
    "                    similar_words.append(w)\n",
    "            self.cache[key] = similar_words\n",
    "\n",
    "        for w in similar_words:\n",
    "            text = f'<div class=\"{html_class}\"><b>{w[0]}({w[1]})</b>: {w[2]}</div>'\n",
    "            res.append(text)\n",
    "        \n",
    "        return ' '.join(res)\n",
    "\n",
    "class ImageSrcUrls(object):\n",
    "    def __init__(self):\n",
    "        self.purge_script_path = '/Users/AlexG/Documents/GitHub/anki_html/run_images.sh'\n",
    "        self.git_push_script_path = '/Users/AlexG/Documents/GitHub/anki_html/run_upload.sh'\n",
    "        self.source_image_folder = MEDIA_FOLDER\n",
    "        self.dest_image_folder = '/Users/AlexG/Documents/GitHub/anki_html/docs/images'\n",
    "        self.IMG_PAT = re.compile(r'<img\\b.*\\bsrc=\"')\n",
    "        self.SRC_PAT = re.compile(r'<img\\b.*\\bsrc=\"([^\"]+)\"')\n",
    "        self.srcs = []\n",
    "        self.exts = set()\n",
    "\n",
    "    def replace_image_url(self, text):\n",
    "        res = self.SRC_PAT.search(text)\n",
    "        if res:\n",
    "            for s in res.groups():\n",
    "                self.srcs.append(s)\n",
    "                ext = os.path.splitext(s)[1]\n",
    "                self.exts.add(ext)\n",
    "                print(termcolor.colored(s, 'red'))\n",
    "\n",
    "        return self.IMG_PAT.sub('<img src=\"./images/', text)\n",
    "    \n",
    "    def purge_images(self):\n",
    "        os.system(f\"bash {self.purge_script_path}\") \n",
    "\n",
    "    def git_push(self):\n",
    "        os.system(f\"bash {self.git_push_script_path}\")\n",
    "        \n",
    "    def move_images(self):\n",
    "        self.purge_images()\n",
    "        for src in tqdm.tqdm(self.srcs):\n",
    "            shutil.copyfile(\n",
    "                os.path.join(self.source_image_folder, src), \n",
    "                os.path.join(self.dest_image_folder, src)\n",
    "            )\n",
    "\n",
    "\n",
    "def format_cards_to_html(cards, collection, outpath, max_word_length=-1, max_cards=200):\n",
    "    data = []\n",
    "    image_src = ImageSrcUrls()\n",
    "    for cid in tqdm.tqdm(cards):\n",
    "        card = collection.get_card(cid)\n",
    "        note = card.note()\n",
    "        fields = note.fields\n",
    "        # fields[1] = fields[1][:20]\n",
    "        # fields = [field if len(field) <= max_text_length else field[:max_text_length] + '...' for field in fields]\n",
    "        data.append((card.ivl, fields))\n",
    "\n",
    "    edist_finder = EditDistanceFinder([fields for _, fields in data])\n",
    "    substr_finder = SubStrFinder([fields for _, fields in data])\n",
    "    substr_finder.load_edict(EDICT_PATH)\n",
    "    data.sort()\n",
    "\n",
    "    # format html table rows\n",
    "    html_table_rows = []\n",
    "    with tqdm.tqdm(total=max_cards) as pbar:\n",
    "        for fields in data:\n",
    "            if max_word_length == -1 or len(fields[1][0]) <= max_word_length:\n",
    "                if len(html_table_rows) >= max_cards:\n",
    "                    break\n",
    "                fields = [image_src.replace_image_url(field) for field in fields[1]]\n",
    "                html_table_rows.append(\n",
    "                    [\n",
    "                        \"<div>{}</div><div>{}</div>\".format(fields[0], fields[1]),\n",
    "                        '<div class=\"textscroll\">{} {} {}</div>'.format(\n",
    "                            edist_finder.find(fields[0]),\n",
    "                            edist_finder.find(\n",
    "                                fields[0],\n",
    "                                edit_distance_lower=2,\n",
    "                                edit_distance_upper=2,\n",
    "                                html_class=\"sim2\",\n",
    "                            ),\n",
    "                            substr_finder.find(fields[0]),\n",
    "                        ),\n",
    "                        fields[2],\n",
    "                    ]\n",
    "                )\n",
    "                # pbar.set_description('processing word: {}'.format(termcolor.colored(fields[0]), 'red'))\n",
    "                pbar.set_description('processing word: {}'.format(fields[0]))\n",
    "                pbar.update(1)\n",
    "\n",
    "    # data = [v for v in data if max_word_length == -1 or len(v[1][0]) <= max_word_length]\n",
    "    # data = [[image_src.replace_image_url(field) for field in fields]\n",
    "    #         for _, fields in data[:max_cards]]\n",
    "    # data = [['<div>{}</div><div>{}</div>'.format(fields[0], fields[1]), \n",
    "    #          '<div class=\"textscroll\">{} {} {}</div>'.format(\n",
    "    #             edist_finder.find(fields[0]),\n",
    "    #             edist_finder.find(fields[0],\n",
    "    #                               edit_distance_lower=2, \n",
    "    #                               edit_distance_upper=2,\n",
    "    #                               html_class=\"sim2\"),\n",
    "    #             substr_finder.find(fields[0]),\n",
    "    #          ),\n",
    "    #          fields[2], \n",
    "    #          ] for fields in tqdm.tqdm(data)]\n",
    "\n",
    "    data = html_table_rows\n",
    "    # save cache\n",
    "    edist_finder.save_cache()\n",
    "    substr_finder.save_cache()\n",
    "\n",
    "    print(image_src.exts)\n",
    "    print(termcolor.colored('Data samples:', 'blue'))\n",
    "    print('\\n'.join(['{}'.format(v) for v in data[:10]]))\n",
    "    image_src.move_images()\n",
    "    # with open(outpath+'.csv', 'w') as f:\n",
    "    #     wt = csv.writer(f)\n",
    "    #     wt.writerows([v[:3] for v in data])\n",
    "    rows = ['<tr><td class=\"indextd\">{}</td><td>{}</td></tr>\\n'.format(fi, '</td><td class=\"fixwtd\">'.join(field)) for fi, field in enumerate(data)]\n",
    "    html = f'''\n",
    "    <head>\n",
    "        <style>\n",
    "            body {{\n",
    "                background: #202020;\n",
    "            }}\n",
    "\n",
    "            table {{\n",
    "                background-color: #A0A0A0;\n",
    "                --color: #d0d0f5;\n",
    "                margin: 3em;\n",
    "            }}\n",
    "\n",
    "            thead,\n",
    "            tfoot {{\n",
    "                background: var(--color);\n",
    "            }}\n",
    "\n",
    "            tbody tr:nth-child(even) {{\n",
    "                background: color-mix(in srgb, var(--color), transparent 60%);\n",
    "            }}\n",
    "            div, p, td {{\n",
    "                width: 200px;\n",
    "                word-wrap: break-word;\n",
    "                white-space: normal;\n",
    "            }}\n",
    "            td img {{\n",
    "                max-height: 8em;\n",
    "            }}\n",
    "            .fixwtd {{\n",
    "                width: 200px;\n",
    "                word-wrap: break-word;\n",
    "                white-space: normal;\n",
    "            }}\n",
    "            .indextd {{\n",
    "                width: 10px;\n",
    "                word-wrap: break-word;\n",
    "                white-space: normal;\n",
    "            }}\n",
    "            td {{\n",
    "                border: 1px ridge #333333;\n",
    "            }}\n",
    "            img {{\n",
    "                max-height: 5em;\n",
    "                max-width: 5em;\n",
    "            }}\n",
    "            .sim1 {{\n",
    "                width: 26em;\n",
    "                background-color: #eadcb7;\n",
    "            }}\n",
    "            .sim2 {{\n",
    "                width: 26em;\n",
    "                background-color: #e99a06;\n",
    "            }}\n",
    "            .simsub {{\n",
    "                width: 26em;\n",
    "                background-color: #afcd95;\n",
    "            }}\n",
    "            b {{\n",
    "                background-color: #f1c40f;\n",
    "            }}\n",
    "            .textscroll {{\n",
    "                height: 3em;\n",
    "                width: 27em;\n",
    "                border: 1px solid #ccc;\n",
    "                overflow: auto;\n",
    "            }}\n",
    "        </style>\n",
    "        <link rel=\"stylesheet\" href=\"css/fancy-button.css\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <button onclick=\"changeWidth(0.5)\" class=\"button-74\">Half Column Width</button>\n",
    "        <button onclick=\"hideRows(10)\" class=\"button-73 fix-width-button\">Hide Rows</button>\n",
    "        <table id=\"cards\">\n",
    "          {\"\".join(rows)}\n",
    "        </table>\n",
    "        <script src=\"js/change-width.js\"></script>\n",
    "        <script src=\"js/hide-rows.js\"></script>\n",
    "    </body>\n",
    "    '''\n",
    "    with open(outpath, 'w') as f:\n",
    "        f.write(html)\n",
    "\n",
    "    # git push\n",
    "    image_src.git_push()\n",
    "\n",
    "try:\n",
    "    col = Collection(ANKI21_PATH)\n",
    "    print(dir(col))\n",
    "    print('total card count: ', col.card_count())\n",
    "    # print('note count: ', col.node_count())\n",
    "\n",
    "    # query = 'prop:ivl<=30'\n",
    "    query = ''\n",
    "    cards = col.find_cards(query)\n",
    "    print(termcolor.colored(f'query {query} find {len(cards)} cards', 'green'))\n",
    "\n",
    "    outpath = os.path.join(outdir, 'cards.html')\n",
    "    # format_cards_to_html(cards, col, outpath)\n",
    "    format_cards_to_html(cards, col, EXPORT_PATH_WEBSITE,\n",
    "                         max_word_length=-1)\n",
    "\n",
    "    # Fetch all notes (cards content) in the collection\n",
    "    # print(\"\\nNotes in collection:\")\n",
    "    # for note_id in col.db.list(\"select id from notes\"):\n",
    "    #     note = col.get_note(note_id)\n",
    "    #     fields = note.fields\n",
    "    #     print(f\"Note ID: {note_id}, Fields: {fields}\")\n",
    "\n",
    "    # # Fetch all cards (metadata) in the collection\n",
    "    # print(\"\\nCards in collection:\")\n",
    "    # for card_id in col.db.list(\"select id from cards\"):\n",
    "    #     card = col.get_card(card_id)\n",
    "    #     print(f\"Card ID: {card.id}, Note ID: {card.nid}, Deck ID: {card.did}, Type: {card.type}\")\n",
    "\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Caught an exception: {e}\")\n",
    "    import traceback\n",
    "    print(traceback.format_exc())\n",
    "finally:\n",
    "    # Close the collection\n",
    "    col.close()\n",
    "\n",
    "col.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------\n",
    "# Offline batch similar words\n",
    "# -------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read sqlite3\n",
    "import sqlite3\n",
    "import anki\n",
    "from anki.collection import Collection\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import termcolor\n",
    "import tqdm\n",
    "import csv\n",
    "import shutil\n",
    "# import pandas as pd\n",
    "import editdistance\n",
    "\n",
    "EDICT_PATH = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data/ecdict.csv'\n",
    "MEDIA_FOLDER = '/Users/AlexG/Library/Application Support/Anki2/User 1/collection.media'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/MyPaperNotes.apkg'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/ivl10.apkg'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/极品GRE红宝书.apkg'\n",
    "outdir = PKG_PATH + '-decompress'\n",
    "# outdir = '/Users/AlexG/Downloads/ivl10-decompress'\n",
    "# ANKI2_PATH = '/Users/AlexG/Downloads/ivl10-decompress/collection.anki2'\n",
    "ANKI21_PATH = outdir + '/collection.anki21'\n",
    "# EXPORT_PATH_WEBSITE = '/Users/AlexG/Documents/GitHub/typingpractices/website/onepager/onepager/templates/onepager/cards.html'\n",
    "EXPORT_PATH_WEBSITE = '/Users/AlexG/Documents/GitHub/anki_html/docs/gre_cards.html'\n",
    "\n",
    "def unzip_anki(pkg_file_path, outdir):\n",
    "    # Create the extract directory if it doesn't exist\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    # Unzip the .apkg file\n",
    "    with zipfile.ZipFile(PKG_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(outdir)\n",
    "\n",
    "    print(f\"Extracted files to {outdir}\")\n",
    "\n",
    "class EditDistanceFinder(object):\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "\n",
    "    def find(self, query, \n",
    "             edit_distance_lower=1, \n",
    "             edit_distance_upper=1,\n",
    "             html_class=\"sim1\"):\n",
    "        res = []\n",
    "        for w in self.words:\n",
    "            if w[0] == query:\n",
    "                continue\n",
    "            d = editdistance.eval(w[0], query)\n",
    "            if d >= edit_distance_lower and d <= edit_distance_upper:\n",
    "                text = f'<div class=\"{html_class}\">{w[0]}: {w[2]}</div>'\n",
    "                res.append(text)\n",
    "        \n",
    "        return ' '.join(res)\n",
    "            \n",
    "        \n",
    "class SubStrFinder(object):\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "\n",
    "    def load_edict(self, path):\n",
    "        total_lines = 0\n",
    "        with open(path, 'r') as f:\n",
    "            rd = csv.reader(f)\n",
    "            for cols in tqdm.tqdm(rd):\n",
    "                if len(cols[0]) < 4 or len(cols[3]) == 0:\n",
    "                    continue\n",
    "                if 'abbr.' in cols[3]:\n",
    "                    continue\n",
    "                if cols[0].isupper():\n",
    "                    continue\n",
    "                fields = (cols[0], cols[1], cols[3])\n",
    "                self.words.append(fields)\n",
    "                total_lines += 1\n",
    "        print(f\"Loaded {total_lines} lines from {path}\")\n",
    "\n",
    "    def find(self, query, html_class=\"simsub\"):\n",
    "        res = []\n",
    "        print(\"Searching for {}...\".format(termcolor.colored(query, 'red')))\n",
    "        for w in tqdm.tqdm(self.words):\n",
    "            if w[0] == query:\n",
    "                continue\n",
    "            if w[0].lower() in query.lower():\n",
    "                text = f'<div class=\"{html_class}\"><b>{w[0]}({w[1]})</b>: {w[2]}</div>'\n",
    "                res.append(text)\n",
    "        \n",
    "        return ' '.join(res)\n",
    "\n",
    "class ImageSrcUrls(object):\n",
    "    def __init__(self):\n",
    "        self.purge_script_path = '/Users/AlexG/Documents/GitHub/anki_html/run_images.sh'\n",
    "        self.git_push_script_path = '/Users/AlexG/Documents/GitHub/anki_html/run_upload.sh'\n",
    "        self.source_image_folder = MEDIA_FOLDER\n",
    "        self.dest_image_folder = '/Users/AlexG/Documents/GitHub/anki_html/docs/images'\n",
    "        self.IMG_PAT = re.compile(r'<img\\b.*\\bsrc=\"')\n",
    "        self.SRC_PAT = re.compile(r'<img\\b.*\\bsrc=\"([^\"]+)\"')\n",
    "        self.srcs = []\n",
    "        self.exts = set()\n",
    "\n",
    "    def replace_image_url(self, text):\n",
    "        res = self.SRC_PAT.search(text)\n",
    "        if res:\n",
    "            for s in res.groups():\n",
    "                self.srcs.append(s)\n",
    "                ext = os.path.splitext(s)[1]\n",
    "                self.exts.add(ext)\n",
    "                print(termcolor.colored(s, 'red'))\n",
    "\n",
    "        return self.IMG_PAT.sub('<img src=\"./images/', text)\n",
    "    \n",
    "    def purge_images(self):\n",
    "        os.system(f\"bash {self.purge_script_path}\") \n",
    "\n",
    "    def git_push(self):\n",
    "        os.system(f\"bash {self.git_push_script_path}\")\n",
    "        \n",
    "    def move_images(self):\n",
    "        self.purge_images()\n",
    "        for src in tqdm.tqdm(self.srcs):\n",
    "            shutil.copyfile(\n",
    "                os.path.join(self.source_image_folder, src), \n",
    "                os.path.join(self.dest_image_folder, src)\n",
    "            )\n",
    "\n",
    "\n",
    "def format_cards_to_html(cards, collection, outpath, max_word_length=-1, max_cards=200):\n",
    "    data = []\n",
    "    image_src = ImageSrcUrls()\n",
    "    for cid in tqdm.tqdm(cards):\n",
    "        card = collection.get_card(cid)\n",
    "        note = card.note()\n",
    "        fields = note.fields\n",
    "        # fields[1] = fields[1][:20]\n",
    "        # fields = [field if len(field) <= max_text_length else field[:max_text_length] + '...' for field in fields]\n",
    "        data.append((card.ivl, fields))\n",
    "\n",
    "    edist_finder = EditDistanceFinder([fields for _, fields in data])\n",
    "    substr_finder = SubStrFinder([fields for _, fields in data])\n",
    "    substr_finder.load_edict(EDICT_PATH)\n",
    "    data.sort()\n",
    "    data = [v for v in data if max_word_length == -1 or len(v[1][0]) <= max_word_length]\n",
    "    data = [[image_src.replace_image_url(field) for field in fields] \n",
    "            for _, fields in data[:max_cards]]\n",
    "    data = [['<div>{}</div><div>{}</div>'.format(fields[0], fields[1]), \n",
    "             '<div class=\"textscroll\">{} {} {}</div>'.format(\n",
    "                edist_finder.find(fields[0]),\n",
    "                edist_finder.find(fields[0],\n",
    "                                  edit_distance_lower=2, \n",
    "                                  edit_distance_upper=2,\n",
    "                                  html_class=\"sim2\"),\n",
    "                substr_finder.find(fields[0]),\n",
    "             ),\n",
    "             fields[2], \n",
    "             ] for fields in data]\n",
    "    print(image_src.exts)\n",
    "    print(termcolor.colored('Data samples:', 'blue'))\n",
    "    print('\\n'.join(['{}'.format(v) for v in data[:10]]))\n",
    "\n",
    "unzip_anki(PKG_PATH, outdir)\n",
    "col = Collection(ANKI21_PATH)\n",
    "# Fetch all decks in the collection\n",
    "decks = col.decks.all()\n",
    "print(\"Decks in collection:\")\n",
    "for deck in decks:\n",
    "    print(f\"Deck ID: {deck['id']}, Name: {deck['name']}\")\n",
    "\n",
    "try:\n",
    "    print(dir(col))\n",
    "    print('total card count: ', col.card_count())\n",
    "    # print('note count: ', col.node_count())\n",
    "\n",
    "    # query = 'prop:ivl<=30'\n",
    "    query = ''\n",
    "    cards = col.find_cards(query)\n",
    "    print(termcolor.colored(f'query {query} find {len(cards)} cards', 'green'))\n",
    "\n",
    "    outpath = os.path.join(outdir, 'cards.html')\n",
    "    # format_cards_to_html(cards, col, outpath)\n",
    "    format_cards_to_html(cards, col, EXPORT_PATH_WEBSITE,\n",
    "                         max_word_length=-1)\n",
    "\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    # Close the collection\n",
    "    col.close()\n",
    "\n",
    "col.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====================\n",
    "# build all words list\n",
    "# ====================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decks in collection:\n",
      "Deck ID: 1, Name: Default\n",
      "Deck ID: 1685695741424, Name: 极品GRE红宝书\n",
      "total card count:  7513\n",
      "\u001b[32mquery  find 7513 cards\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7513/7513 [00:02<00:00, 3308.24it/s]\n",
      "770612it [00:03, 229250.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 750698 lines from /Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data/ecdict.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750698/750698 [00:00<00:00, 1021786.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 758211\n"
     ]
    }
   ],
   "source": [
    "# read sqlite3\n",
    "import sqlite3\n",
    "import anki\n",
    "from anki.collection import Collection\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import termcolor\n",
    "import tqdm\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "# import pandas as pd\n",
    "import editdistance\n",
    "\n",
    "EDICT_PATH = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data/ecdict.csv'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/极品GRE红宝书.apkg'\n",
    "outdir = PKG_PATH + '-decompress'\n",
    "ANKI21_PATH = outdir + '/collection.anki21'\n",
    "\n",
    "class SubStrFinder(object):\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "\n",
    "    def load_edict(self, path):\n",
    "        total_lines = 0\n",
    "        with open(path, 'r') as f:\n",
    "            rd = csv.reader(f)\n",
    "            for cols in tqdm.tqdm(rd):\n",
    "                if len(cols[0]) < 4 or len(cols[3]) == 0:\n",
    "                    continue\n",
    "                if 'abbr.' in cols[3]:\n",
    "                    continue\n",
    "                if cols[0].isupper():\n",
    "                    continue\n",
    "                fields = (cols[0], cols[1], cols[3])\n",
    "                self.words.append(fields)\n",
    "                total_lines += 1\n",
    "        print(f\"Loaded {total_lines} lines from {path}\")\n",
    "\n",
    "\n",
    "col = Collection(ANKI21_PATH)\n",
    "# Fetch all decks in the collection\n",
    "decks = col.decks.all()\n",
    "all_words_set = set()\n",
    "all_words = []\n",
    "gre_words = []\n",
    "print(\"Decks in collection:\")\n",
    "for deck in decks:\n",
    "    print(f\"Deck ID: {deck['id']}, Name: {deck['name']}\")\n",
    "\n",
    "try:\n",
    "    print('total card count: ', col.card_count())\n",
    "\n",
    "    query = ''\n",
    "    cards = col.find_cards(query)\n",
    "    print(termcolor.colored(f'query {query} find {len(cards)} cards', 'green'))\n",
    "\n",
    "    for cid in tqdm.tqdm(cards):\n",
    "        card = col.get_card(cid)\n",
    "        note = card.note()\n",
    "        fields = note.fields\n",
    "        all_words_set.add(fields[0])\n",
    "        gre_words.append(fields[0])\n",
    "    substr_finder = SubStrFinder([])\n",
    "    substr_finder.load_edict(EDICT_PATH)\n",
    "    for w in tqdm.tqdm(substr_finder.words):\n",
    "        all_words_set.add(w)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    # Close the collection\n",
    "    col.close()\n",
    "\n",
    "col.close()\n",
    "\n",
    "all_words = list(all_words_set)\n",
    "print(f\"Total words: {len(all_words)}\")\n",
    "with open('/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/all_words.json', 'w') as f:\n",
    "    json.dump(all_words, f)\n",
    "with open('/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/gre_words.json', 'w') as f:\n",
    "    json.dump(gre_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from /Users/AlexG/Documents/GitHub/mindmaps/anki_tools/gre_words.json and /Users/AlexG/Documents/GitHub/mindmaps/anki_tools/all_words.json\n",
      "loaded 7513 gre words and 758211 all words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 152303449/5696439243 [01:34<57:08, 1616872.64it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m N_gre \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(gre_words)\n\u001b[1;32m     40\u001b[0m N_all \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_words)\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n1, n2 \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(itertools\u001b[38;5;241m.\u001b[39mproduct(\u001b[38;5;28mrange\u001b[39m(N_gre), \u001b[38;5;28mrange\u001b[39m(N_all)), total\u001b[38;5;241m=\u001b[39mN_gre \u001b[38;5;241m*\u001b[39m N_all):\n\u001b[1;32m     42\u001b[0m     w1 \u001b[38;5;241m=\u001b[39m gre_words[n1][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     43\u001b[0m     w2 \u001b[38;5;241m=\u001b[39m all_words[n2][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/tqdm/std.py:1187\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m n \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlast_print_n\u001b[49m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminiters:\n\u001b[1;32m   1188\u001b[0m     cur_t \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m   1189\u001b[0m     dt \u001b[38;5;241m=\u001b[39m cur_t \u001b[38;5;241m-\u001b[39m last_print_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "# read sqlite3\n",
    "import sqlite3\n",
    "import anki\n",
    "from anki.collection import Collection\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import termcolor\n",
    "import tqdm\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "# import pandas as pd\n",
    "import editdistance\n",
    "# a = [1,2,3]\n",
    "# b= itertools.product(a, a)\n",
    "# print(list(b))\n",
    "\n",
    "gre_words_path = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/gre_words.json'\n",
    "all_words_path = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/all_words.json'\n",
    "print('loading from {} and {}'.format(gre_words_path, all_words_path))\n",
    "gre_words = json.load(open(gre_words_path))\n",
    "all_words = json.load(open(all_words_path))\n",
    "print('loaded {} gre words and {} all words'.format(len(gre_words), len(all_words)))\n",
    "\n",
    "def find(self, query, html_class=\"simsub\"):\n",
    "    res = []\n",
    "    print(\"Searching for {}...\".format(termcolor.colored(query, 'red')))\n",
    "    for w in tqdm.tqdm(self.words):\n",
    "        if w[0] == query:\n",
    "            continue\n",
    "        if w[0].lower() in query.lower():\n",
    "            text = f'<div class=\"{html_class}\"><b>{w[0]}({w[1]})</b>: {w[2]}</div>'\n",
    "            res.append(text)\n",
    "    \n",
    "    return ' '.join(res)\n",
    "\n",
    "N_gre = len(gre_words)\n",
    "N_all = len(all_words)\n",
    "for n1, n2 in tqdm.tqdm(itertools.product(range(N_gre), range(N_all)), total=N_gre * N_all):\n",
    "    w1 = gre_words[n1][0]\n",
    "    w2 = all_words[n2][0]\n",
    "    # d = editdistance.eval(w1, w2)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
