{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted files to /Users/AlexG/Downloads/极品GRE红宝书.apkg-decompress\n",
      "blocked main thread for 409ms:\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/m0/h3b4dj7x09vfsx38dqn5s41r0000gn/T/ipykernel_8975/3843105603.py\", line 307, in <module>\n",
      "    col = Collection(ANKI21_PATH)\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/anki/collection.py\", line 150, in __init__\n",
      "    self.reopen()\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/anki/collection.py\", line 291, in reopen\n",
      "    self._backend.open_collection(\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/anki/_backend_generated.py\", line 143, in open_collection\n",
      "    raw_bytes = self._run_command(3, 0, message.SerializeToString())\n",
      "  File \"/Users/AlexG/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/anki/_backend.py\", line 167, in _run_command\n",
      "    print(\"\".join(traceback.format_stack()))\n",
      "\n",
      "Decks in collection:\n",
      "Deck ID: 1, Name: Default\n",
      "Deck ID: 1685695741424, Name: 极品GRE红宝书\n",
      "['__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_backend', '_build_sort_mode', '_check_backend_undo_status', '_clear_caches', '_deprecated_aliases', '_deprecated_attributes', '_load_scheduler', '_logRem', '_pb_search_separator', '_remNotes', '_startReps', '_supported_scheduler_versions', 'abort_media_sync', 'abort_sync', 'addNote', 'add_custom_undo_entry', 'add_image_occlusion_note', 'add_image_occlusion_notetype', 'add_note', 'add_notes', 'after_note_updates', 'all_browser_columns', 'all_config', 'autosave', 'await_backup_completion', 'backend', 'browser_row_for_id', 'build_search_string', 'cardStats', 'card_count', 'card_ids_of_note', 'card_stats', 'card_stats_data', 'close', 'close_for_full_sync', 'compare_answer', 'compute_memory_state', 'conf', 'create_backup', 'crt', 'db', 'decks', 'default_deck_for_notetype', 'defaults_for_adding', 'emptyCids', 'export_anki_package', 'export_card_csv', 'export_collection_package', 'export_note_csv', 'extract_cloze_for_typing', 'field_names_for_note_ids', 'find_and_replace', 'find_cards', 'find_dupes', 'find_notes', 'fix_integrity', 'flush', 'format_timespan', 'full_upload_or_download', 'fuzz_delta', 'genCards', 'get_aux_notetype_config', 'get_aux_template_config', 'get_browser_column', 'get_card', 'get_config', 'get_config_bool', 'get_config_string', 'get_csv_metadata', 'get_empty_cards', 'get_image_for_occlusion', 'get_image_occlusion_note', 'get_note', 'get_preferences', 'group_searches', 'i18n_resources', 'import_anki_package', 'import_csv', 'import_json_file', 'import_json_string', 'initialize_backend_logging', 'is_empty', 'join_searches', 'latest_progress', 'load_browser_card_columns', 'load_browser_note_columns', 'log', 'media', 'media_sync_status', 'merge_undo_entries', 'mod', 'mod_schema', 'models', 'name', 'newNote', 'new_note', 'nextID', 'note_count', 'op_made_changes', 'optimize', 'path', 'redo', 'register_deprecated_aliases', 'register_deprecated_attributes', 'remNotes', 'remove_cards_and_orphaned_notes', 'remove_config', 'remove_notes', 'remove_notes_by_card', 'render_markdown', 'reopen', 'replace_in_search_node', 'reset', 'save', 'sched', 'sched_ver', 'schema_changed', 'server', 'setMod', 'set_aux_notetype_config', 'set_aux_template_config', 'set_browser_card_columns', 'set_browser_note_columns', 'set_config', 'set_config_bool', 'set_config_string', 'set_deck', 'set_preferences', 'set_schema_modified', 'set_user_flag_for_cards', 'set_v3_scheduler', 'set_wants_abort', 'startTimebox', 'stats', 'studied_today', 'sync_collection', 'sync_login', 'sync_media', 'sync_status', 'tags', 'timeboxReached', 'tr', 'undo', 'undo_name', 'undo_status', 'updateFieldCache', 'update_card', 'update_cards', 'update_image_occlusion_note', 'update_note', 'update_notes', 'upgrade_to_v2_scheduler', 'usn', 'v3_scheduler', 'weakref']\n",
      "total card count:  7513\n",
      "\u001b[32mquery  find 7513 cards\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7513/7513 [00:01<00:00, 4832.34it/s]\n",
      "770612it [00:04, 155693.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 750698 lines from /Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data/ecdict.csv\n",
      "\u001b[31mimages-5186a2c0d72bb6ced6ab1099107be5415e7b0709.jpg\u001b[0m\n",
      "\u001b[31mGuinea-asks-bauxite-miners-to-present-local-refinery-plans-by-May.jpeg\u001b[0m\n",
      "\u001b[31mpaste-9c4c03b0a6345aaa754b5767e49078d8f7af2fe2.png\u001b[0m\n",
      "\u001b[31mpaste-cfae8b5068b640cdbd4a2e2fbbe3dcdbc08a3ac6.jpg\u001b[0m\n",
      "\u001b[31mimages-b98b8556ba0a25e88d1579b2ffc16a79c139c395.jpg\u001b[0m\n",
      "\u001b[31mpaste-6ba10c81153adc71fe825e59ebf26ff0f720307c.png\u001b[0m\n",
      "\u001b[31mpaste-62f481c52aa82b6eaa5daa5b73639b2703d21517.jpg\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 8248.71it/s]\n",
      "ls: images: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for \u001b[31mabstinent\u001b[0m...\n",
      "Searching for \u001b[31marachnid\u001b[0m...\n",
      "Searching for \u001b[31mbroach\u001b[0m...\n",
      "Searching for \u001b[31mcalamity\u001b[0m...\n",
      "Searching for \u001b[31mcanard\u001b[0m...\n",
      "Searching for \u001b[31mcongregate\u001b[0m...\n",
      "Searching for \u001b[31mconsonance\u001b[0m...\n",
      "Searching for \u001b[31mdeference\u001b[0m...\n",
      "Searching for \u001b[31mdespondent\u001b[0m...\n",
      "Searching for \u001b[31mdilettante\u001b[0m...\n",
      "Searching for \u001b[31mdissemble\u001b[0m...\n",
      "Searching for \u001b[31memollient\u001b[0m...\n",
      "Searching for \u001b[31mephemeral\u001b[0m...\n",
      "Searching for \u001b[31mfeign\u001b[0m...\n",
      "Searching for \u001b[31mflabby\u001b[0m...\n",
      "Searching for \u001b[31mfraught\u001b[0m...\n",
      "Searching for \u001b[31mgaucherie\u001b[0m...\n",
      "Searching for \u001b[31mgrueling\u001b[0m...\n",
      "Searching for \u001b[31mhavoc\u001b[0m...\n",
      "Searching for \u001b[31mimmolate\u001b[0m...\n",
      "Searching for \u001b[31mimpassive\u001b[0m...\n",
      "Searching for \u001b[31mlassitude\u001b[0m...\n",
      "Searching for \u001b[31mluminary\u001b[0m...\n",
      "Searching for \u001b[31mmalcontent\u001b[0m...\n",
      "Searching for \u001b[31mmeek\u001b[0m...\n",
      "Searching for \u001b[31mmesmerism\u001b[0m...\n",
      "Searching for \u001b[31mmuzzy\u001b[0m...\n",
      "Searching for \u001b[31mpallid\u001b[0m...\n",
      "Searching for \u001b[31mpaltry\u001b[0m...\n",
      "Searching for \u001b[31mprecipitant\u001b[0m...\n",
      "Searching for \u001b[31mprecipitation\u001b[0m...\n",
      "Searching for \u001b[31mquisling\u001b[0m...\n",
      "Searching for \u001b[31mraillery\u001b[0m...\n",
      "Searching for \u001b[31mrespite\u001b[0m...\n",
      "Searching for \u001b[31mshingle\u001b[0m...\n",
      "Searching for \u001b[31msolvent\u001b[0m...\n",
      "Searching for \u001b[31mspeleology\u001b[0m...\n",
      "Searching for \u001b[31mspurious\u001b[0m...\n",
      "Searching for \u001b[31mstriate\u001b[0m...\n",
      "Searching for \u001b[31msupple\u001b[0m...\n",
      "Searching for \u001b[31mtendentious\u001b[0m...\n",
      "Searching for \u001b[31mthwart\u001b[0m...\n",
      "Searching for \u001b[31mvacillate\u001b[0m...\n",
      "Searching for \u001b[31macarpous\u001b[0m...\n",
      "Searching for \u001b[31mapprise\u001b[0m...\n",
      "Searching for \u001b[31mastray\u001b[0m...\n",
      "Searching for \u001b[31maureole\u001b[0m...\n",
      "Searching for \u001b[31mauspices\u001b[0m...\n",
      "Searching for \u001b[31mawning\u001b[0m...\n",
      "Searching for \u001b[31mbadinage\u001b[0m...\n",
      "Searching for \u001b[31mbaffle\u001b[0m...\n",
      "Searching for \u001b[31mbale\u001b[0m...\n",
      "Searching for \u001b[31mbauxite\u001b[0m...\n",
      "Searching for \u001b[31mbedlam\u001b[0m...\n",
      "Searching for \u001b[31mbeseech\u001b[0m...\n",
      "Searching for \u001b[31mbounteous\u001b[0m...\n",
      "Searching for \u001b[31mbraid\u001b[0m...\n",
      "Searching for \u001b[31mbrocade\u001b[0m...\n",
      "Searching for \u001b[31mcactus\u001b[0m...\n",
      "Searching for \u001b[31mcalipers\u001b[0m...\n",
      "Searching for \u001b[31mcantankerous\u001b[0m...\n",
      "Searching for \u001b[31mcapsize\u001b[0m...\n",
      "Searching for \u001b[31mcarapace\u001b[0m...\n",
      "Searching for \u001b[31mcaricature\u001b[0m...\n",
      "Searching for \u001b[31mcelibate\u001b[0m...\n",
      "Searching for \u001b[31mcephalic\u001b[0m...\n",
      "Searching for \u001b[31mchalice\u001b[0m...\n",
      "Searching for \u001b[31mcolander\u001b[0m...\n",
      "Searching for \u001b[31mcommensurate\u001b[0m...\n",
      "Searching for \u001b[31mcompendium\u001b[0m...\n",
      "Searching for \u001b[31mconclave\u001b[0m...\n",
      "Searching for \u001b[31mconfection\u001b[0m...\n",
      "Searching for \u001b[31mconjecture\u001b[0m...\n",
      "Searching for \u001b[31mconjugal\u001b[0m...\n",
      "Searching for \u001b[31mconsensus\u001b[0m...\n",
      "Searching for \u001b[31mconsummate\u001b[0m...\n",
      "Searching for \u001b[31mcontiguous\u001b[0m...\n",
      "Searching for \u001b[31mcontumacious\u001b[0m...\n",
      "Searching for \u001b[31mcopious\u001b[0m...\n",
      "Searching for \u001b[31mcoruscate\u001b[0m...\n",
      "Searching for \u001b[31mcountenance\u001b[0m...\n",
      "Searching for \u001b[31mcrockery\u001b[0m...\n",
      "Searching for \u001b[31mcrux\u001b[0m...\n",
      "Searching for \u001b[31mcurator\u001b[0m...\n",
      "Searching for \u001b[31mcurtail\u001b[0m...\n",
      "Searching for \u001b[31mdalliance\u001b[0m...\n",
      "Searching for \u001b[31mdeciduous\u001b[0m...\n",
      "Searching for \u001b[31mdeign\u001b[0m...\n",
      "Searching for \u001b[31mdemur\u001b[0m...\n",
      "Searching for \u001b[31mdesultory\u001b[0m...\n",
      "Searching for \u001b[31mdiaphanous\u001b[0m...\n",
      "Searching for \u001b[31mdiffident\u001b[0m...\n",
      "Searching for \u001b[31mdisclaimer\u001b[0m...\n",
      "Searching for \u001b[31mdisconcert\u001b[0m...\n",
      "Searching for \u001b[31mdisquisition\u001b[0m...\n",
      "Searching for \u001b[31mdistrait\u001b[0m...\n",
      "Searching for \u001b[31mdivagate\u001b[0m...\n",
      "Searching for \u001b[31mdivulge\u001b[0m...\n",
      "Searching for \u001b[31mdoff\u001b[0m...\n",
      "Searching for \u001b[31mdoggo\u001b[0m...\n",
      "Searching for \u001b[31mdomicile\u001b[0m...\n",
      "Searching for \u001b[31mdullard\u001b[0m...\n",
      "Searching for \u001b[31mearthly\u001b[0m...\n",
      "Searching for \u001b[31measel\u001b[0m...\n",
      "Searching for \u001b[31meffete\u001b[0m...\n",
      "Searching for \u001b[31mefficacious\u001b[0m...\n",
      "Searching for \u001b[31memphatic\u001b[0m...\n",
      "Searching for \u001b[31menamored\u001b[0m...\n",
      "Searching for \u001b[31mengrave\u001b[0m...\n",
      "Searching for \u001b[31mentrench\u001b[0m...\n",
      "Searching for \u001b[31meschew\u001b[0m...\n",
      "Searching for \u001b[31mexplicate\u001b[0m...\n",
      "Searching for \u001b[31mexpository\u001b[0m...\n",
      "Searching for \u001b[31mexpostulate\u001b[0m...\n",
      "Searching for \u001b[31mexpunge\u001b[0m...\n",
      "Searching for \u001b[31mextant\u001b[0m...\n",
      "Searching for \u001b[31mextraneous\u001b[0m...\n",
      "Searching for \u001b[31mfacetious\u001b[0m...\n",
      "Searching for \u001b[31mfacile\u001b[0m...\n",
      "Searching for \u001b[31mfactotum\u001b[0m...\n",
      "Searching for \u001b[31mfanfare\u001b[0m...\n",
      "Searching for \u001b[31mfatuous\u001b[0m...\n",
      "Searching for \u001b[31mfilly\u001b[0m...\n",
      "Searching for \u001b[31mflatcar\u001b[0m...\n",
      "Searching for \u001b[31mflorid\u001b[0m...\n",
      "Searching for \u001b[31mflout\u001b[0m...\n",
      "Searching for \u001b[31mfoppish\u001b[0m...\n",
      "Searching for \u001b[31mfoster\u001b[0m...\n",
      "Searching for \u001b[31mfringe\u001b[0m...\n",
      "Searching for \u001b[31mfrowzy\u001b[0m...\n",
      "Searching for \u001b[31mgall\u001b[0m...\n",
      "Searching for \u001b[31mgambol\u001b[0m...\n",
      "Searching for \u001b[31mgenial\u001b[0m...\n",
      "Searching for \u001b[31mgild\u001b[0m...\n",
      "Searching for \u001b[31mgrasping\u001b[0m...\n",
      "Searching for \u001b[31mgripe\u001b[0m...\n",
      "Searching for \u001b[31mgrovel\u001b[0m...\n",
      "Searching for \u001b[31mgulch\u001b[0m...\n",
      "Searching for \u001b[31mgully\u001b[0m...\n",
      "Searching for \u001b[31mhallow\u001b[0m...\n",
      "Searching for \u001b[31mhaughty\u001b[0m...\n",
      "Searching for \u001b[31mhemostat\u001b[0m...\n",
      "Searching for \u001b[31mheyday\u001b[0m...\n",
      "Searching for \u001b[31mhistology\u001b[0m...\n",
      "Searching for \u001b[31mhobble\u001b[0m...\n",
      "Searching for \u001b[31mhomily\u001b[0m...\n",
      "Searching for \u001b[31mhonorarium\u001b[0m...\n",
      "Searching for \u001b[31mignominious\u001b[0m...\n",
      "Searching for \u001b[31mimbue\u001b[0m...\n",
      "Searching for \u001b[31mimpart\u001b[0m...\n",
      "Searching for \u001b[31mimpassioned\u001b[0m...\n",
      "Searching for \u001b[31mimpediment\u001b[0m...\n",
      "Searching for \u001b[31mimperative\u001b[0m...\n",
      "Searching for \u001b[31mimpuissance\u001b[0m...\n",
      "Searching for \u001b[31minadvertence\u001b[0m...\n",
      "Searching for \u001b[31mincense\u001b[0m...\n",
      "Searching for \u001b[31minchoate\u001b[0m...\n",
      "Searching for \u001b[31mincisor\u001b[0m...\n",
      "Searching for \u001b[31minsipid\u001b[0m...\n",
      "Searching for \u001b[31minterdict\u001b[0m...\n",
      "Searching for \u001b[31minterloper\u001b[0m...\n",
      "Searching for \u001b[31minterstice\u001b[0m...\n",
      "Searching for \u001b[31minundate\u001b[0m...\n",
      "Searching for \u001b[31minured\u001b[0m...\n",
      "Searching for \u001b[31mirksome\u001b[0m...\n",
      "Searching for \u001b[31mjolt\u001b[0m...\n",
      "Searching for \u001b[31mjudiciousness\u001b[0m...\n",
      "Searching for \u001b[31mlance\u001b[0m...\n",
      "Searching for \u001b[31mlandfill\u001b[0m...\n",
      "Searching for \u001b[31mlanguor\u001b[0m...\n",
      "Searching for \u001b[31mleery\u001b[0m...\n",
      "Searching for \u001b[31mleeward\u001b[0m...\n",
      "Searching for \u001b[31mlicentious\u001b[0m...\n",
      "Searching for \u001b[31mligneous\u001b[0m...\n",
      "Searching for \u001b[31mlimn\u001b[0m...\n",
      "Searching for \u001b[31mlist\u001b[0m...\n",
      "Searching for \u001b[31mlurid\u001b[0m...\n",
      "Searching for \u001b[31mmacerate\u001b[0m...\n",
      "Searching for \u001b[31mmaculate\u001b[0m...\n",
      "Searching for \u001b[31mmakeshift\u001b[0m...\n",
      "Searching for \u001b[31mmangle\u001b[0m...\n",
      "Searching for \u001b[31mmaudlin\u001b[0m...\n",
      "Searching for \u001b[31mmeddlesome\u001b[0m...\n",
      "Searching for \u001b[31mmenace\u001b[0m...\n",
      "Searching for \u001b[31mmenial\u001b[0m...\n",
      "Searching for \u001b[31mmolten\u001b[0m...\n",
      "Searching for \u001b[31mmoribund\u001b[0m...\n",
      "Searching for \u001b[31mmorose\u001b[0m...\n",
      "Searching for \u001b[31mnautical\u001b[0m...\n",
      "Searching for \u001b[31mnick\u001b[0m...\n",
      "Searching for \u001b[31mnipping\u001b[0m...\n",
      "Searching for \u001b[31mnonchalance\u001b[0m...\n",
      "Searching for \u001b[31mnostalgia\u001b[0m...\n",
      "Searching for \u001b[31moblique\u001b[0m...\n",
      "Searching for \u001b[31mobservance\u001b[0m...\n",
      "Searching for \u001b[31mobverse\u001b[0m...\n",
      "Searching for \u001b[31mopalescence\u001b[0m...\n",
      "Searching for \u001b[31mordain\u001b[0m...\n",
      "Searching for \u001b[31mostrich\u001b[0m...\n",
      "Searching for \u001b[31moverhaul\u001b[0m...\n",
      "{'.jpg', '.jpeg', '.png'}\n",
      "\u001b[34mData samples:\u001b[0m\n",
      "[\"<div>abstinent</div><div>['æbstinənt]</div>\", '<div class=\"textscroll\">  <div class=\"simsub\"><b>stin()</b>: n. 永续台湾资讯网</div> <div class=\"simsub\"><b>stine()</b>: （人名）.斯泰恩</div> <div class=\"simsub\"><b>tine(tain)</b>: n. 齿, 尖头</div></div>', 'adj. 饮食有度的，有节制的，禁欲的']\n",
      "[\"<div>arachnid</div><div>[ə'ræknid]</div>\", '<div class=\"textscroll\">  <div class=\"simsub\"><b>arac()</b>: 航空航天研究应用中心（Aerospace Research Application Center）\\\\n阵列简化分析电路（Array Reduction Analysis Circuit）</div></div>', 'n. 蜘蛛类节肢动物']\n",
      "['<div>broach</div><div>[brəutʃ]</div>', '<div class=\"textscroll\"><div class=\"sim1\">1: n.裂缝，缺口；v.打破，裂开；违背</div> <div class=\"sim1\">1: n. 胸针</div> <div class=\"sim2\">2: v.漂白</div> <div class=\"sim2\">2: n.(皮肤上的)红斑点；(墨水等)大斑点</div> <div class=\"sim2\">2: v.使稳固，架稳；n.支撑物</div> <div class=\"sim2\">2: a.性急的；无礼的</div> <div class=\"sim2\">2: v.蹲伏，弯腰</div> <div class=\"sim2\">2: n.牢骚，不满</div> <div class=\"sim2\">2: v.传教，讲道</div> <div class=\"sim2\">2: v. 笨手笨脚地)弄坏某事</div> <div class=\"sim2\">2: v. 偷猎，窃取</div> <div class=\"simsub\"><b>roach(rәutʃ)</b>: n. 斜齿鳊\\\\nvt. 使成凹状</div></div>', 'v.开瓶；提出(讨论)']\n",
      "[\"<div>calamity</div><div>[kə'læmiti]</div>\", '<div class=\"textscroll\"> <div class=\"sim2\">2: n.清楚</div> <div class=\"simsub\"><b>amity([\\'æməti])</b>: n.(人或国之间的)友好关系</div> <div class=\"simsub\"><b>alam()</b>: n. 阿拉姆（姓氏）</div> <div class=\"simsub\"><b>alami()</b>: n. (Alami)人名；(阿拉伯、以、法、西、芬)阿拉米</div> <div class=\"simsub\"><b>amit()</b>: n. 阿米特（男子名）</div> <div class=\"simsub\"><b>amity(\\'æmiti)</b>: n. 友好, 亲善关系</div> <div class=\"simsub\"><b>cala(\\'kælә)</b>: n. 熏腌猪肩下肉</div> <div class=\"simsub\"><b>calami(\\'kæləmai)</b>: n. 省藤属植物, 菖蒲, 羽根( calamus的名词复数 )</div> <div class=\"simsub\"><b>mity(\\'maiti)</b>: a. 多螨的</div></div>', 'n.大灾祸，不幸之事']\n",
      "[\"<div>canard</div><div>[kæ'nɑ:d]</div>\", '<div class=\"textscroll\"><div class=\"sim1\">1: n.金丝雀；女歌星</div> <div class=\"sim2\">2: a.率直的</div> <div class=\"sim2\">2: n.胆小鬼</div> <div class=\"sim2\">2: n.危险</div> <div class=\"simsub\"><b>anar()</b>: n. (Anar)人名；(土、阿塞、土库)阿纳尔</div> <div class=\"simsub\"><b>cana(\\'keinә)</b>: n. 迦南（巴勒斯坦北部的一个村庄）</div> <div class=\"simsub\"><b>nard(nɑ:d)</b>: n. 甘松, 甘松油膏\\\\n[医] 甘松香, 甘松</div></div>', 'n. 谣言，假新闻']\n",
      "[\"<div>congregate</div><div>['kɔŋgrigeit]</div>\", '<div class=\"textscroll\">  <div class=\"simsub\"><b>gate(geit)</b>: n. 门, 牌楼, 大门, 通道, 闸\\\\nvt. 装门于\\\\n[计] 门; 栅</div> <div class=\"simsub\"><b>greg(greg)</b>: n. 格雷格（男子名, 等于Gregory）</div> <div class=\"simsub\"><b>rega()</b>: n. 君子（英国音响品牌）；君威（别克的汽车名）</div></div>', 'v.聚集，集合']\n",
      "[\"<div>consonance</div><div>['kɔnsənəns]</div>\", '<div class=\"textscroll\"> <div class=\"sim2\">2: a.调和的，一致的</div> <div class=\"simsub\"><b>ance(eins)</b>: n. 状态；性质；情况</div> <div class=\"simsub\"><b>nanc()</b>:  [医][=non-adrenergic,non-cholinergic]非肾上腺素能，非胆碱能</div> <div class=\"simsub\"><b>nance(\\'nɑ:nsei)</b>: n. 女人气的男人, 搞同性关系的人</div> <div class=\"simsub\"><b>onan()</b>:  油浸自冷（变压器冷却方式，Oil Natural Air Natural）</div> <div class=\"simsub\"><b>sonance(\\'sәjnәns)</b>: n. 有声状态, 声音</div></div>', 'n. 一致，调和；和音']\n",
      "[\"<div>deference</div><div>['defərəns]</div>\", '<div class=\"textscroll\">  <div class=\"simsub\"><b>defer([di\\'fə:])</b>: v.推延；听从</div> <div class=\"simsub\"><b>defer(di\\'fә:)</b>: vi. 推迟, 延期, 听从\\\\nvt. 使推迟, 使延期</div> <div class=\"simsub\"><b>ence()</b>: 具...的行为,状态</div> <div class=\"simsub\"><b>fere(fɪə)</b>: n. 伴侣, 配偶</div> <div class=\"simsub\"><b>ferenc()</b>: n. (Ferenc)人名；(德、匈、罗、塞、波、捷)费伦茨</div> <div class=\"simsub\"><b>ference()</b>:  [人名] 费尔伦斯</div> <div class=\"simsub\"><b>rence(rens)</b>: 盲目降落区的障碍物间隔</div></div>', 'n.敬意，尊重']\n",
      "[\"<div>despondent</div><div>[dis'pɔndənt]</div>\", '<div class=\"textscroll\"><div class=\"sim1\">1: n.被告</div>  <div class=\"simsub\"><b>den([den])</b>: n.兽穴，窝</div> <div class=\"simsub\"><b>dent([dent])</b>: n.缺口，凹痕；v.弄凹</div> <div class=\"simsub\"><b>dent(dent)</b>: n. 凹痕\\\\nvt. 使凹下, 削弱\\\\nvi. 塌陷</div> <div class=\"simsub\"><b>desp()</b>:  [医][=decaspiride]苯螺旋酮，螺葵酮</div> <div class=\"simsub\"><b>despond(di\\'spɒnd)</b>: vi. 沮丧, 失去勇气\\\\nn. 失去勇气, 失望</div> <div class=\"simsub\"><b>espo()</b>:  [医]利血保（重组人类红细胞生成素α注射剂）<抗贫血药></div> <div class=\"simsub\"><b>pond(pɒnd)</b>: n. 池塘\\\\nv. 筑成池塘</div></div>', 'a.失望的，意气消沉的']\n",
      "[\"<div>dilettante</div><div>[`dili'tænti]</div>\", '<div class=\"textscroll\">  <div class=\"simsub\"><b>tan([tæn])</b>: v.鞣(革)</div> <div class=\"simsub\"><b>ante(\\'ænti)</b>: n. 赌注\\\\nv. 下赌注</div> <div class=\"simsub\"><b>dile()</b>:  [医][=drug-induced lupus erythematosus]药物诱发红斑狼疮</div> <div class=\"simsub\"><b>Lett(let)</b>: n. 列托人, 列托语</div> <div class=\"simsub\"><b>letta()</b>:  [地名] [喀麦隆] 莱塔</div> <div class=\"simsub\"><b>Tante()</b>: n. (Tante)人名；(西)坦特</div></div>', 'n.半瓶醋，业余爱好者']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 52.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20250112(01:54:00)\n",
      "[main a576105] update cards.html on date 20250112(01:54:00)\n",
      " 14 files changed, 237 insertions(+), 110 deletions(-)\n",
      " create mode 100644 docs/images/Guinea-asks-bauxite-miners-to-present-local-refinery-plans-by-May.jpeg\n",
      " create mode 100644 docs/images/images-5186a2c0d72bb6ced6ab1099107be5415e7b0709.jpg\n",
      " create mode 100644 docs/images/images-b98b8556ba0a25e88d1579b2ffc16a79c139c395.jpg\n",
      " delete mode 100644 docs/images/images-e232c45b1db6f1af89fc1688113275e37baaa329.jpg\n",
      " delete mode 100644 docs/images/paste-3734f2bb9ca20bb7bd2e2c08e753015ab130772f.jpg\n",
      " create mode 100644 docs/images/paste-62f481c52aa82b6eaa5daa5b73639b2703d21517.jpg\n",
      " create mode 100644 docs/images/paste-6ba10c81153adc71fe825e59ebf26ff0f720307c.png\n",
      " create mode 100644 docs/images/paste-9c4c03b0a6345aaa754b5767e49078d8f7af2fe2.png\n",
      " delete mode 100644 docs/images/paste-c0dd42e84aaaf23e322d5e9b978e109d75b170c6.jpg\n",
      " create mode 100644 docs/images/paste-cfae8b5068b640cdbd4a2e2fbbe3dcdbc08a3ac6.jpg\n",
      " delete mode 100644 docs/images/paste-dea8bb55b40d6a755fc9c8190c7bcfa0cff0cbd4.jpg\n",
      " delete mode 100644 docs/images/paste-ebebd3acddf1167667e5afbea68bd1142c6c8447.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To github.com:pengfeigao2021/anki_html.git\n",
      " + a616b22...a576105 main -> main (forced update)\n"
     ]
    }
   ],
   "source": [
    "# read sqlite3\n",
    "import sqlite3\n",
    "import anki\n",
    "from anki.collection import Collection\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import termcolor\n",
    "import tqdm\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "import pickle\n",
    "# import pandas as pd\n",
    "import editdistance\n",
    "\n",
    "EDICT_PATH = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data/ecdict.csv'\n",
    "MEDIA_FOLDER = '/Users/AlexG/Library/Application Support/Anki2/User 1/collection.media'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/MyPaperNotes.apkg'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/ivl10.apkg'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/极品GRE红宝书.apkg'\n",
    "DATA_PATH = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data'\n",
    "outdir = PKG_PATH + '-decompress'\n",
    "# outdir = '/Users/AlexG/Downloads/ivl10-decompress'\n",
    "# ANKI2_PATH = '/Users/AlexG/Downloads/ivl10-decompress/collection.anki2'\n",
    "ANKI21_PATH = outdir + '/collection.anki21'\n",
    "# EXPORT_PATH_WEBSITE = '/Users/AlexG/Documents/GitHub/typingpractices/website/onepager/onepager/templates/onepager/cards.html'\n",
    "EXPORT_PATH_WEBSITE = '/Users/AlexG/Documents/GitHub/anki_html/docs/gre_cards.html'\n",
    "\n",
    "def unzip_anki(pkg_file_path, outdir):\n",
    "    # Create the extract directory if it doesn't exist\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    # Unzip the .apkg file\n",
    "    with zipfile.ZipFile(PKG_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(outdir)\n",
    "\n",
    "    print(f\"Extracted files to {outdir}\")\n",
    "\n",
    "class EditDistanceFinder(object):\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "        self.load_cache()\n",
    "\n",
    "    def load_cache(self):\n",
    "        cache_path = os.path.join(DATA_PATH, 'edit_distance_cache.pkl')\n",
    "        if os.path.exists(cache_path):\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                self.cache = pickle.load(f)\n",
    "        else:\n",
    "            self.cache = {}\n",
    "\n",
    "    def save_cache(self):\n",
    "        cache_path = os.path.join(DATA_PATH, 'edit_distance_cache.pkl')\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump(self.cache, f)\n",
    "\n",
    "    def find(self, query, \n",
    "             edit_distance_lower=1, \n",
    "             edit_distance_upper=1,\n",
    "             html_class=\"sim1\"):\n",
    "        res = []\n",
    "\n",
    "        # find similar words with cache\n",
    "        similar_words = []\n",
    "        key = (query, edit_distance_lower, edit_distance_upper)\n",
    "        if key in self.cache:\n",
    "            # use cache\n",
    "            similar_words = self.cache[key]\n",
    "        else:\n",
    "            for w in self.words:\n",
    "                if w[0] == query:\n",
    "                    continue\n",
    "                d = editdistance.eval(w[0], query)\n",
    "                if d >= edit_distance_lower and d <= edit_distance_upper:\n",
    "                    similar_words.append((d, w[0], w[2]))\n",
    "            # save cache\n",
    "            self.cache[key] = similar_words\n",
    "        for w in similar_words:\n",
    "            text = f'<div class=\"{html_class}\">{w[0]}: {w[2]}</div>'\n",
    "            res.append(text)\n",
    "        \n",
    "        return ' '.join(res)\n",
    "            \n",
    "        \n",
    "class SubStrFinder(object):\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "        self.load_cache()\n",
    "\n",
    "    def load_cache(self):\n",
    "        cache_path = os.path.join(DATA_PATH, 'sub_word_cache.pkl')\n",
    "        if os.path.exists(cache_path):\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                self.cache = pickle.load(f)\n",
    "        else:\n",
    "            self.cache = {}\n",
    "\n",
    "    def save_cache(self):\n",
    "        cache_path = os.path.join(DATA_PATH, 'sub_word_cache.pkl')\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump(self.cache, f)\n",
    "\n",
    "\n",
    "    def load_edict(self, path):\n",
    "        total_lines = 0\n",
    "        with open(path, 'r') as f:\n",
    "            rd = csv.reader(f)\n",
    "            for cols in tqdm.tqdm(rd):\n",
    "                if len(cols[0]) < 4 or len(cols[3]) == 0:\n",
    "                    continue\n",
    "                if 'abbr.' in cols[3]:\n",
    "                    continue\n",
    "                if cols[0].isupper():\n",
    "                    continue\n",
    "                fields = (cols[0], cols[1], cols[3])\n",
    "                self.words.append(fields)\n",
    "                total_lines += 1\n",
    "        print(f\"Loaded {total_lines} lines from {path}\")\n",
    "\n",
    "    def find(self, query, html_class=\"simsub\"):\n",
    "        res = []\n",
    "        print(\"Searching for {}...\".format(termcolor.colored(query, 'red')))\n",
    "        similar_words = []\n",
    "        key = query.lower()\n",
    "        if key in self.cache:\n",
    "            similar_words = self.cache[key]\n",
    "        else:\n",
    "            for w in self.words:\n",
    "                if w[0] == query:\n",
    "                    continue\n",
    "                if w[0].lower() in query.lower():\n",
    "                    similar_words.append(w)\n",
    "            self.cache[key] = similar_words\n",
    "\n",
    "        for w in similar_words:\n",
    "            text = f'<div class=\"{html_class}\"><b>{w[0]}({w[1]})</b>: {w[2]}</div>'\n",
    "            res.append(text)\n",
    "        \n",
    "        return ' '.join(res)\n",
    "\n",
    "class ImageSrcUrls(object):\n",
    "    def __init__(self):\n",
    "        self.purge_script_path = '/Users/AlexG/Documents/GitHub/anki_html/run_images.sh'\n",
    "        self.git_push_script_path = '/Users/AlexG/Documents/GitHub/anki_html/run_upload.sh'\n",
    "        self.source_image_folder = MEDIA_FOLDER\n",
    "        self.dest_image_folder = '/Users/AlexG/Documents/GitHub/anki_html/docs/images'\n",
    "        self.IMG_PAT = re.compile(r'<img\\b.*\\bsrc=\"')\n",
    "        self.SRC_PAT = re.compile(r'<img\\b.*\\bsrc=\"([^\"]+)\"')\n",
    "        self.srcs = []\n",
    "        self.exts = set()\n",
    "\n",
    "    def replace_image_url(self, text):\n",
    "        res = self.SRC_PAT.search(text)\n",
    "        if res:\n",
    "            for s in res.groups():\n",
    "                self.srcs.append(s)\n",
    "                ext = os.path.splitext(s)[1]\n",
    "                self.exts.add(ext)\n",
    "                print(termcolor.colored(s, 'red'))\n",
    "\n",
    "        return self.IMG_PAT.sub('<img src=\"./images/', text)\n",
    "    \n",
    "    def purge_images(self):\n",
    "        os.system(f\"bash {self.purge_script_path}\") \n",
    "\n",
    "    def git_push(self):\n",
    "        os.system(f\"bash {self.git_push_script_path}\")\n",
    "        \n",
    "    def move_images(self):\n",
    "        self.purge_images()\n",
    "        for src in tqdm.tqdm(self.srcs):\n",
    "            shutil.copyfile(\n",
    "                os.path.join(self.source_image_folder, src), \n",
    "                os.path.join(self.dest_image_folder, src)\n",
    "            )\n",
    "\n",
    "\n",
    "def format_cards_to_html(cards, collection, outpath, max_word_length=-1, max_cards=200):\n",
    "    data = []\n",
    "    image_src = ImageSrcUrls()\n",
    "    for cid in tqdm.tqdm(cards):\n",
    "        card = collection.get_card(cid)\n",
    "        note = card.note()\n",
    "        fields = note.fields\n",
    "        # fields[1] = fields[1][:20]\n",
    "        # fields = [field if len(field) <= max_text_length else field[:max_text_length] + '...' for field in fields]\n",
    "        data.append((card.ivl, fields))\n",
    "\n",
    "    edist_finder = EditDistanceFinder([fields for _, fields in data])\n",
    "    substr_finder = SubStrFinder([fields for _, fields in data])\n",
    "    substr_finder.load_edict(EDICT_PATH)\n",
    "    data.sort()\n",
    "    data = [v for v in data if max_word_length == -1 or len(v[1][0]) <= max_word_length]\n",
    "    data = [[image_src.replace_image_url(field) for field in fields] \n",
    "            for _, fields in data[:max_cards]]\n",
    "    data = [['<div>{}</div><div>{}</div>'.format(fields[0], fields[1]), \n",
    "             '<div class=\"textscroll\">{} {} {}</div>'.format(\n",
    "                edist_finder.find(fields[0]),\n",
    "                edist_finder.find(fields[0],\n",
    "                                  edit_distance_lower=2, \n",
    "                                  edit_distance_upper=2,\n",
    "                                  html_class=\"sim2\"),\n",
    "                substr_finder.find(fields[0]),\n",
    "             ),\n",
    "             fields[2], \n",
    "             ] for fields in tqdm.tqdm(data)]\n",
    "    # save cache\n",
    "    edist_finder.save_cache()\n",
    "    substr_finder.save_cache()\n",
    "    \n",
    "    print(image_src.exts)\n",
    "    print(termcolor.colored('Data samples:', 'blue'))\n",
    "    print('\\n'.join(['{}'.format(v) for v in data[:10]]))\n",
    "    image_src.move_images()\n",
    "    # with open(outpath+'.csv', 'w') as f:\n",
    "    #     wt = csv.writer(f)\n",
    "    #     wt.writerows([v[:3] for v in data])\n",
    "    rows = ['<tr><td class=\"indextd\">{}</td><td>{}</td></tr>\\n'.format(fi, '</td><td class=\"fixwtd\">'.join(field)) for fi, field in enumerate(data)]\n",
    "    html = f'''\n",
    "    <head>\n",
    "        <style>\n",
    "            body {{\n",
    "                background: #202020;\n",
    "            }}\n",
    "\n",
    "            table {{\n",
    "                background-color: #A0A0A0;\n",
    "                --color: #d0d0f5;\n",
    "                margin: 3em;\n",
    "            }}\n",
    "\n",
    "            thead,\n",
    "            tfoot {{\n",
    "                background: var(--color);\n",
    "            }}\n",
    "\n",
    "            tbody tr:nth-child(even) {{\n",
    "                background: color-mix(in srgb, var(--color), transparent 60%);\n",
    "            }}\n",
    "            div, p, td {{\n",
    "                width: 200px;\n",
    "                word-wrap: break-word;\n",
    "                white-space: normal;\n",
    "            }}\n",
    "            td img {{\n",
    "                max-height: 8em;\n",
    "            }}\n",
    "            .fixwtd {{\n",
    "                width: 200px;\n",
    "                word-wrap: break-word;\n",
    "                white-space: normal;\n",
    "            }}\n",
    "            .indextd {{\n",
    "                width: 10px;\n",
    "                word-wrap: break-word;\n",
    "                white-space: normal;\n",
    "            }}\n",
    "            td {{\n",
    "                border: 1px ridge #333333;\n",
    "            }}\n",
    "            img {{\n",
    "                max-height: 5em;\n",
    "                max-width: 5em;\n",
    "            }}\n",
    "            .sim1 {{\n",
    "                width: 26em;\n",
    "                background-color: #eadcb7;\n",
    "            }}\n",
    "            .sim2 {{\n",
    "                width: 26em;\n",
    "                background-color: #e99a06;\n",
    "            }}\n",
    "            .simsub {{\n",
    "                width: 26em;\n",
    "                background-color: #afcd95;\n",
    "            }}\n",
    "            b {{\n",
    "                background-color: #f1c40f;\n",
    "            }}\n",
    "            .textscroll {{\n",
    "                height: 3em;\n",
    "                width: 27em;\n",
    "                border: 1px solid #ccc;\n",
    "                overflow: auto;\n",
    "            }}\n",
    "        </style>\n",
    "        <link rel=\"stylesheet\" href=\"css/fancy-button.css\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <button onclick=\"changeWidth(0.5)\" class=\"button-74\">Half Column Width</button>\n",
    "        <button onclick=\"hideRows(10)\" class=\"button-73 fix-width-button\">Hide Rows</button>\n",
    "        <table id=\"cards\">\n",
    "          {\"\".join(rows)}\n",
    "        </table>\n",
    "        <script src=\"js/change-width.js\"></script>\n",
    "        <script src=\"js/hide-rows.js\"></script>\n",
    "    </body>\n",
    "    '''\n",
    "    with open(outpath, 'w') as f:\n",
    "        f.write(html)\n",
    "\n",
    "    # git push\n",
    "    image_src.git_push()\n",
    "\n",
    "unzip_anki(PKG_PATH, outdir)\n",
    "col = Collection(ANKI21_PATH)\n",
    "# Fetch all decks in the collection\n",
    "decks = col.decks.all()\n",
    "print(\"Decks in collection:\")\n",
    "for deck in decks:\n",
    "    print(f\"Deck ID: {deck['id']}, Name: {deck['name']}\")\n",
    "\n",
    "try:\n",
    "    print(dir(col))\n",
    "    print('total card count: ', col.card_count())\n",
    "    # print('note count: ', col.node_count())\n",
    "\n",
    "    # query = 'prop:ivl<=30'\n",
    "    query = ''\n",
    "    cards = col.find_cards(query)\n",
    "    print(termcolor.colored(f'query {query} find {len(cards)} cards', 'green'))\n",
    "\n",
    "    outpath = os.path.join(outdir, 'cards.html')\n",
    "    # format_cards_to_html(cards, col, outpath)\n",
    "    format_cards_to_html(cards, col, EXPORT_PATH_WEBSITE,\n",
    "                         max_word_length=-1)\n",
    "\n",
    "    # Fetch all notes (cards content) in the collection\n",
    "    # print(\"\\nNotes in collection:\")\n",
    "    # for note_id in col.db.list(\"select id from notes\"):\n",
    "    #     note = col.get_note(note_id)\n",
    "    #     fields = note.fields\n",
    "    #     print(f\"Note ID: {note_id}, Fields: {fields}\")\n",
    "\n",
    "    # # Fetch all cards (metadata) in the collection\n",
    "    # print(\"\\nCards in collection:\")\n",
    "    # for card_id in col.db.list(\"select id from cards\"):\n",
    "    #     card = col.get_card(card_id)\n",
    "    #     print(f\"Card ID: {card.id}, Note ID: {card.nid}, Deck ID: {card.did}, Type: {card.type}\")\n",
    "\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    # Close the collection\n",
    "    col.close()\n",
    "\n",
    "col.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------\n",
    "# Offline batch similar words\n",
    "# -------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read sqlite3\n",
    "import sqlite3\n",
    "import anki\n",
    "from anki.collection import Collection\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import termcolor\n",
    "import tqdm\n",
    "import csv\n",
    "import shutil\n",
    "# import pandas as pd\n",
    "import editdistance\n",
    "\n",
    "EDICT_PATH = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data/ecdict.csv'\n",
    "MEDIA_FOLDER = '/Users/AlexG/Library/Application Support/Anki2/User 1/collection.media'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/MyPaperNotes.apkg'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/ivl10.apkg'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/极品GRE红宝书.apkg'\n",
    "outdir = PKG_PATH + '-decompress'\n",
    "# outdir = '/Users/AlexG/Downloads/ivl10-decompress'\n",
    "# ANKI2_PATH = '/Users/AlexG/Downloads/ivl10-decompress/collection.anki2'\n",
    "ANKI21_PATH = outdir + '/collection.anki21'\n",
    "# EXPORT_PATH_WEBSITE = '/Users/AlexG/Documents/GitHub/typingpractices/website/onepager/onepager/templates/onepager/cards.html'\n",
    "EXPORT_PATH_WEBSITE = '/Users/AlexG/Documents/GitHub/anki_html/docs/gre_cards.html'\n",
    "\n",
    "def unzip_anki(pkg_file_path, outdir):\n",
    "    # Create the extract directory if it doesn't exist\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    # Unzip the .apkg file\n",
    "    with zipfile.ZipFile(PKG_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(outdir)\n",
    "\n",
    "    print(f\"Extracted files to {outdir}\")\n",
    "\n",
    "class EditDistanceFinder(object):\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "\n",
    "    def find(self, query, \n",
    "             edit_distance_lower=1, \n",
    "             edit_distance_upper=1,\n",
    "             html_class=\"sim1\"):\n",
    "        res = []\n",
    "        for w in self.words:\n",
    "            if w[0] == query:\n",
    "                continue\n",
    "            d = editdistance.eval(w[0], query)\n",
    "            if d >= edit_distance_lower and d <= edit_distance_upper:\n",
    "                text = f'<div class=\"{html_class}\">{w[0]}: {w[2]}</div>'\n",
    "                res.append(text)\n",
    "        \n",
    "        return ' '.join(res)\n",
    "            \n",
    "        \n",
    "class SubStrFinder(object):\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "\n",
    "    def load_edict(self, path):\n",
    "        total_lines = 0\n",
    "        with open(path, 'r') as f:\n",
    "            rd = csv.reader(f)\n",
    "            for cols in tqdm.tqdm(rd):\n",
    "                if len(cols[0]) < 4 or len(cols[3]) == 0:\n",
    "                    continue\n",
    "                if 'abbr.' in cols[3]:\n",
    "                    continue\n",
    "                if cols[0].isupper():\n",
    "                    continue\n",
    "                fields = (cols[0], cols[1], cols[3])\n",
    "                self.words.append(fields)\n",
    "                total_lines += 1\n",
    "        print(f\"Loaded {total_lines} lines from {path}\")\n",
    "\n",
    "    def find(self, query, html_class=\"simsub\"):\n",
    "        res = []\n",
    "        print(\"Searching for {}...\".format(termcolor.colored(query, 'red')))\n",
    "        for w in tqdm.tqdm(self.words):\n",
    "            if w[0] == query:\n",
    "                continue\n",
    "            if w[0].lower() in query.lower():\n",
    "                text = f'<div class=\"{html_class}\"><b>{w[0]}({w[1]})</b>: {w[2]}</div>'\n",
    "                res.append(text)\n",
    "        \n",
    "        return ' '.join(res)\n",
    "\n",
    "class ImageSrcUrls(object):\n",
    "    def __init__(self):\n",
    "        self.purge_script_path = '/Users/AlexG/Documents/GitHub/anki_html/run_images.sh'\n",
    "        self.git_push_script_path = '/Users/AlexG/Documents/GitHub/anki_html/run_upload.sh'\n",
    "        self.source_image_folder = MEDIA_FOLDER\n",
    "        self.dest_image_folder = '/Users/AlexG/Documents/GitHub/anki_html/docs/images'\n",
    "        self.IMG_PAT = re.compile(r'<img\\b.*\\bsrc=\"')\n",
    "        self.SRC_PAT = re.compile(r'<img\\b.*\\bsrc=\"([^\"]+)\"')\n",
    "        self.srcs = []\n",
    "        self.exts = set()\n",
    "\n",
    "    def replace_image_url(self, text):\n",
    "        res = self.SRC_PAT.search(text)\n",
    "        if res:\n",
    "            for s in res.groups():\n",
    "                self.srcs.append(s)\n",
    "                ext = os.path.splitext(s)[1]\n",
    "                self.exts.add(ext)\n",
    "                print(termcolor.colored(s, 'red'))\n",
    "\n",
    "        return self.IMG_PAT.sub('<img src=\"./images/', text)\n",
    "    \n",
    "    def purge_images(self):\n",
    "        os.system(f\"bash {self.purge_script_path}\") \n",
    "\n",
    "    def git_push(self):\n",
    "        os.system(f\"bash {self.git_push_script_path}\")\n",
    "        \n",
    "    def move_images(self):\n",
    "        self.purge_images()\n",
    "        for src in tqdm.tqdm(self.srcs):\n",
    "            shutil.copyfile(\n",
    "                os.path.join(self.source_image_folder, src), \n",
    "                os.path.join(self.dest_image_folder, src)\n",
    "            )\n",
    "\n",
    "\n",
    "def format_cards_to_html(cards, collection, outpath, max_word_length=-1, max_cards=200):\n",
    "    data = []\n",
    "    image_src = ImageSrcUrls()\n",
    "    for cid in tqdm.tqdm(cards):\n",
    "        card = collection.get_card(cid)\n",
    "        note = card.note()\n",
    "        fields = note.fields\n",
    "        # fields[1] = fields[1][:20]\n",
    "        # fields = [field if len(field) <= max_text_length else field[:max_text_length] + '...' for field in fields]\n",
    "        data.append((card.ivl, fields))\n",
    "\n",
    "    edist_finder = EditDistanceFinder([fields for _, fields in data])\n",
    "    substr_finder = SubStrFinder([fields for _, fields in data])\n",
    "    substr_finder.load_edict(EDICT_PATH)\n",
    "    data.sort()\n",
    "    data = [v for v in data if max_word_length == -1 or len(v[1][0]) <= max_word_length]\n",
    "    data = [[image_src.replace_image_url(field) for field in fields] \n",
    "            for _, fields in data[:max_cards]]\n",
    "    data = [['<div>{}</div><div>{}</div>'.format(fields[0], fields[1]), \n",
    "             '<div class=\"textscroll\">{} {} {}</div>'.format(\n",
    "                edist_finder.find(fields[0]),\n",
    "                edist_finder.find(fields[0],\n",
    "                                  edit_distance_lower=2, \n",
    "                                  edit_distance_upper=2,\n",
    "                                  html_class=\"sim2\"),\n",
    "                substr_finder.find(fields[0]),\n",
    "             ),\n",
    "             fields[2], \n",
    "             ] for fields in data]\n",
    "    print(image_src.exts)\n",
    "    print(termcolor.colored('Data samples:', 'blue'))\n",
    "    print('\\n'.join(['{}'.format(v) for v in data[:10]]))\n",
    "\n",
    "unzip_anki(PKG_PATH, outdir)\n",
    "col = Collection(ANKI21_PATH)\n",
    "# Fetch all decks in the collection\n",
    "decks = col.decks.all()\n",
    "print(\"Decks in collection:\")\n",
    "for deck in decks:\n",
    "    print(f\"Deck ID: {deck['id']}, Name: {deck['name']}\")\n",
    "\n",
    "try:\n",
    "    print(dir(col))\n",
    "    print('total card count: ', col.card_count())\n",
    "    # print('note count: ', col.node_count())\n",
    "\n",
    "    # query = 'prop:ivl<=30'\n",
    "    query = ''\n",
    "    cards = col.find_cards(query)\n",
    "    print(termcolor.colored(f'query {query} find {len(cards)} cards', 'green'))\n",
    "\n",
    "    outpath = os.path.join(outdir, 'cards.html')\n",
    "    # format_cards_to_html(cards, col, outpath)\n",
    "    format_cards_to_html(cards, col, EXPORT_PATH_WEBSITE,\n",
    "                         max_word_length=-1)\n",
    "\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    # Close the collection\n",
    "    col.close()\n",
    "\n",
    "col.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====================\n",
    "# build all words list\n",
    "# ====================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decks in collection:\n",
      "Deck ID: 1, Name: Default\n",
      "Deck ID: 1685695741424, Name: 极品GRE红宝书\n",
      "total card count:  7513\n",
      "\u001b[32mquery  find 7513 cards\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7513/7513 [00:02<00:00, 3308.24it/s]\n",
      "770612it [00:03, 229250.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 750698 lines from /Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data/ecdict.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750698/750698 [00:00<00:00, 1021786.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 758211\n"
     ]
    }
   ],
   "source": [
    "# read sqlite3\n",
    "import sqlite3\n",
    "import anki\n",
    "from anki.collection import Collection\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import termcolor\n",
    "import tqdm\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "# import pandas as pd\n",
    "import editdistance\n",
    "\n",
    "EDICT_PATH = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data/ecdict.csv'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/极品GRE红宝书.apkg'\n",
    "outdir = PKG_PATH + '-decompress'\n",
    "ANKI21_PATH = outdir + '/collection.anki21'\n",
    "\n",
    "class SubStrFinder(object):\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "\n",
    "    def load_edict(self, path):\n",
    "        total_lines = 0\n",
    "        with open(path, 'r') as f:\n",
    "            rd = csv.reader(f)\n",
    "            for cols in tqdm.tqdm(rd):\n",
    "                if len(cols[0]) < 4 or len(cols[3]) == 0:\n",
    "                    continue\n",
    "                if 'abbr.' in cols[3]:\n",
    "                    continue\n",
    "                if cols[0].isupper():\n",
    "                    continue\n",
    "                fields = (cols[0], cols[1], cols[3])\n",
    "                self.words.append(fields)\n",
    "                total_lines += 1\n",
    "        print(f\"Loaded {total_lines} lines from {path}\")\n",
    "\n",
    "\n",
    "col = Collection(ANKI21_PATH)\n",
    "# Fetch all decks in the collection\n",
    "decks = col.decks.all()\n",
    "all_words_set = set()\n",
    "all_words = []\n",
    "gre_words = []\n",
    "print(\"Decks in collection:\")\n",
    "for deck in decks:\n",
    "    print(f\"Deck ID: {deck['id']}, Name: {deck['name']}\")\n",
    "\n",
    "try:\n",
    "    print('total card count: ', col.card_count())\n",
    "\n",
    "    query = ''\n",
    "    cards = col.find_cards(query)\n",
    "    print(termcolor.colored(f'query {query} find {len(cards)} cards', 'green'))\n",
    "\n",
    "    for cid in tqdm.tqdm(cards):\n",
    "        card = col.get_card(cid)\n",
    "        note = card.note()\n",
    "        fields = note.fields\n",
    "        all_words_set.add(fields[0])\n",
    "        gre_words.append(fields[0])\n",
    "    substr_finder = SubStrFinder([])\n",
    "    substr_finder.load_edict(EDICT_PATH)\n",
    "    for w in tqdm.tqdm(substr_finder.words):\n",
    "        all_words_set.add(w)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    # Close the collection\n",
    "    col.close()\n",
    "\n",
    "col.close()\n",
    "\n",
    "all_words = list(all_words_set)\n",
    "print(f\"Total words: {len(all_words)}\")\n",
    "with open('/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/all_words.json', 'w') as f:\n",
    "    json.dump(all_words, f)\n",
    "with open('/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/gre_words.json', 'w') as f:\n",
    "    json.dump(gre_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from /Users/AlexG/Documents/GitHub/mindmaps/anki_tools/gre_words.json and /Users/AlexG/Documents/GitHub/mindmaps/anki_tools/all_words.json\n",
      "loaded 7513 gre words and 758211 all words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 152303449/5696439243 [01:34<57:08, 1616872.64it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m N_gre \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(gre_words)\n\u001b[1;32m     40\u001b[0m N_all \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_words)\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n1, n2 \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(itertools\u001b[38;5;241m.\u001b[39mproduct(\u001b[38;5;28mrange\u001b[39m(N_gre), \u001b[38;5;28mrange\u001b[39m(N_all)), total\u001b[38;5;241m=\u001b[39mN_gre \u001b[38;5;241m*\u001b[39m N_all):\n\u001b[1;32m     42\u001b[0m     w1 \u001b[38;5;241m=\u001b[39m gre_words[n1][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     43\u001b[0m     w2 \u001b[38;5;241m=\u001b[39m all_words[n2][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/tqdm/std.py:1187\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m n \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlast_print_n\u001b[49m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminiters:\n\u001b[1;32m   1188\u001b[0m     cur_t \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m   1189\u001b[0m     dt \u001b[38;5;241m=\u001b[39m cur_t \u001b[38;5;241m-\u001b[39m last_print_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "# read sqlite3\n",
    "import sqlite3\n",
    "import anki\n",
    "from anki.collection import Collection\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import termcolor\n",
    "import tqdm\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "# import pandas as pd\n",
    "import editdistance\n",
    "# a = [1,2,3]\n",
    "# b= itertools.product(a, a)\n",
    "# print(list(b))\n",
    "\n",
    "gre_words_path = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/gre_words.json'\n",
    "all_words_path = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/all_words.json'\n",
    "print('loading from {} and {}'.format(gre_words_path, all_words_path))\n",
    "gre_words = json.load(open(gre_words_path))\n",
    "all_words = json.load(open(all_words_path))\n",
    "print('loaded {} gre words and {} all words'.format(len(gre_words), len(all_words)))\n",
    "\n",
    "def find(self, query, html_class=\"simsub\"):\n",
    "    res = []\n",
    "    print(\"Searching for {}...\".format(termcolor.colored(query, 'red')))\n",
    "    for w in tqdm.tqdm(self.words):\n",
    "        if w[0] == query:\n",
    "            continue\n",
    "        if w[0].lower() in query.lower():\n",
    "            text = f'<div class=\"{html_class}\"><b>{w[0]}({w[1]})</b>: {w[2]}</div>'\n",
    "            res.append(text)\n",
    "    \n",
    "    return ' '.join(res)\n",
    "\n",
    "N_gre = len(gre_words)\n",
    "N_all = len(all_words)\n",
    "for n1, n2 in tqdm.tqdm(itertools.product(range(N_gre), range(N_all)), total=N_gre * N_all):\n",
    "    w1 = gre_words[n1][0]\n",
    "    w2 = all_words[n2][0]\n",
    "    # d = editdistance.eval(w1, w2)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
