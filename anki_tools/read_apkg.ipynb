{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted files to /Users/AlexG/Downloads/极品GRE红宝书.apkg-decompress\n",
      "Decks in collection:\n",
      "Deck ID: 1685695741424, Name: 极品GRE红宝书\n",
      "Deck ID: 1, Name: Default\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7513/7513 [00:00<00:00, 14877.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mLast review time: time.struct_time(tm_year=2025, tm_mon=2, tm_mday=5, tm_hour=20, tm_min=54, tm_sec=8, tm_wday=2, tm_yday=36, tm_isdst=0)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7513/7513 [00:00<00:00, 10467.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[ivl=1]\u001b[0m \u001b[32m['canard', \"[kæ'nɑ:d]\", 'n. 谣言，假新闻', '<b>【英】</b> n. 谣言，假新闻(a false malicious report)<br><b>【记】</b> 和下面的canary一起 记，联想：金丝雀特 别会叫，似乎造谣(ca nary makes canard )', 'canard (n.) <br>before 1850, from French canard \"a hoax,\" literally \"a duck\" (from Old French quanart, probably echoic of a duck\\'s quack); said by Littré to be from the phrase vendre un canard à moitié \"to half-sell a duck,\" thus, from some long-forgotten joke, \"to cheat.\"', 'n. (故意误导的)谣言, 误传<br>【例】There are stories canarding about her 若干关于她的留言正在流传着.<br>【源】原来是象声词, 鸭子的叫声, 后来指一种半价出售鸭子的方式, 实际上是一种欺骗手段, 最后演化成谣言的意思.<br>【记】读: 加拿大. 经常有关于加拿大留学, 移民的谣传.', '【记】音：坑吶，原来是个坑吶(网络用语“坑”指编造的假文章，用来骗人玩的)；canal 各种渠道总是用来传播谣言', '[sound:canard.mp3]']\u001b[0m\n",
      "\u001b[31m[ivl=1]\u001b[0m \u001b[32m['candid', \"['kændid]\", 'a.率直的', \"<b>【英】</b> adj. 率直的(not hiding one's thoughts)<br><b>【考】</b> 反义词: dissembling(掩饰的)<br><b>【记】</b> cand(白,发光)+id→ 白的→坦白的\", 'candid (adj.) <br>1620s, \"white,\" from Latin candidum \"white; pure; sincere, honest, upright,\" from candere \"to shine,\" from PIE root *kand- \"to glow, to shine\" (see candle). In English, metaphoric extension to \"frank\" first recorded 1670s (compare French candide \"open, frank, ingenuous, sincere\"). Of photography, 1929. Related: Candidly; candidness.', 'adj. 率直的(毫无保留地); 公正的; 坦白的<br>【例】To be candid, I think you acted foolishly 坦率地说, 你做得很愚蠢.<br>【记】cand白的, 纯洁的－坦白的can＋did: 能做事的人是公正的.<br>【反】dissembling(adj 掩饰的)', '【记】作为candidate候选人要求公正坦率无偏见正直； candle 象烛光一样光明正大', '[sound:candid.mp3]']\u001b[0m\n",
      "\u001b[31m[ivl=1]\u001b[0m \u001b[32m['congregate', \"['kɔŋgrigeit]\", 'v.聚集，集合', \"<b>【英】</b> v,聚集，集合 (to gather into a crowd; assemble)<br><b>【记】</b> con＋greg(群体)＋at e→聚成群体→集合<br><b>【同】</b> 同根词：aggregate( 聚集, 合计); gregarious(喜社交的 , 爱合群的)派生词: congregation (n. 集合, 会合)<br><b>【例】</b> Congregate in the park to hear the major's speech.\", 'congregate (v.) <br>mid-15c., from Latin congregatus \"flocking together,\" past participle of congregare \"to herd together, collect in a flock, swarm; assemble,\" from com- \"together\" (see com-) + gregare \"to collect into a flock, gather,\" from grex (genitive gregis) \"a flock\" (see gregarious). Related: Congregated; congregating.', 'v. 聚集, 集合(成一组、一群或集会)assemble<br>【记】con共同, greg群体－共同聚成群体－集合con共同, GRE, gate门: 考GRE的同学们共同聚集在美国大使馆的门前示威: 取缔GRE! 还我2400!<br>【参】aggregate(n /v 聚集, 合计); gregarious(adj 社交的; 群居的)', '【记】greg 聚集；con 大家 + gre + gate GRE的大门口聚集着很多人，都想通过这道关<br>【相关】aggregate 聚集，congregate 聚集，egregious 超群的(distinguished)，gregarious 群居的, segregate隔离(separate)', '[sound:congregate.mp3]']\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# extract package\n",
    "import sqlite3\n",
    "import anki\n",
    "from anki.collection import Collection\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import termcolor\n",
    "import tqdm\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "import pickle\n",
    "# import pandas as pd\n",
    "import editdistance\n",
    "\n",
    "EDICT_PATH = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data/ecdict.csv'\n",
    "MEDIA_FOLDER = '/Users/AlexG/Library/Application Support/Anki2/User 1/collection.media'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/MyPaperNotes.apkg'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/ivl10.apkg'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/极品GRE红宝书.apkg'\n",
    "DATA_PATH = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data'\n",
    "outdir = PKG_PATH + '-decompress'\n",
    "# outdir = '/Users/AlexG/Downloads/ivl10-decompress'\n",
    "# ANKI2_PATH = '/Users/AlexG/Downloads/ivl10-decompress/collection.anki2'\n",
    "ANKI21_PATH = outdir + '/collection.anki21'\n",
    "# EXPORT_PATH_WEBSITE = '/Users/AlexG/Documents/GitHub/typingpractices/website/onepager/onepager/templates/onepager/cards.html'\n",
    "EXPORT_PATH_WEBSITE = '/Users/AlexG/Documents/GitHub/anki_html/docs/gre_cards.html'\n",
    "\n",
    "def unzip_anki(pkg_file_path, outdir):\n",
    "    # Create the extract directory if it doesn't exist\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    # Unzip the .apkg file\n",
    "    with zipfile.ZipFile(PKG_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(outdir)\n",
    "\n",
    "    print(f\"Extracted files to {outdir}\")\n",
    "\n",
    "def get_lastest_review_time(col, cards):\n",
    "    rev_times = []\n",
    "    for card in tqdm.tqdm(cards):\n",
    "        timestamp_ms = col.db.scalar(\n",
    "            \"SELECT max(id) FROM revlog WHERE cid = {} AND type IN (1, 3)\".format(card)\n",
    "        )\n",
    "        rev_times.append(timestamp_ms)\n",
    "    mt = max(rev_times)\n",
    "    return time.localtime(mt/1000)\n",
    "\n",
    "try:\n",
    "    unzip_anki(PKG_PATH, outdir)\n",
    "    col = Collection(ANKI21_PATH)\n",
    "    # Fetch all decks in the collection\n",
    "    decks = col.decks.all()\n",
    "    print(\"Decks in collection:\")\n",
    "    for deck in decks:\n",
    "        print(f\"Deck ID: {deck['id']}, Name: {deck['name']}\")\n",
    "    # query = 'prop:ivl<=30'\n",
    "    cards = col.find_cards('')\n",
    "\n",
    "    # max review time\n",
    "    last_review_time = get_lastest_review_time(col, cards)\n",
    "    print(termcolor.colored(f\"Last review time: {last_review_time}\", 'green'))\n",
    "\n",
    "    # top 3 cards by ivl\n",
    "    data = []\n",
    "    for cid in tqdm.tqdm(cards):\n",
    "        card = col.get_card(cid)\n",
    "        note = card.note()\n",
    "        fields = note.fields\n",
    "        data.append((card.ivl, fields))\n",
    "    data.sort()\n",
    "    for ivl, c in data[:3]:\n",
    "        print(termcolor.colored('[ivl={}]'.format(ivl), 'red'), \n",
    "              termcolor.colored('{}'.format(c), 'green'))\n",
    "\n",
    "except Exception as e:\n",
    "    error = '{}'.format(e)\n",
    "    print('exceptin while extracting apkg: {}...'.format(e[:12]))\n",
    "\n",
    "col.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please write a story using the following comma separated words: \"canard,candid,congregate,deference,dilettante,disparate,emollient,feign,flabby,gaucherie,havoc,impassive,indignant,lassitude,malcontent,meek,pallid,precipitant,precipitation,shingle,somnolent,supple,thwart,abound,abstinent,acarpous,ancillary,appellation,apprise,arachnid,astray,awning,badinage,bale,banal,bauble,bauxite,belch,beseech,blight,braid,broach,brocade,cactus,calamity,calipers,cantankerous,capsize,carapace,caricature,celibate,cephalic,checkered,chirp,clairvoyance,colander,commonwealth,conclave,concoct,confection,conjugal,consummate,contiguous,contumacious,copious,coruscate,countenance,crux,curator,curtail,dalliance,dampen,deciduous,deign,demur,despondent,desultory,diaphanous,diffident,disclaimer,disconcert,discriminatory,disquisition,dissemble,distrait,divagate,divulge,doff,doggo,domicile,dullard,earthly,easel,effete,egregious,emphatic,enamored,encomium,enervate,engrave\"\n"
     ]
    }
   ],
   "source": [
    "# write story prompt\n",
    "words = []\n",
    "for ivl, c in data[:100]:\n",
    "    words.append(c[0])\n",
    "\n",
    "print('Please write a story using the following comma separated words, and please also provide Chinese translation of the provided words in parentheses: \"{}\"'.format(','.join(words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_backend', '_build_sort_mode', '_check_backend_undo_status', '_clear_caches', '_deprecated_aliases', '_deprecated_attributes', '_load_scheduler', '_logRem', '_pb_search_separator', '_remNotes', '_startReps', '_supported_scheduler_versions', 'abort_media_sync', 'abort_sync', 'addNote', 'add_custom_undo_entry', 'add_image_occlusion_note', 'add_image_occlusion_notetype', 'add_note', 'add_notes', 'after_note_updates', 'all_browser_columns', 'all_config', 'autosave', 'await_backup_completion', 'backend', 'browser_row_for_id', 'build_search_string', 'cardStats', 'card_count', 'card_ids_of_note', 'card_stats', 'card_stats_data', 'close', 'close_for_full_sync', 'compare_answer', 'compute_memory_state', 'conf', 'create_backup', 'crt', 'db', 'decks', 'default_deck_for_notetype', 'defaults_for_adding', 'emptyCids', 'export_anki_package', 'export_card_csv', 'export_collection_package', 'export_note_csv', 'extract_cloze_for_typing', 'field_names_for_note_ids', 'find_and_replace', 'find_cards', 'find_dupes', 'find_notes', 'fix_integrity', 'flush', 'format_timespan', 'full_upload_or_download', 'fuzz_delta', 'genCards', 'get_aux_notetype_config', 'get_aux_template_config', 'get_browser_column', 'get_card', 'get_config', 'get_config_bool', 'get_config_string', 'get_csv_metadata', 'get_empty_cards', 'get_image_for_occlusion', 'get_image_occlusion_note', 'get_note', 'get_preferences', 'group_searches', 'i18n_resources', 'import_anki_package', 'import_csv', 'import_json_file', 'import_json_string', 'initialize_backend_logging', 'is_empty', 'join_searches', 'latest_progress', 'load_browser_card_columns', 'load_browser_note_columns', 'log', 'media', 'media_sync_status', 'merge_undo_entries', 'mod', 'mod_schema', 'models', 'name', 'newNote', 'new_note', 'nextID', 'note_count', 'op_made_changes', 'optimize', 'path', 'redo', 'register_deprecated_aliases', 'register_deprecated_attributes', 'remNotes', 'remove_cards_and_orphaned_notes', 'remove_config', 'remove_notes', 'remove_notes_by_card', 'render_markdown', 'reopen', 'replace_in_search_node', 'reset', 'save', 'sched', 'sched_ver', 'schema_changed', 'server', 'setMod', 'set_aux_notetype_config', 'set_aux_template_config', 'set_browser_card_columns', 'set_browser_note_columns', 'set_config', 'set_config_bool', 'set_config_string', 'set_deck', 'set_preferences', 'set_schema_modified', 'set_user_flag_for_cards', 'set_v3_scheduler', 'set_wants_abort', 'startTimebox', 'stats', 'studied_today', 'sync_collection', 'sync_login', 'sync_media', 'sync_status', 'tags', 'timeboxReached', 'tr', 'undo', 'undo_name', 'undo_status', 'updateFieldCache', 'update_card', 'update_cards', 'update_image_occlusion_note', 'update_note', 'update_notes', 'upgrade_to_v2_scheduler', 'usn', 'v3_scheduler', 'weakref']\n",
      "total card count:  7513\n",
      "\u001b[32mquery  find 7513 cards\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7513/7513 [00:00<00:00, 9186.80it/s] \n",
      "770612it [00:03, 195053.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 750698 lines from /Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data/ecdict.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing word: bale:  16%|█▋        | 33/200 [00:00<00:04, 35.74it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mpaste-1438c0813bb4175e1f903f799b7063aaa3187c13.jpg\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing word: beseech:  19%|█▉        | 38/200 [00:01<00:04, 33.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mGuinea-asks-bauxite-miners-to-present-local-refinery-plans-by-May.jpeg\u001b[0m\n",
      "\u001b[31mpaste-9c4c03b0a6345aaa754b5767e49078d8f7af2fe2.png\u001b[0m\n",
      "\u001b[31mpaste-3523e57ad956c937882df3f3007eeec60c6a0a3d.jpg\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing word: checkered:  26%|██▌       | 52/200 [00:01<00:05, 24.98it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mpaste-cfae8b5068b640cdbd4a2e2fbbe3dcdbc08a3ac6.jpg\u001b[0m\n",
      "\u001b[31mimages-b98b8556ba0a25e88d1579b2ffc16a79c139c395.jpg\u001b[0m\n",
      "\u001b[31mpaste-c10718275d766cfd76becc4dbf268c97de695ebb.png\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing word: engrave:  50%|████▉     | 99/200 [00:02<00:01, 75.18it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mpaste-6ba10c81153adc71fe825e59ebf26ff0f720307c.png\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing word: gripe:  64%|██████▍   | 129/200 [00:02<00:01, 61.20it/s]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mpaste-d09601d83066b3063f7e388121fa2fedee1d1c0f.jpg\u001b[0m\n",
      "\u001b[31mpaste-c6d9ef7b846c57cb5c1ef5adb746da2c61a00a84.png\u001b[0m\n",
      "\u001b[31mpaste-b8739423b0a7ba9cfebee845f29554a1cafe1f47.jpg\u001b[0m\n",
      "\u001b[31mpaste-874ee63383f0585352d353c95290c6159defebf7.jpg\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing word: inured:  80%|████████  | 160/200 [00:03<00:01, 38.24it/s]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mpaste-4d6dcb69a7f61358ac70d067530f28f683cf7a2a.jpg\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing word: mesmerism:  96%|█████████▌| 192/200 [00:04<00:00, 28.56it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mimages-5186a2c0d72bb6ced6ab1099107be5415e7b0709.jpg\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing word: morose: 100%|██████████| 200/200 [00:05<00:00, 38.07it/s]    \n",
      "ls: images: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mpaste-62f481c52aa82b6eaa5daa5b73639b2703d21517.jpg\u001b[0m\n",
      "{'.jpg', '.png', '.jpeg'}\n",
      "\u001b[34mData samples:\u001b[0m\n",
      "[\"<div>canard</div><div>[kæ'nɑ:d]</div>\", '<div class=\"textscroll\"><div class=\"sim1\">canary: n.金丝雀；女歌星</div> <div class=\"sim2\">candid: a.率直的</div> <div class=\"sim2\">coward: n.胆小鬼</div> <div class=\"sim2\">hazard: n.危险</div> <div class=\"simsub\"><b>anar()</b>: n. (Anar)人名；(土、阿塞、土库)阿纳尔</div> <div class=\"simsub\"><b>cana(\\'keinә)</b>: n. 迦南（巴勒斯坦北部的一个村庄）</div> <div class=\"simsub\"><b>nard(nɑ:d)</b>: n. 甘松, 甘松油膏\\\\n[医] 甘松香, 甘松</div></div>', 'n. 谣言，假新闻']\n",
      "[\"<div>candid</div><div>['kændid]</div>\", '<div class=\"textscroll\"> <div class=\"sim2\">candor: n.坦白，率直</div> <div class=\"sim2\">rancid: a.不新鲜的，变味的</div> <div class=\"sim2\">canard: n. 谣言，假新闻</div> <div class=\"simsub\"><b>andi()</b>: 安迪（名字）</div> <div class=\"simsub\"><b>cand()</b>: [化] 萤石</div> <div class=\"simsub\"><b>candi()</b>: n. (Candi)人名；(意)坎迪</div></div>', 'a.率直的']\n",
      "[\"<div>congregate</div><div>['kɔŋgrigeit]</div>\", '<div class=\"textscroll\">  <div class=\"simsub\"><b>gate(geit)</b>: n. 门, 牌楼, 大门, 通道, 闸\\\\nvt. 装门于\\\\n[计] 门; 栅</div> <div class=\"simsub\"><b>greg(greg)</b>: n. 格雷格（男子名, 等于Gregory）</div> <div class=\"simsub\"><b>rega()</b>: n. 君子（英国音响品牌）；君威（别克的汽车名）</div></div>', 'v.聚集，集合']\n",
      "[\"<div>deference</div><div>['defərəns]</div>\", '<div class=\"textscroll\">  <div class=\"simsub\"><b>defer([di\\'fə:])</b>: v.推延；听从</div> <div class=\"simsub\"><b>defer(di\\'fә:)</b>: vi. 推迟, 延期, 听从\\\\nvt. 使推迟, 使延期</div> <div class=\"simsub\"><b>ence()</b>: 具...的行为,状态</div> <div class=\"simsub\"><b>fere(fɪə)</b>: n. 伴侣, 配偶</div> <div class=\"simsub\"><b>ferenc()</b>: n. (Ferenc)人名；(德、匈、罗、塞、波、捷)费伦茨</div> <div class=\"simsub\"><b>ference()</b>:  [人名] 费尔伦斯</div> <div class=\"simsub\"><b>rence(rens)</b>: 盲目降落区的障碍物间隔</div></div>', 'n.敬意，尊重']\n",
      "[\"<div>dilettante</div><div>[`dili'tænti]</div>\", '<div class=\"textscroll\">  <div class=\"simsub\"><b>tan([tæn])</b>: v.鞣(革)</div> <div class=\"simsub\"><b>ante(\\'ænti)</b>: n. 赌注\\\\nv. 下赌注</div> <div class=\"simsub\"><b>dile()</b>:  [医][=drug-induced lupus erythematosus]药物诱发红斑狼疮</div> <div class=\"simsub\"><b>Lett(let)</b>: n. 列托人, 列托语</div> <div class=\"simsub\"><b>letta()</b>:  [地名] [喀麦隆] 莱塔</div> <div class=\"simsub\"><b>Tante()</b>: n. (Tante)人名；(西)坦特</div></div>', 'n.半瓶醋，业余爱好者']\n",
      "[\"<div>disparate</div><div>['dispərit]</div>\", '<div class=\"textscroll\"><div class=\"sim1\">disparage: v.贬抑，轻蔑</div> <div class=\"sim2\">desperate: a.不顾死活的，拼命的</div> <div class=\"sim2\">disparity: n.不同，差异</div> <div class=\"simsub\"><b>dispar()</b>: [医] 不等的, 不相称的</div> <div class=\"simsub\"><b>para(\\'pɑ:rә)</b>: [经] 帕拉</div> <div class=\"simsub\"><b>rate(reit)</b>: n. 比率, 率, 速度, 价格, 费用, 等级\\\\nvt. 估价, 认为, 鉴定等级, 责骂\\\\nvi. 被评价, 责骂</div> <div class=\"simsub\"><b>spar(spɑ:)</b>: n. 晶石, 圆材, 拳斗, 争论\\\\nvt. 装圆材于\\\\nvi. 拳斗, 争论</div></div>', 'a.迥然不同的']\n",
      "[\"<div>emollient</div><div>[i'mɔliənt]</div>\", '<div class=\"textscroll\"> <div class=\"sim2\">emolument: n. 报酬，薪水</div> <div class=\"simsub\"><b>lien([\\'li(:)ən])</b>: n.扣押权；留置权</div> <div class=\"simsub\"><b>emol(\\'i:mɔl)</b>: 伊莫(皮肤润滑剂)</div> <div class=\"simsub\"><b>lien(\\'li:әn)</b>: n. 留置权, 扣押权\\\\n[医] 脾</div> <div class=\"simsub\"><b>moll(mɒl)</b>: n. 情妇, 妓女</div> <div class=\"simsub\"><b>molli()</b>: n. (Molli)人名；(意)莫利</div> <div class=\"simsub\"><b>mollie(\\'mɔli)</b>: n. 帆鳍鳉（等于mollienisia）</div> <div class=\"simsub\"><b>mollient(\\'mɔliәnt)</b>: n. 软化剂, 缓和剂\\\\nv. 使柔软</div> <div class=\"simsub\"><b>olli()</b>: n. (Olli)人名；(芬、瑞典)奥利</div> <div class=\"simsub\"><b>Ollie(\\'ɔli)</b>: 奥利(男子名, Oliver 的昵称)</div></div>', 'n.润肤剂']\n",
      "['<div>feign</div><div>[fein]</div>', '<div class=\"textscroll\"><div class=\"sim1\">deign: v.屈尊，惠允(做某事)</div> <div class=\"sim1\">reign: n.统治时期；王朝；领域</div> <div class=\"sim2\">align: v.将某物排列在一条直线上；与某人结盟</div> <div class=\"sim2\">benign: a.慈祥的</div> <div class=\"sim2\">feigned: a.假装的；不真诚的</div> <div class=\"sim2\">feint: v./n.佯攻，佯击</div> <div class=\"sim2\">felon: n.重罪犯</div> <div class=\"sim2\">fern: n.羊齿植物，蕨<br><img alt=\"Fern - Spore, Rhizome, Frond | Britannica\" src=\"paste-25da9c260e0162f92a3eb2a7e1864aef8a0d7537.jpg\"></div> <div class=\"sim2\">fig: n.无花果；一点儿</div> <div class=\"sim2\">rein: n.缰绳；v.控制</div> </div>', 'v.假装，伪装']\n",
      "[\"<div>flabby</div><div>['flæbi]</div>\", '<div class=\"textscroll\"> <div class=\"sim2\">flaggy: a.枯萎的；松软无力的</div> <div class=\"sim2\">lobby: n.大厅，休息厅</div> <div class=\"sim2\">flay: v. 剥皮；诈取；严厉指责</div> <div class=\"sim2\">shabby: adj. 破烂的；卑鄙的</div> <div class=\"simsub\"><b>abby(\\'æbi)</b>: n. 艾比（女子名）</div> <div class=\"simsub\"><b>flab(flæb)</b>: n. 松弛</div></div>', 'adj. (肌肉)松软的；意志薄弱的']\n",
      "[\"<div>gaucherie</div><div>[gəuʃə'ri:]</div>\", '<div class=\"textscroll\">  <div class=\"simsub\"><b>gauche([gəuʃ])</b>: adj. 笨拙的，不会社交的</div> <div class=\"simsub\"><b>auch()</b>: n. 欧什（法国西南部热尔省首府）</div> <div class=\"simsub\"><b>cher()</b>: n. 谢尔河（法国）</div> <div class=\"simsub\"><b>cheri()</b>: n. 谢利（电影名）</div> <div class=\"simsub\"><b>cherie(ʃəri)</b>: n. 切丽（女子名）</div> <div class=\"simsub\"><b>Erie(\\'iәri)</b>: n. 伊利(湖)</div> <div class=\"simsub\"><b>gauch()</b>:  [人名] 高奇</div> <div class=\"simsub\"><b>gauche(gәuʃ)</b>: a. 缺乏社经验的, 笨拙的, 粗鲁的, 无礼的, 偏转的</div> <div class=\"simsub\"><b>gaucher()</b>: n. 高雪氏症</div></div>', 'n.笨拙']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 217.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20250204(21:34:37)\n",
      "[main 98f5f9e] update cards.html on date 20250204(21:34:37)\n",
      " 22 files changed, 237 insertions(+), 110 deletions(-)\n",
      " create mode 100644 docs/images/Guinea-asks-bauxite-miners-to-present-local-refinery-plans-by-May.jpeg\n",
      " create mode 100644 docs/images/images-5186a2c0d72bb6ced6ab1099107be5415e7b0709.jpg\n",
      " create mode 100644 docs/images/images-b98b8556ba0a25e88d1579b2ffc16a79c139c395.jpg\n",
      " delete mode 100644 docs/images/images-e232c45b1db6f1af89fc1688113275e37baaa329.jpg\n",
      " create mode 100644 docs/images/paste-1438c0813bb4175e1f903f799b7063aaa3187c13.jpg\n",
      " create mode 100644 docs/images/paste-3523e57ad956c937882df3f3007eeec60c6a0a3d.jpg\n",
      " delete mode 100644 docs/images/paste-3734f2bb9ca20bb7bd2e2c08e753015ab130772f.jpg\n",
      " create mode 100644 docs/images/paste-4d6dcb69a7f61358ac70d067530f28f683cf7a2a.jpg\n",
      " create mode 100644 docs/images/paste-62f481c52aa82b6eaa5daa5b73639b2703d21517.jpg\n",
      " create mode 100644 docs/images/paste-6ba10c81153adc71fe825e59ebf26ff0f720307c.png\n",
      " create mode 100644 docs/images/paste-874ee63383f0585352d353c95290c6159defebf7.jpg\n",
      " create mode 100644 docs/images/paste-9c4c03b0a6345aaa754b5767e49078d8f7af2fe2.png\n",
      " create mode 100644 docs/images/paste-b8739423b0a7ba9cfebee845f29554a1cafe1f47.jpg\n",
      " delete mode 100644 docs/images/paste-c0dd42e84aaaf23e322d5e9b978e109d75b170c6.jpg\n",
      " create mode 100644 docs/images/paste-c10718275d766cfd76becc4dbf268c97de695ebb.png\n",
      " create mode 100644 docs/images/paste-c6d9ef7b846c57cb5c1ef5adb746da2c61a00a84.png\n",
      " create mode 100644 docs/images/paste-cfae8b5068b640cdbd4a2e2fbbe3dcdbc08a3ac6.jpg\n",
      " create mode 100644 docs/images/paste-d09601d83066b3063f7e388121fa2fedee1d1c0f.jpg\n",
      " delete mode 100644 docs/images/paste-dea8bb55b40d6a755fc9c8190c7bcfa0cff0cbd4.jpg\n",
      " delete mode 100644 docs/images/paste-ebebd3acddf1167667e5afbea68bd1142c6c8447.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To github.com:pengfeigao2021/anki_html.git\n",
      " + 61bd976...98f5f9e main -> main (forced update)\n"
     ]
    }
   ],
   "source": [
    "# read sqlite3\n",
    "import sqlite3\n",
    "import anki\n",
    "from anki.collection import Collection\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import termcolor\n",
    "import tqdm\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "import pickle\n",
    "# import pandas as pd\n",
    "import editdistance\n",
    "\n",
    "EDICT_PATH = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data/ecdict.csv'\n",
    "MEDIA_FOLDER = '/Users/AlexG/Library/Application Support/Anki2/User 1/collection.media'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/MyPaperNotes.apkg'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/ivl10.apkg'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/极品GRE红宝书.apkg'\n",
    "DATA_PATH = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data'\n",
    "outdir = PKG_PATH + '-decompress'\n",
    "# outdir = '/Users/AlexG/Downloads/ivl10-decompress'\n",
    "# ANKI2_PATH = '/Users/AlexG/Downloads/ivl10-decompress/collection.anki2'\n",
    "ANKI21_PATH = outdir + '/collection.anki21'\n",
    "# EXPORT_PATH_WEBSITE = '/Users/AlexG/Documents/GitHub/typingpractices/website/onepager/onepager/templates/onepager/cards.html'\n",
    "EXPORT_PATH_WEBSITE = '/Users/AlexG/Documents/GitHub/anki_html/docs/gre_cards.html'\n",
    "\n",
    "def unzip_anki(pkg_file_path, outdir):\n",
    "    # Create the extract directory if it doesn't exist\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    # Unzip the .apkg file\n",
    "    with zipfile.ZipFile(PKG_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(outdir)\n",
    "\n",
    "    print(f\"Extracted files to {outdir}\")\n",
    "\n",
    "class EditDistanceFinder(object):\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "        self.load_cache()\n",
    "\n",
    "    def load_cache(self):\n",
    "        cache_path = os.path.join(DATA_PATH, 'edit_distance_cache.pkl')\n",
    "        if os.path.exists(cache_path):\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                self.cache = pickle.load(f)\n",
    "        else:\n",
    "            self.cache = {}\n",
    "\n",
    "    def save_cache(self):\n",
    "        cache_path = os.path.join(DATA_PATH, 'edit_distance_cache.pkl')\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump(self.cache, f)\n",
    "\n",
    "    def find(self, query, \n",
    "             edit_distance_lower=1, \n",
    "             edit_distance_upper=1,\n",
    "             html_class=\"sim1\"):\n",
    "        res = []\n",
    "\n",
    "        # find similar words with cache\n",
    "        similar_words = []\n",
    "        key = (query, edit_distance_lower, edit_distance_upper)\n",
    "        if key in self.cache:\n",
    "            # use cache\n",
    "            similar_words = self.cache[key]\n",
    "        else:\n",
    "            for w in self.words:\n",
    "                if w[0] == query:\n",
    "                    continue\n",
    "                d = editdistance.eval(w[0], query)\n",
    "                if d >= edit_distance_lower and d <= edit_distance_upper:\n",
    "                    similar_words.append((d, w[0], w[2]))\n",
    "            # save cache\n",
    "            self.cache[key] = similar_words\n",
    "        for w in similar_words:\n",
    "            text = f'<div class=\"{html_class}\">{w[1]}: {w[2]}</div>'\n",
    "            res.append(text)\n",
    "        \n",
    "        return ' '.join(res)\n",
    "\n",
    "\n",
    "class SubStrFinder(object):\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "        self.load_cache()\n",
    "\n",
    "    def load_cache(self):\n",
    "        cache_path = os.path.join(DATA_PATH, 'sub_word_cache.pkl')\n",
    "        if os.path.exists(cache_path):\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                self.cache = pickle.load(f)\n",
    "        else:\n",
    "            self.cache = {}\n",
    "\n",
    "    def save_cache(self):\n",
    "        cache_path = os.path.join(DATA_PATH, 'sub_word_cache.pkl')\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump(self.cache, f)\n",
    "\n",
    "\n",
    "    def load_edict(self, path):\n",
    "        total_lines = 0\n",
    "        with open(path, 'r') as f:\n",
    "            rd = csv.reader(f)\n",
    "            for cols in tqdm.tqdm(rd):\n",
    "                if len(cols[0]) < 4 or len(cols[3]) == 0:\n",
    "                    continue\n",
    "                if 'abbr.' in cols[3]:\n",
    "                    continue\n",
    "                if cols[0].isupper():\n",
    "                    continue\n",
    "                fields = (cols[0], cols[1], cols[3])\n",
    "                self.words.append(fields)\n",
    "                total_lines += 1\n",
    "        print(f\"Loaded {total_lines} lines from {path}\")\n",
    "\n",
    "    def find(self, query, html_class=\"simsub\"):\n",
    "        res = []\n",
    "        # print(\"Searching for {}...\".format(termcolor.colored(query, 'red')))\n",
    "        similar_words = []\n",
    "        key = query.lower()\n",
    "        if key in self.cache:\n",
    "            similar_words = self.cache[key]\n",
    "        else:\n",
    "            for w in self.words:\n",
    "                if w[0] == query:\n",
    "                    continue\n",
    "                if w[0].lower() in query.lower():\n",
    "                    similar_words.append(w)\n",
    "            self.cache[key] = similar_words\n",
    "\n",
    "        for w in similar_words:\n",
    "            text = f'<div class=\"{html_class}\"><b>{w[0]}({w[1]})</b>: {w[2]}</div>'\n",
    "            res.append(text)\n",
    "        \n",
    "        return ' '.join(res)\n",
    "\n",
    "class ImageSrcUrls(object):\n",
    "    def __init__(self):\n",
    "        self.purge_script_path = '/Users/AlexG/Documents/GitHub/anki_html/run_images.sh'\n",
    "        self.git_push_script_path = '/Users/AlexG/Documents/GitHub/anki_html/run_upload.sh'\n",
    "        self.source_image_folder = MEDIA_FOLDER\n",
    "        self.dest_image_folder = '/Users/AlexG/Documents/GitHub/anki_html/docs/images'\n",
    "        self.IMG_PAT = re.compile(r'<img\\b.*\\bsrc=\"')\n",
    "        self.SRC_PAT = re.compile(r'<img\\b.*\\bsrc=\"([^\"]+)\"')\n",
    "        self.srcs = []\n",
    "        self.exts = set()\n",
    "\n",
    "    def replace_image_url(self, text):\n",
    "        res = self.SRC_PAT.search(text)\n",
    "        if res:\n",
    "            for s in res.groups():\n",
    "                self.srcs.append(s)\n",
    "                ext = os.path.splitext(s)[1]\n",
    "                self.exts.add(ext)\n",
    "                print(termcolor.colored(s, 'red'))\n",
    "\n",
    "        return self.IMG_PAT.sub('<img src=\"./images/', text)\n",
    "    \n",
    "    def purge_images(self):\n",
    "        os.system(f\"bash {self.purge_script_path}\") \n",
    "\n",
    "    def git_push(self):\n",
    "        os.system(f\"bash {self.git_push_script_path}\")\n",
    "        \n",
    "    def move_images(self):\n",
    "        self.purge_images()\n",
    "        for src in tqdm.tqdm(self.srcs):\n",
    "            shutil.copyfile(\n",
    "                os.path.join(self.source_image_folder, src), \n",
    "                os.path.join(self.dest_image_folder, src)\n",
    "            )\n",
    "\n",
    "\n",
    "def format_cards_to_html(cards, collection, outpath, max_word_length=-1, max_cards=200):\n",
    "    data = []\n",
    "    image_src = ImageSrcUrls()\n",
    "    for cid in tqdm.tqdm(cards):\n",
    "        card = collection.get_card(cid)\n",
    "        note = card.note()\n",
    "        fields = note.fields\n",
    "        # fields[1] = fields[1][:20]\n",
    "        # fields = [field if len(field) <= max_text_length else field[:max_text_length] + '...' for field in fields]\n",
    "        data.append((card.ivl, fields))\n",
    "\n",
    "    edist_finder = EditDistanceFinder([fields for _, fields in data])\n",
    "    substr_finder = SubStrFinder([fields for _, fields in data])\n",
    "    substr_finder.load_edict(EDICT_PATH)\n",
    "    data.sort()\n",
    "\n",
    "    # format html table rows\n",
    "    html_table_rows = []\n",
    "    with tqdm.tqdm(total=max_cards) as pbar:\n",
    "        for fields in data:\n",
    "            if max_word_length == -1 or len(fields[1][0]) <= max_word_length:\n",
    "                if len(html_table_rows) >= max_cards:\n",
    "                    break\n",
    "                fields = [image_src.replace_image_url(field) for field in fields[1]]\n",
    "                html_table_rows.append(\n",
    "                    [\n",
    "                        \"<div>{}</div><div>{}</div>\".format(fields[0], fields[1]),\n",
    "                        '<div class=\"textscroll\">{} {} {}</div>'.format(\n",
    "                            edist_finder.find(fields[0]),\n",
    "                            edist_finder.find(\n",
    "                                fields[0],\n",
    "                                edit_distance_lower=2,\n",
    "                                edit_distance_upper=2,\n",
    "                                html_class=\"sim2\",\n",
    "                            ),\n",
    "                            substr_finder.find(fields[0]),\n",
    "                        ),\n",
    "                        fields[2],\n",
    "                    ]\n",
    "                )\n",
    "                # pbar.set_description('processing word: {}'.format(termcolor.colored(fields[0]), 'red'))\n",
    "                pbar.set_description('processing word: {}'.format(fields[0]))\n",
    "                pbar.update(1)\n",
    "\n",
    "    # data = [v for v in data if max_word_length == -1 or len(v[1][0]) <= max_word_length]\n",
    "    # data = [[image_src.replace_image_url(field) for field in fields]\n",
    "    #         for _, fields in data[:max_cards]]\n",
    "    # data = [['<div>{}</div><div>{}</div>'.format(fields[0], fields[1]), \n",
    "    #          '<div class=\"textscroll\">{} {} {}</div>'.format(\n",
    "    #             edist_finder.find(fields[0]),\n",
    "    #             edist_finder.find(fields[0],\n",
    "    #                               edit_distance_lower=2, \n",
    "    #                               edit_distance_upper=2,\n",
    "    #                               html_class=\"sim2\"),\n",
    "    #             substr_finder.find(fields[0]),\n",
    "    #          ),\n",
    "    #          fields[2], \n",
    "    #          ] for fields in tqdm.tqdm(data)]\n",
    "\n",
    "    data = html_table_rows\n",
    "    # save cache\n",
    "    edist_finder.save_cache()\n",
    "    substr_finder.save_cache()\n",
    "\n",
    "    print(image_src.exts)\n",
    "    print(termcolor.colored('Data samples:', 'blue'))\n",
    "    print('\\n'.join(['{}'.format(v) for v in data[:10]]))\n",
    "    image_src.move_images()\n",
    "    # with open(outpath+'.csv', 'w') as f:\n",
    "    #     wt = csv.writer(f)\n",
    "    #     wt.writerows([v[:3] for v in data])\n",
    "    rows = ['<tr><td class=\"indextd\">{}</td><td>{}</td></tr>\\n'.format(fi, '</td><td class=\"fixwtd\">'.join(field)) for fi, field in enumerate(data)]\n",
    "    html = f'''\n",
    "    <head>\n",
    "        <style>\n",
    "            body {{\n",
    "                background: #202020;\n",
    "            }}\n",
    "\n",
    "            table {{\n",
    "                background-color: #A0A0A0;\n",
    "                --color: #d0d0f5;\n",
    "                margin: 3em;\n",
    "            }}\n",
    "\n",
    "            thead,\n",
    "            tfoot {{\n",
    "                background: var(--color);\n",
    "            }}\n",
    "\n",
    "            tbody tr:nth-child(even) {{\n",
    "                background: color-mix(in srgb, var(--color), transparent 60%);\n",
    "            }}\n",
    "            div, p, td {{\n",
    "                width: 200px;\n",
    "                word-wrap: break-word;\n",
    "                white-space: normal;\n",
    "            }}\n",
    "            td img {{\n",
    "                max-height: 8em;\n",
    "            }}\n",
    "            .fixwtd {{\n",
    "                width: 200px;\n",
    "                word-wrap: break-word;\n",
    "                white-space: normal;\n",
    "            }}\n",
    "            .indextd {{\n",
    "                width: 10px;\n",
    "                word-wrap: break-word;\n",
    "                white-space: normal;\n",
    "            }}\n",
    "            td {{\n",
    "                border: 1px ridge #333333;\n",
    "            }}\n",
    "            img {{\n",
    "                max-height: 5em;\n",
    "                max-width: 5em;\n",
    "            }}\n",
    "            .sim1 {{\n",
    "                width: 26em;\n",
    "                background-color: #eadcb7;\n",
    "            }}\n",
    "            .sim2 {{\n",
    "                width: 26em;\n",
    "                background-color: #e99a06;\n",
    "            }}\n",
    "            .simsub {{\n",
    "                width: 26em;\n",
    "                background-color: #afcd95;\n",
    "            }}\n",
    "            b {{\n",
    "                background-color: #f1c40f;\n",
    "            }}\n",
    "            .textscroll {{\n",
    "                /* height: 3em; */\n",
    "                width: 27em;\n",
    "                border: 1px solid #ccc;\n",
    "                overflow: auto;\n",
    "            }}\n",
    "        </style>\n",
    "        <link rel=\"stylesheet\" href=\"css/fancy-button.css\">\n",
    "    </head>\n",
    "    <body>\n",
    "        <button onclick=\"changeWidth(0.5)\" class=\"button-74\">Half Column Width</button>\n",
    "        <button onclick=\"hideRows(10)\" class=\"button-73 fix-width-button\">Hide Rows</button>\n",
    "        <table id=\"cards\">\n",
    "          {\"\".join(rows)}\n",
    "        </table>\n",
    "        <script src=\"js/change-width.js\"></script>\n",
    "        <script src=\"js/hide-rows.js\"></script>\n",
    "    </body>\n",
    "    '''\n",
    "    with open(outpath, 'w') as f:\n",
    "        f.write(html)\n",
    "\n",
    "    # git push\n",
    "    image_src.git_push()\n",
    "\n",
    "try:\n",
    "    col = Collection(ANKI21_PATH)\n",
    "    print(dir(col))\n",
    "    print('total card count: ', col.card_count())\n",
    "    # print('note count: ', col.node_count())\n",
    "\n",
    "    # query = 'prop:ivl<=30'\n",
    "    query = ''\n",
    "    cards = col.find_cards(query)\n",
    "    print(termcolor.colored(f'query {query} find {len(cards)} cards', 'green'))\n",
    "\n",
    "    outpath = os.path.join(outdir, 'cards.html')\n",
    "    # format_cards_to_html(cards, col, outpath)\n",
    "    format_cards_to_html(cards, col, EXPORT_PATH_WEBSITE,\n",
    "                         max_word_length=-1)\n",
    "\n",
    "    # Fetch all notes (cards content) in the collection\n",
    "    # print(\"\\nNotes in collection:\")\n",
    "    # for note_id in col.db.list(\"select id from notes\"):\n",
    "    #     note = col.get_note(note_id)\n",
    "    #     fields = note.fields\n",
    "    #     print(f\"Note ID: {note_id}, Fields: {fields}\")\n",
    "\n",
    "    # # Fetch all cards (metadata) in the collection\n",
    "    # print(\"\\nCards in collection:\")\n",
    "    # for card_id in col.db.list(\"select id from cards\"):\n",
    "    #     card = col.get_card(card_id)\n",
    "    #     print(f\"Card ID: {card.id}, Note ID: {card.nid}, Deck ID: {card.did}, Type: {card.type}\")\n",
    "\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Caught an exception: {e}\")\n",
    "    import traceback\n",
    "    print(traceback.format_exc())\n",
    "finally:\n",
    "    # Close the collection\n",
    "    col.close()\n",
    "\n",
    "col.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------\n",
    "# Offline batch similar words\n",
    "# -------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read sqlite3\n",
    "import sqlite3\n",
    "import anki\n",
    "from anki.collection import Collection\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import termcolor\n",
    "import tqdm\n",
    "import csv\n",
    "import shutil\n",
    "# import pandas as pd\n",
    "import editdistance\n",
    "\n",
    "EDICT_PATH = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data/ecdict.csv'\n",
    "MEDIA_FOLDER = '/Users/AlexG/Library/Application Support/Anki2/User 1/collection.media'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/MyPaperNotes.apkg'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/ivl10.apkg'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/极品GRE红宝书.apkg'\n",
    "outdir = PKG_PATH + '-decompress'\n",
    "# outdir = '/Users/AlexG/Downloads/ivl10-decompress'\n",
    "# ANKI2_PATH = '/Users/AlexG/Downloads/ivl10-decompress/collection.anki2'\n",
    "ANKI21_PATH = outdir + '/collection.anki21'\n",
    "# EXPORT_PATH_WEBSITE = '/Users/AlexG/Documents/GitHub/typingpractices/website/onepager/onepager/templates/onepager/cards.html'\n",
    "EXPORT_PATH_WEBSITE = '/Users/AlexG/Documents/GitHub/anki_html/docs/gre_cards.html'\n",
    "\n",
    "def unzip_anki(pkg_file_path, outdir):\n",
    "    # Create the extract directory if it doesn't exist\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    # Unzip the .apkg file\n",
    "    with zipfile.ZipFile(PKG_PATH, 'r') as zip_ref:\n",
    "        zip_ref.extractall(outdir)\n",
    "\n",
    "    print(f\"Extracted files to {outdir}\")\n",
    "\n",
    "class EditDistanceFinder(object):\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "\n",
    "    def find(self, query, \n",
    "             edit_distance_lower=1, \n",
    "             edit_distance_upper=1,\n",
    "             html_class=\"sim1\"):\n",
    "        res = []\n",
    "        for w in self.words:\n",
    "            if w[0] == query:\n",
    "                continue\n",
    "            d = editdistance.eval(w[0], query)\n",
    "            if d >= edit_distance_lower and d <= edit_distance_upper:\n",
    "                text = f'<div class=\"{html_class}\">{w[0]}: {w[2]}</div>'\n",
    "                res.append(text)\n",
    "        \n",
    "        return ' '.join(res)\n",
    "            \n",
    "        \n",
    "class SubStrFinder(object):\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "\n",
    "    def load_edict(self, path):\n",
    "        total_lines = 0\n",
    "        with open(path, 'r') as f:\n",
    "            rd = csv.reader(f)\n",
    "            for cols in tqdm.tqdm(rd):\n",
    "                if len(cols[0]) < 4 or len(cols[3]) == 0:\n",
    "                    continue\n",
    "                if 'abbr.' in cols[3]:\n",
    "                    continue\n",
    "                if cols[0].isupper():\n",
    "                    continue\n",
    "                fields = (cols[0], cols[1], cols[3])\n",
    "                self.words.append(fields)\n",
    "                total_lines += 1\n",
    "        print(f\"Loaded {total_lines} lines from {path}\")\n",
    "\n",
    "    def find(self, query, html_class=\"simsub\"):\n",
    "        res = []\n",
    "        print(\"Searching for {}...\".format(termcolor.colored(query, 'red')))\n",
    "        for w in tqdm.tqdm(self.words):\n",
    "            if w[0] == query:\n",
    "                continue\n",
    "            if w[0].lower() in query.lower():\n",
    "                text = f'<div class=\"{html_class}\"><b>{w[0]}({w[1]})</b>: {w[2]}</div>'\n",
    "                res.append(text)\n",
    "        \n",
    "        return ' '.join(res)\n",
    "\n",
    "class ImageSrcUrls(object):\n",
    "    def __init__(self):\n",
    "        self.purge_script_path = '/Users/AlexG/Documents/GitHub/anki_html/run_images.sh'\n",
    "        self.git_push_script_path = '/Users/AlexG/Documents/GitHub/anki_html/run_upload.sh'\n",
    "        self.source_image_folder = MEDIA_FOLDER\n",
    "        self.dest_image_folder = '/Users/AlexG/Documents/GitHub/anki_html/docs/images'\n",
    "        self.IMG_PAT = re.compile(r'<img\\b.*\\bsrc=\"')\n",
    "        self.SRC_PAT = re.compile(r'<img\\b.*\\bsrc=\"([^\"]+)\"')\n",
    "        self.srcs = []\n",
    "        self.exts = set()\n",
    "\n",
    "    def replace_image_url(self, text):\n",
    "        res = self.SRC_PAT.search(text)\n",
    "        if res:\n",
    "            for s in res.groups():\n",
    "                self.srcs.append(s)\n",
    "                ext = os.path.splitext(s)[1]\n",
    "                self.exts.add(ext)\n",
    "                print(termcolor.colored(s, 'red'))\n",
    "\n",
    "        return self.IMG_PAT.sub('<img src=\"./images/', text)\n",
    "    \n",
    "    def purge_images(self):\n",
    "        os.system(f\"bash {self.purge_script_path}\") \n",
    "\n",
    "    def git_push(self):\n",
    "        os.system(f\"bash {self.git_push_script_path}\")\n",
    "        \n",
    "    def move_images(self):\n",
    "        self.purge_images()\n",
    "        for src in tqdm.tqdm(self.srcs):\n",
    "            shutil.copyfile(\n",
    "                os.path.join(self.source_image_folder, src), \n",
    "                os.path.join(self.dest_image_folder, src)\n",
    "            )\n",
    "\n",
    "\n",
    "def format_cards_to_html(cards, collection, outpath, max_word_length=-1, max_cards=200):\n",
    "    data = []\n",
    "    image_src = ImageSrcUrls()\n",
    "    for cid in tqdm.tqdm(cards):\n",
    "        card = collection.get_card(cid)\n",
    "        note = card.note()\n",
    "        fields = note.fields\n",
    "        # fields[1] = fields[1][:20]\n",
    "        # fields = [field if len(field) <= max_text_length else field[:max_text_length] + '...' for field in fields]\n",
    "        data.append((card.ivl, fields))\n",
    "\n",
    "    edist_finder = EditDistanceFinder([fields for _, fields in data])\n",
    "    substr_finder = SubStrFinder([fields for _, fields in data])\n",
    "    substr_finder.load_edict(EDICT_PATH)\n",
    "    data.sort()\n",
    "    data = [v for v in data if max_word_length == -1 or len(v[1][0]) <= max_word_length]\n",
    "    data = [[image_src.replace_image_url(field) for field in fields] \n",
    "            for _, fields in data[:max_cards]]\n",
    "    data = [['<div>{}</div><div>{}</div>'.format(fields[0], fields[1]), \n",
    "             '<div class=\"textscroll\">{} {} {}</div>'.format(\n",
    "                edist_finder.find(fields[0]),\n",
    "                edist_finder.find(fields[0],\n",
    "                                  edit_distance_lower=2, \n",
    "                                  edit_distance_upper=2,\n",
    "                                  html_class=\"sim2\"),\n",
    "                substr_finder.find(fields[0]),\n",
    "             ),\n",
    "             fields[2], \n",
    "             ] for fields in data]\n",
    "    print(image_src.exts)\n",
    "    print(termcolor.colored('Data samples:', 'blue'))\n",
    "    print('\\n'.join(['{}'.format(v) for v in data[:10]]))\n",
    "\n",
    "unzip_anki(PKG_PATH, outdir)\n",
    "col = Collection(ANKI21_PATH)\n",
    "# Fetch all decks in the collection\n",
    "decks = col.decks.all()\n",
    "print(\"Decks in collection:\")\n",
    "for deck in decks:\n",
    "    print(f\"Deck ID: {deck['id']}, Name: {deck['name']}\")\n",
    "\n",
    "try:\n",
    "    print(dir(col))\n",
    "    print('total card count: ', col.card_count())\n",
    "    # print('note count: ', col.node_count())\n",
    "\n",
    "    # query = 'prop:ivl<=30'\n",
    "    query = ''\n",
    "    cards = col.find_cards(query)\n",
    "    print(termcolor.colored(f'query {query} find {len(cards)} cards', 'green'))\n",
    "\n",
    "    outpath = os.path.join(outdir, 'cards.html')\n",
    "    # format_cards_to_html(cards, col, outpath)\n",
    "    format_cards_to_html(cards, col, EXPORT_PATH_WEBSITE,\n",
    "                         max_word_length=-1)\n",
    "\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    # Close the collection\n",
    "    col.close()\n",
    "\n",
    "col.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====================\n",
    "# build all words list\n",
    "# ====================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decks in collection:\n",
      "Deck ID: 1, Name: Default\n",
      "Deck ID: 1685695741424, Name: 极品GRE红宝书\n",
      "total card count:  7513\n",
      "\u001b[32mquery  find 7513 cards\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7513/7513 [00:02<00:00, 3308.24it/s]\n",
      "770612it [00:03, 229250.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 750698 lines from /Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data/ecdict.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750698/750698 [00:00<00:00, 1021786.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 758211\n"
     ]
    }
   ],
   "source": [
    "# read sqlite3\n",
    "import sqlite3\n",
    "import anki\n",
    "from anki.collection import Collection\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import termcolor\n",
    "import tqdm\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "# import pandas as pd\n",
    "import editdistance\n",
    "\n",
    "EDICT_PATH = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/data/ecdict.csv'\n",
    "PKG_PATH = '/Users/AlexG/Downloads/极品GRE红宝书.apkg'\n",
    "outdir = PKG_PATH + '-decompress'\n",
    "ANKI21_PATH = outdir + '/collection.anki21'\n",
    "\n",
    "class SubStrFinder(object):\n",
    "    def __init__(self, words):\n",
    "        self.words = words\n",
    "\n",
    "    def load_edict(self, path):\n",
    "        total_lines = 0\n",
    "        with open(path, 'r') as f:\n",
    "            rd = csv.reader(f)\n",
    "            for cols in tqdm.tqdm(rd):\n",
    "                if len(cols[0]) < 4 or len(cols[3]) == 0:\n",
    "                    continue\n",
    "                if 'abbr.' in cols[3]:\n",
    "                    continue\n",
    "                if cols[0].isupper():\n",
    "                    continue\n",
    "                fields = (cols[0], cols[1], cols[3])\n",
    "                self.words.append(fields)\n",
    "                total_lines += 1\n",
    "        print(f\"Loaded {total_lines} lines from {path}\")\n",
    "\n",
    "\n",
    "col = Collection(ANKI21_PATH)\n",
    "# Fetch all decks in the collection\n",
    "decks = col.decks.all()\n",
    "all_words_set = set()\n",
    "all_words = []\n",
    "gre_words = []\n",
    "print(\"Decks in collection:\")\n",
    "for deck in decks:\n",
    "    print(f\"Deck ID: {deck['id']}, Name: {deck['name']}\")\n",
    "\n",
    "try:\n",
    "    print('total card count: ', col.card_count())\n",
    "\n",
    "    query = ''\n",
    "    cards = col.find_cards(query)\n",
    "    print(termcolor.colored(f'query {query} find {len(cards)} cards', 'green'))\n",
    "\n",
    "    for cid in tqdm.tqdm(cards):\n",
    "        card = col.get_card(cid)\n",
    "        note = card.note()\n",
    "        fields = note.fields\n",
    "        all_words_set.add(fields[0])\n",
    "        gre_words.append(fields[0])\n",
    "    substr_finder = SubStrFinder([])\n",
    "    substr_finder.load_edict(EDICT_PATH)\n",
    "    for w in tqdm.tqdm(substr_finder.words):\n",
    "        all_words_set.add(w)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    # Close the collection\n",
    "    col.close()\n",
    "\n",
    "col.close()\n",
    "\n",
    "all_words = list(all_words_set)\n",
    "print(f\"Total words: {len(all_words)}\")\n",
    "with open('/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/all_words.json', 'w') as f:\n",
    "    json.dump(all_words, f)\n",
    "with open('/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/gre_words.json', 'w') as f:\n",
    "    json.dump(gre_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from /Users/AlexG/Documents/GitHub/mindmaps/anki_tools/gre_words.json and /Users/AlexG/Documents/GitHub/mindmaps/anki_tools/all_words.json\n",
      "loaded 7513 gre words and 758211 all words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 152303449/5696439243 [01:34<57:08, 1616872.64it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m N_gre \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(gre_words)\n\u001b[1;32m     40\u001b[0m N_all \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_words)\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n1, n2 \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(itertools\u001b[38;5;241m.\u001b[39mproduct(\u001b[38;5;28mrange\u001b[39m(N_gre), \u001b[38;5;28mrange\u001b[39m(N_all)), total\u001b[38;5;241m=\u001b[39mN_gre \u001b[38;5;241m*\u001b[39m N_all):\n\u001b[1;32m     42\u001b[0m     w1 \u001b[38;5;241m=\u001b[39m gre_words[n1][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     43\u001b[0m     w2 \u001b[38;5;241m=\u001b[39m all_words[n2][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/GitHub/mindmaps/devenv/lib/python3.9/site-packages/tqdm/std.py:1187\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m n \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlast_print_n\u001b[49m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminiters:\n\u001b[1;32m   1188\u001b[0m     cur_t \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m   1189\u001b[0m     dt \u001b[38;5;241m=\u001b[39m cur_t \u001b[38;5;241m-\u001b[39m last_print_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "# read sqlite3\n",
    "import sqlite3\n",
    "import anki\n",
    "from anki.collection import Collection\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import termcolor\n",
    "import tqdm\n",
    "import csv\n",
    "import json\n",
    "import shutil\n",
    "# import pandas as pd\n",
    "import editdistance\n",
    "# a = [1,2,3]\n",
    "# b= itertools.product(a, a)\n",
    "# print(list(b))\n",
    "\n",
    "gre_words_path = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/gre_words.json'\n",
    "all_words_path = '/Users/AlexG/Documents/GitHub/mindmaps/anki_tools/all_words.json'\n",
    "print('loading from {} and {}'.format(gre_words_path, all_words_path))\n",
    "gre_words = json.load(open(gre_words_path))\n",
    "all_words = json.load(open(all_words_path))\n",
    "print('loaded {} gre words and {} all words'.format(len(gre_words), len(all_words)))\n",
    "\n",
    "def find(self, query, html_class=\"simsub\"):\n",
    "    res = []\n",
    "    print(\"Searching for {}...\".format(termcolor.colored(query, 'red')))\n",
    "    for w in tqdm.tqdm(self.words):\n",
    "        if w[0] == query:\n",
    "            continue\n",
    "        if w[0].lower() in query.lower():\n",
    "            text = f'<div class=\"{html_class}\"><b>{w[0]}({w[1]})</b>: {w[2]}</div>'\n",
    "            res.append(text)\n",
    "    \n",
    "    return ' '.join(res)\n",
    "\n",
    "N_gre = len(gre_words)\n",
    "N_all = len(all_words)\n",
    "for n1, n2 in tqdm.tqdm(itertools.product(range(N_gre), range(N_all)), total=N_gre * N_all):\n",
    "    w1 = gre_words[n1][0]\n",
    "    w2 = all_words[n2][0]\n",
    "    # d = editdistance.eval(w1, w2)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
